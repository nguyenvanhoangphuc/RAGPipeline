{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chromadb==0.4.14 # xài bản này để không bị lỗi read only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "import re\n",
    "import json\n",
    "load_dotenv(find_dotenv())\n",
    "import my_utils\n",
    "# from my_utils import extract_json_from_string\n",
    "import os\n",
    "my_utils.setupCuda()\n",
    "os.environ['TAVILY_API_KEY'] = \"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\"\n",
    "# os.getenv(\"TAVILY_API_KEY\")\n",
    "def extract_triple_quotes(text):\n",
    "    match = re.search(r'\"\"\"(.*?)\"\"\"', text, re.DOTALL)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "def extract_json_from_string(text):\n",
    "    # Xóa dấu phẩy cuối cùng trong JSON nếu có\n",
    "    text = re.sub(r',\\s*}', '}', text)\n",
    "    text = re.sub(r',\\s*\\]', ']', text)\n",
    "\n",
    "    # Lấy từ dấu \"{\" đầu tiên đến dấu \"}\" cuối cùng\n",
    "    match = re.search(r'\\{.*\\}', text, re.DOTALL)\n",
    "    \n",
    "    if match:\n",
    "        json_str = match.group(0).replace(\".\",\"\")\n",
    "        try:\n",
    "            return json.loads(json_str)\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Lỗi: Không thể parse JSON\")\n",
    "            print(f\"json_str: {json_str}\")\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def retry_failed_batches(batch_inputs, responses,generater, max_retries=3):\n",
    "    \"\"\"Retry batch chỉ cho những input bị lỗi.\"\"\"\n",
    "    failed_indices = [i for i, res in enumerate(responses) if not res]\n",
    "    for attempt in range(max_retries):\n",
    "        if not failed_indices:\n",
    "            break  # Nếu không còn lỗi thì dừng lại\n",
    "\n",
    "        failed_inputs = [batch_inputs[i] for i in failed_indices]\n",
    "        new_responses = generater.batch(failed_inputs)\n",
    "\n",
    "        # Cập nhật responses với kết quả mới từ retry\n",
    "        for i, idx in enumerate(failed_indices):\n",
    "            if new_responses[i]:\n",
    "                responses[idx] = new_responses[i]  # Thay thế response cũ bị lỗi\n",
    "\n",
    "        # Lọc lại danh sách lỗi sau retry\n",
    "        failed_indices = [i for i, res in enumerate(responses) if not res]\n",
    "        print(f\"Retry {attempt + 1}/{max_retries}: {len(failed_indices)} cases left.\")\n",
    "    if failed_indices:\n",
    "        print(\"THERE IS STILL ERROR EVEN TRIED REGEN 3 TIMES\")\n",
    "        print(f\"FAIL INPUTS {failed_inputs}\")\n",
    "        print(f\"FAIL RESPONSE {new_responses}\")        \n",
    "    return responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRAPH new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms.vllm import VLLM\n",
    "# from vllm import SamplingParams\n",
    "import json\n",
    " \n",
    "vllm_model = VLLM(\n",
    "    model=\"/home/thanhnguyen/Data/code/RAG_techniques/Qwen2.5-14B-Instruct-GPTQ-Int4\",\n",
    "    tensor_parallel_size=1,\n",
    "    n=1,\n",
    "    presence_penalty=0.0,\n",
    "    frequency_penalty=0.0,\n",
    "    temperature=0.7,\n",
    "    top_p=0.6,\n",
    "    top_k=5,\n",
    "    stop=None,\n",
    "    ignore_eos=False,\n",
    "    max_new_tokens=2048,\n",
    "    logprobs=None,\n",
    "    download_dir=None,\n",
    "    vllm_kwargs={\n",
    "        \"quantization\":\"gptq\",\n",
    "        \"max_model_len\": 3036,\n",
    "        \"gpu_memory_utilization\":0.8\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### routerQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Router\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Prompt \n",
    "routerquery_template = \"\"\"<|im_start|>system\n",
    "\n",
    "You are an expert in routing user questions to the appropriate data source: either a vectorstore or web search.\n",
    "\n",
    "Data Sources:\n",
    "Vectorstore: Contains laws related to labor contracts, labor-related issues, and agreements between employers and employees.\n",
    "Use this source for questions about employment contracts, worker rights, labor laws, employer-employee agreements, and related regulations.\n",
    "Web Search: Use this for all other topics that are not covered by the vectorstore.\n",
    "\n",
    "If the question is ambiguous or partially related to labor laws, prioritize vectorstore unless it explicitly requires external, real-time, or broader information.\n",
    "Instructions:\n",
    "Determine the appropriate data source based on the question's topic.\n",
    "Return the result in the following JSON format:\n",
    "{{\n",
    "  \"datasource\": \"vectorstore\" | \"web_search\"\n",
    "}}\n",
    "<|im_end|>\n",
    "\n",
    "<|im_start|>user\n",
    "Question: {question}\n",
    "\n",
    "<|im_end|>\n",
    "\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "routerquery_prompt = PromptTemplate.from_template(routerquery_template)\n",
    "\n",
    "# Chain\n",
    "question_router = routerquery_prompt | vllm_model |RunnableLambda(extract_json_from_string)\n",
    "\n",
    "print(question_router.invoke({\"question\": \"who is Biden\"}))\n",
    "print(question_router.invoke({\"question\": \"\"\"第1章　総則\n",
    "第1条（目的）\n",
    "１．この就業規則（以下「規則」という。）は、株式会社●●●●の労働者の就業に関する事項を定めるものである。\n",
    "\"\"\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### propositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proposition_template=\"\"\"<|im_start|>system\n",
    "# The following input is a legal document or contract. Your task is to break down the document into simple, self-contained propositions that are easy to understand. Follow these steps:\n",
    "\n",
    "# 1.Identify key sentences: Read the entire document and identify sentences that contain factual information, rules, or conditions. Ignore introductory sentences, transitional sentences, or ambiguous statements (e.g., \"The leave period is as follows.\" or \"The contents of this contract are as follows.\").\n",
    "\n",
    "# 2.Make each proposition self-contained: Each proposition should be understandable on its own, without additional context. Replace pronouns and vague references with specific names or terms.\n",
    "\n",
    "# 3.Ensure accuracy and completeness: Include necessary details such as dates, legal references, and conditions to ensure the propositions are accurate. Avoid incomplete or ambiguous propositions (e.g., \"The company will respond appropriately.\" → \"The company will provide appropriate compensation if an employee suffers a work-related accident.\").\n",
    "\n",
    "# 4.Maintain logical structure: Each proposition should be a complete sentence with a clear subject and predicate. If a rule contains multiple conditions, split them into separate propositions while maintaining logical relationships. Merge related propositions when necessary to preserve meaning.\n",
    "\n",
    "# 5.Avoid overlapping propositions: Each proposition must convey distinct information and should not be a restatement of another proposition with minor modifications. Avoid cases where one proposition is a broader version of another with additional details. Instead, structure the propositions so that they remain independent while covering all necessary details.\n",
    "\n",
    "# 6.Extract sub-clause number(s) if available:\n",
    "\n",
    "# If a sub-clause number (e.g., ①, ②, (1), (2)) appears at the beginning of a sentence in the input document, extract it and associate it with the corresponding proposition.\n",
    "# Convert numbered symbols to integers (e.g., ① → 1, (2) → 2).\n",
    "# If multiple sub-clauses are combined into a single proposition, return them as an array (e.g., [1, 2]).\n",
    "# If no sub-clause number is present, return an empty array [].\n",
    "# Use appropriate legal or technical terms: Write all propositions in English and use proper legal or technical terminology.\n",
    "# The response should be in Japanese and returned in following JSON format:\n",
    "# {{\n",
    "#   \"propositions\": \n",
    "#     [\n",
    "#     {{\"proposition\":\"Generated proposition 1\",\n",
    "#       \"sub-clause\": [int]\n",
    "#     }},\n",
    "#     {{\"proposition\":\"Generated proposition 2\",\n",
    "#       \"sub-clause\": [int]\n",
    "#     }}...\n",
    "#     ]\n",
    "# }}\n",
    "# <|im_end|>\n",
    "\n",
    "# <|im_start|>user\n",
    "# contract:{document}\n",
    "# <|im_end|>\n",
    "\n",
    "# <|im_start|>assistant\n",
    "# \"\"\"\n",
    "proposition_template=\"\"\"<|im_start|>system\n",
    "The following input is a legal document or contract. Your task is to break down the document into simple, self-contained propositions that are easy to understand. Follow these steps:\n",
    "\n",
    "1. Identify key sentences: \n",
    "Read the entire document and extract factual information, rules, or conditions. Ignore introductory or transitional phrases that do not contain substantial content.\n",
    "\n",
    "2. Ensure completeness of definitions:  \n",
    "   - If a proposition defines a term (for example: \"Xとは…\"), ensure it contains a complete definition.  \n",
    "   - Do not generate incomplete propositions such as:  \n",
    "    \"A worker is defined as follows.\" → This does not provide a definition.  \n",
    "    Instead, include the full definition within the same proposition:  \n",
    "    \"A worker is a person who joins the company according to the procedure set forth in Chapter 2 and meets the following criteria: [list of criteria].\"  \n",
    "\n",
    "3. Make each proposition self-contained:  \n",
    "   - Ensure the proposition is understandable without additional context.  \n",
    "   - Replace pronouns or vague references with explicit terms.  \n",
    "\n",
    "4. Maintain accuracy and include all necessary details:  \n",
    "   - Retain legal references, dates, and conditions to avoid ambiguity.  \n",
    "   - If a rule contains multiple conditions, split them into separate propositions while preserving logical relationships.  \n",
    "\n",
    "5. Avoid overlapping or redundant propositions:  \n",
    "   - Do not generate broad statements that are just incomplete versions of other propositions.  \n",
    "\n",
    "6. Use appropriate legal or technical terms: Write all propositions in janpanese and use proper legal or technical terminology.\n",
    "The response should be in Japanese and returned in following JSON format:\n",
    "{{\n",
    "  \"propositions\": \n",
    "    [\n",
    "    {{\"proposition\":\"Generated proposition in japanese\"}}\n",
    "    ]\n",
    "}}\n",
    "<|im_end|>\n",
    "\n",
    "<|im_start|>user\n",
    "contract:{document}\n",
    "<|im_end|>\n",
    "\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "proposition_prompt = PromptTemplate.from_template(proposition_template)\n",
    "proposition_generator = proposition_prompt | vllm_model| RunnableLambda(extract_json_from_string)\n",
    "# proposition_generator.invoke({\"document\":\"\"\"会社は、業務上必要がある場合に労働者に対し以下の人事異動を命ずることがある。\n",
    "# (1)就業する場所及び従事する業務の変更\n",
    "# (2)関連会社への出向\n",
    "# (3)関連会社への転籍\n",
    "# (4)会社は、第３条第２項に定めるところにより、従業員にその他の業務を行わせることができる。                             \n",
    "# \"\"\"})\n",
    "# proposition_generator.invoke({\"document\":\"\"\"労働者として採用された者は、採用を通知された日から会社が指定する期日までに次の書類を提出しなければならない。ただし、会社が認めた場合には、その一部を省略または提出期限を延期することができる。\n",
    "# 　⑫身元保証書\n",
    "# 　⑬雇用保険被保険者証の写し\n",
    "# 　⑭マイナンバーカード又はマイナンバー通知書の写し\n",
    "# 　⑮資格免許証の写し\n",
    "# 　⑯健康診断結果\n",
    "# 　⑰学生証の写し\n",
    "# 　⑱パスポートの写し\n",
    "# 　⑲その他会社が必要と認めたもの\n",
    "# \"\"\"})\n",
    "# proposition_generator.invoke({\"document\":\"\"\"労働者として採用された者は、採用を通知された日から会社が指定する期日までに次の書類を提出しなければならない。ただし、会社が認めた場合には、その一部を省略または提出期限を延期することができる。\n",
    "# 　①履歴書・職務経歴書\n",
    "# 　②入社誓約書\n",
    "# 　③個人情報保護法に基づく誓約書\n",
    "# 　④住民票記載事項証明書\n",
    "# 　⑤給与所得者の扶養控除等（異動）申告書\n",
    "# 　⑥源泉徴収票\n",
    "# 　⑦雇用契約書\n",
    "# 　⑧年金手帳の写し\n",
    "# 　⑨通勤手当申請書\n",
    "# 　⑩住宅手当申請書\n",
    "# 　⑪給与振込先申請書\n",
    "# 　⑫身元保証書\n",
    "# 　⑬雇用保険被保険者証の写し\n",
    "# 　⑭マイナンバーカード又はマイナンバー通知書の写し\n",
    "# 　⑮資格免許証の写し\n",
    "# 　⑯健康診断結果\n",
    "# 　⑰学生証の写し\n",
    "# 　⑱パスポートの写し\n",
    "# 　⑲その他会社が必要と認めたもの\n",
    "# \"\"\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### extract reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_reference_template=\"\"\"<|im_start|>system\n",
    "You will receive:\n",
    "1. A passage from a contract that may include references to other sections, articles, clauses, or parts of the contract (e.g., “as specified in Section 2”, “according to the clause above”, “refer to Article 3 below”).\n",
    "2. Metadata providing context for the current passage, including details such as the current Chapter, Article, and Clause.\n",
    "\n",
    "### Task:\n",
    "#### Step 1: Extract References\n",
    "- Identify and extract all phrases in the contract text that refer to other sections, articles, or clauses.\n",
    "\n",
    "#### Step 2: Resolve References\n",
    "- Use the provided metadata to determine the specific Chapter, Article, or Clause that each reference points to.\n",
    "- Examples:\n",
    "  -第5項 means clause 5\n",
    "  - If the metadata indicates that the current passage is in Article 1:\n",
    "    - \"the above article\" refers to Article 0 (or the last article of the previous section, if applicable).\n",
    "    - \"clause 3 below\" refers to Clause 3 in the subsequent article or section.\n",
    "- If a reference cannot be precisely resolved, set its \"resolved\" value to -1.\n",
    "-\"has_reference\": \"yes\" if there is at least one reference, otherwise \"no\".\n",
    "-\"is_extractable\": \"yes\" if the reference can be extracted and resolved, otherwise \"no\".\n",
    "-\"references\": A list of extracted references, each containing:\n",
    "-\"text\": The extracted reference phrase.\n",
    "-\"resolved\": The resolved location, with -1 if it cannot be determined.\n",
    "\n",
    "The response should be in following JSON format. The answer contain json only:\n",
    "{{\n",
    "  \"has_reference\" : \"yes\" | \"no\",\n",
    "  \"is_extractable\" : \"yes\" | \"no\"\n",
    "  \"references\": [\n",
    "    {{\n",
    "      \"text\": str,\n",
    "      \"resolved\": {{\n",
    "        \"Chapter\": int| -1,\n",
    "        \"Article\": int| -1,\n",
    "        \"clause\": int| -1,\n",
    "        \"sub_clause\": int|-1,\n",
    "      }}\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "<|im_end|>\n",
    "\n",
    "<|im_start|>user\n",
    "contract:{document}\n",
    "metadata:{metadata}\n",
    "<|im_end|>\n",
    "\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "extract_reference_prompt = PromptTemplate.from_template(extract_reference_template)\n",
    "extract_reference_generator = extract_reference_prompt | vllm_model| RunnableLambda(extract_json_from_string)\n",
    "# document = \"\"\"本契約の第2条に基づき、甲は乙に対し、必要な情報を提供しなければならない。また、第1章の規定に従い、支払いは第3条に定められた条件で行われる。\n",
    "# \"\"\"\n",
    "# document=\"\"\"本契約の第4条に基づき、甲は、上記2条に記載されている条件に従うものとする。\"\"\"\n",
    "# document=\"\"\"本契約の第4条第2項第3号に基づき、甲は、上記第2条第1項に記載されている条件に従うものとする。\"\"\"\n",
    "document =\"\"\"採用後、配属された担当職種(職務)での勤務成績が劣る場合には、会社は、労働者本人との面接を実施し、教育指導・改善指導を行う。ただし、その後もなお同様の勤務成績が継続した場合には、採用時に明示した労働条件(所定の基本賃金額・諸手当額を含む)を見直し、担当職種、および担当職務の変更を伴う配置転換、転勤、出向等を検討して人事異動を命じる。労働者はこの命令を拒むことはできないものとする。 ただし、配置転換、転勤、出向等の人事異動を行うについて異動により配属する部署、配置する担当職務等、就業場所の確保が困難な場合には、労基法の定めに従い解雇する。\"\"\"\n",
    "# metadata=\"\"\"{\n",
    "#   \"current_chapter\": 2,\n",
    "#   \"current_article\": 3\n",
    "# }\"\"\"\n",
    "metadata=\"\"\"{\n",
    "  \"current_chapter\": 1,\n",
    "  \"current_article\": 4,\n",
    "  \"current_clause\": 4,\n",
    "  \"current_sub_clause\": 5\n",
    "}\"\"\"\n",
    "extract_reference_generator.invoke({\"document\":document,\"metadata\":metadata})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sematic reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sematic_reference_template=\"\"\"<|im_start|>system\n",
    "# Determine whether the two clauses are directly related in a way that they require each other to be fully understood. If one clause cannot be fully understood without the other, or if they directly complement each other in meaning, return \"yes\". If each clause can exist independently without losing essential meaning, return \"no\".\n",
    "\n",
    "# The response should be returned in following JSON format:\n",
    "# {{\n",
    "#   \"analysis\": \"A brief explanation of why they are related or not.\"\n",
    "#   \"is_related\": \"yes\"|\"no\",\n",
    "# }}\n",
    "# <|im_end|>\n",
    "\n",
    "# <|im_start|>user\n",
    "# Clause 1: {clause_1}\n",
    "# Clause 2: {clause_2}\n",
    "# <|im_end|>\n",
    "\n",
    "# <|im_start|>assistant\n",
    "# \"\"\"\n",
    "\n",
    "sematic_reference_template = \"\"\"<|im_start|>system\n",
    "You are a language model specializing in analyzing relationships between texts. Your task is to determine whether a referenced text (Referenced Text) provides additional meaning that helps in understanding the main text (Main Text).\n",
    "\n",
    "Task:\n",
    "Check whether the referenced text provides necessary additional information to better understand the main text.\n",
    "\n",
    "If the referenced text contains important supplementary information that clarifies, explains, or provides essential context for understanding the main text, respond with \"yes\".\n",
    "If the referenced text does not add significant meaning or merely repeats existing information, respond with \"no\".\n",
    "\n",
    "The response must be returned in following JSON format, return json only:\n",
    "{{\n",
    "  \"analysis\": \"A brief explanation of the relationship between the texts, must not include `\"`.\",\n",
    "  \"provide_additional_meaning\": \"yes\"|\"no\",\n",
    "  \n",
    "}}\n",
    "<|im_end|>\n",
    "\n",
    "<|im_start|>user\n",
    "Main Text: {main_text}\n",
    "Referenced Text: {referenced_text}\n",
    "<|im_end|>\n",
    "\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "sematic_reference_prompt = PromptTemplate.from_template(sematic_reference_template)\n",
    "sematic_reference_generator = sematic_reference_prompt | vllm_model| RunnableLambda(extract_json_from_string)\n",
    "\n",
    "main_text =\"\"\"解雇の予告期間は労働基準法に基づき、最低30日とするが、当社は40日間の通知期間を設ける。\"\"\"\n",
    "referenced_text =\"\"\"解雇の予告は労働基準法に準じ、30日以内とする。\"\"\"\n",
    "\n",
    "sematic_reference_generator.invoke({\"main_text\":main_text,\"referenced_text\":referenced_text})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rewrite with query and refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewrite_clause_template = \"\"\"<|im_start|>system\n",
    "You are a legal text processing expert. Your task is to merge two contract sections into a single coherent text while ensuring clarity, accuracy, and legal consistency.\n",
    "\n",
    "main_section: This is the primary contract section, which may reference specific clauses from another section.\n",
    "refer_section: This section contains detailed information referenced in main_section.\n",
    "Requirements:\n",
    "1.If the main_section references a specific clause in the refer_section, replace the reference with the actual content from the refer_section.\n",
    "2.Completely remove any reference phrases such as \"as defined in clause X\", \"according to article Y\", or any similar wording. The final text should only contain the actual content without referring back to any section or clause.\n",
    "3.Preserve the original meaning of the contract without altering its legal intent.\n",
    "4.Ensure the final merged text maintains a clear, structured, and legally sound format.\n",
    "5.The final \"generated_clause\" must be self-contained and must not contain any reference phrases even if reference phrases are present in the refer_sections.\n",
    "6.All newlines are represented using \"\\\\n\" instead of actual line breaks.\n",
    "The response must be in following JSON format.:\n",
    "{{\n",
    "  \"analysis\": \"Logical reasoning behind how the generated clause was created, including how references were replaced.\",\n",
    "  \"generated_section\": \"The fully rewritten section with all references replaced by full content, properly formatted and structured, without any reference phrases.Ensure that in the JSON response\"\n",
    "}}\n",
    "<|im_end|>\n",
    "\n",
    "<|im_start|>user\n",
    "main section:\n",
    "{main_section}\n",
    "refer sections:\n",
    "{refer_sections}\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "rewrite_clause_prompt = PromptTemplate.from_template(rewrite_clause_template)\n",
    "\n",
    "rewrite_clause_generator = rewrite_clause_prompt | vllm_model | RunnableLambda(extract_json_from_string)\n",
    "\n",
    "main_section = \"\"\"この規則は、本条第2項に規定するすべての労働者に適用する。また本規則における労働者とは本条第2項に規定するもののことを指す。\"\"\"\n",
    "\n",
    "refer_sections = \"\"\"第1章\\u3000総則 \\n 第2条（労働者の定義及び適用範囲）\\n2．この規則で労働者とは第2章に定める手続きにより入社した者で、次の通り定義する。 ①\\u3000正社員・・・・・・・契約社員・嘱託社員・パートタイマー・アルバイト・派遣社員以外の労働 者で期間の定めがなく雇用される基幹業務に従事する労働者 ②\\u3000勤務地限定社員・・・契約社員・アルバイト以外の労働者で期間の定めがなく勤務地が正社員と 比べ限定され雇用される労働者 ③\\u3000職務限定社員・・・・契約社員・アルバイト以外の労働者で期間の定めがなく職務が正社員と比 べ限定され雇用される労働者 ④\\u3000短時間正社員・・・・契約社員・嘱託社員・パートタイマー・アルバイト・派遣社員以外の労働 者で期間の定めがなくフルタイム勤務で働く正社員に比べ短い労働時間で 雇用される労働者 ⑤\\u3000無期契約社員・・・・正社員以外で無期労働契約により雇用される基幹業務の補助をする労働者 ⑥\\u3000有期契約社員・・・・有期労働契約により雇用される基幹業務の補助をする労働者。原則1年ご との更新とし、個別に定めるものとする。 ⑤\\u3000嘱託社員・・・・・・定年退職後に引き続き有期労働契約により雇用される労働者。原則1年ご との更新とし、個別に定めるものとする。 ⑥\\u3000パート／アルバイト\\u3000契約期間の有無にかかわらず、業務の一部や臨時的に業務を行う労働者 ⑦\\u3000派遣社員・・・・・・派遣元事業所より派遣され、派遣元の指揮命令を受け就業する労働者\"\"\"\n",
    "\n",
    "\n",
    "new_query = rewrite_clause_generator.invoke({\"main_section\": main_section, \"refer_sections\": refer_sections})\n",
    "print(new_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Prompt\n",
    "\n",
    "# template=\"\"\"<|im_start|>system\n",
    "# You are a legal analysis AI specializing in contract evaluation. Your task is to analyze a given contract segment and determine whether it violates, complies with, or lacks sufficient information to conclude compliance or violation based on the provided laws.\n",
    "# Inputs are Laws and Contract Segment.\n",
    "# Laws: A set of legal provisions relevant to the contract.\n",
    "# Contract Segment: A specific portion of a contract that needs evaluation.\n",
    "\n",
    "# Evaluation Criteria:\n",
    "# Compliance: The contract fully adheres to the relevant legal provisions.\n",
    "# Violation: The contract contradicts or does not meet the requirements of the legal provisions.\n",
    "# Insufficient Information: The given contract segment does not provide enough details to determine compliance or violation.\n",
    "# The response should be in Japanese and returned in following JSON format:\n",
    "# {{\n",
    "#   \"evaluation\": \"compliance\" | \"violation\" | \"insufficient_information\",\n",
    "#   \"explanation\": \"A brief but clear explanation of why the contract complies, violates, or lacks sufficient information.\",\n",
    "#   \"legal_references\": [\n",
    "#     {{\n",
    "#       \"law\": \"The specific law that was referenced\",\n",
    "#       \"relevance\": \"Explanation of how this law applies to the contract\"\n",
    "#     }}\n",
    "#   ],\n",
    "#   \"suggested_corrections\": \"Suggested modifications (if necessary) to ensure compliance.\"\n",
    "# }}\n",
    "# <|im_end|>\n",
    "\n",
    "# <|im_start|>user\n",
    "# Laws: [{context}].\n",
    "# Contract Segment: [{contract}]\n",
    "\n",
    "# <|im_end|>\n",
    "\n",
    "# <|im_start|>assistant\n",
    "# \"\"\"\n",
    "\n",
    "# template=\"\"\"<|im_start|>system\n",
    "# You are a legal analysis AI specializing in contract evaluation. Your task is to analyze a given contract segment and determine whether it violates, complies with, or lacks sufficient information to conclude compliance or violation based on the provided laws.\n",
    "# Inputs are Laws and Contract Segment.\n",
    "# Laws: A set of legal provisions relevant to the contract.\n",
    "# Contract Segment: A specific portion of a contract that needs evaluation.\n",
    "\n",
    "# Evaluation Criteria:\n",
    "# Compliance: The contract fully adheres to the relevant legal provisions.\n",
    "# Violation: The contract contradicts or does not meet the requirements of the legal provisions.\n",
    "# Insufficient Information: The given contract segment does not provide enough details to determine compliance or violation.\n",
    "# The response should be in Japanese and returned in following JSON format:\n",
    "# {{\n",
    "#   \"evaluation\": \"compliance\" | \"violation\" | \"insufficient_information\",\n",
    "#   \"explanation\": \"A brief but clear explanation of why the contract complies, violates, or lacks sufficient information.\",\n",
    "# }}\n",
    "# <|im_end|>\n",
    "\n",
    "# <|im_start|>user\n",
    "# Laws: [{context}].\n",
    "# Contract Segment: [{contract}]\n",
    "\n",
    "# <|im_end|>\n",
    "\n",
    "# <|im_start|>assistant\n",
    "# \"\"\"\n",
    "\n",
    "# prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# # Chain\n",
    "# rag_chain = prompt | vllm_model | RunnableLambda(extract_json_from_string)\n",
    "\n",
    "\n",
    "# contract=\"\"\"第5条（労働時間および休憩時間）\n",
    "\n",
    "# 従業員の労働時間は、1日8時間、週40時間を超えないものとする。\"\"\"\n",
    "# context=\"\"\"労働基準法 (労基法) 第32条 - 労働時間の制限\n",
    "# 使用者は、労働者に対し、1週間について40時間を超えて労働させてはならない。\n",
    "# 使用者は、労働者に対し、1日について8時間を超えて労働させてはならない。\n",
    "# 労働基準法 (労基法) 第34条 - 休憩時間\n",
    "# 使用者は、労働時間が6時間を超える場合は少なくとも45分、8時間を超える場合は少なくとも1時間の休憩時間を労働者に与えなければならない。\n",
    "# 休憩時間は、労働時間の途中に与えなければならず、自由に利用させなければならない。\"\"\"\n",
    "# # Run\n",
    "# generation = rag_chain.invoke({\"context\": context, \"contract\": contract})\n",
    "# print(generation)\n",
    "# # print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Template với cả query, context và new_query\n",
    "template_both = \"\"\"<|im_start|>system\n",
    "You are a legal AI specializing in contract evaluation. Your task is to analyze a given contract clause and determine whether it *complies with, violates, or lacks sufficient information* to conclude compliance based on the provided legal provisions.\n",
    "\n",
    "### *Inputs:*  \n",
    "- *Laws:* Relevant legal provisions related to the contract.  \n",
    "- *Contract Clause:* The specific contract segment to be evaluated.  \n",
    "- *Rewritten Contract Clause:* An alternative, clearer version of the contract clause for better understanding.\n",
    "\n",
    "### *Evaluation Criteria:*  \n",
    "1. *Compliance:* The contract fully adheres to the relevant legal provisions.  \n",
    "2. *Violation:* The contract contradicts or does not meet the legal requirements.  \n",
    "3. *Insufficient Information:* The provided contract clause lacks enough details to determine compliance or violation.  \n",
    "\n",
    "### *Response Format (in Japanese, JSON format):*  \n",
    "json\n",
    "{{\n",
    "  \"evaluation\": \"compliance\" | \"violation\" | \"insufficient_information\",\n",
    "  \"explanation\": \"A concise explanation of why the contract complies, violates, or lacks sufficient information.\"\n",
    "}}\n",
    "\n",
    "<|im_end|>\n",
    "\n",
    "<|im_start|>user\n",
    "Laws: [{context}].  \n",
    "Contract Clause: [{contract}].  \n",
    "Rewritten Contract Clause: [{new_query}].  \n",
    "<|im_end|>\n",
    "\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "# Tạo PromptTemplate\n",
    "prompt_both = PromptTemplate.from_template(template_both)\n",
    "\n",
    "# Chuỗi chạy RAG\n",
    "rag_chain_both = prompt_both | vllm_model | RunnableLambda(extract_json_from_string)\n",
    "\n",
    "# Dữ liệu đầu vào\n",
    "contract = \"\"\"第5条（労働時間および休憩時間）\n",
    "従業員の労働時間は、1日8時間、週40時間を超えないものとする。\"\"\"\n",
    "\n",
    "context = \"\"\"労働基準法 (労基法) 第32条 - 労働時間の制限\n",
    "使用者は、労働者に対し、1週間について40時間を超えて労働させてはならない。\n",
    "使用者は、労働者に対し、1日について8時間を超えて労働させてはならない。\n",
    "労働基準法 (労基法) 第34条 - 休憩時間\n",
    "使用者は、労働時間が6時間を超える場合は少なくとも45分、8時間を超える場合は少なくとも1時間の休憩時間を労働者に与えなければならない。\n",
    "休憩時間は、労働時間の途中に与えなければならず、自由に利用させなければならない。\"\"\"\n",
    "\n",
    "new_query = \"\"\"従業員は1日8時間以内、または週40時間以内の労働が求められる。\"\"\"\n",
    "\n",
    "# Chạy RAG chain\n",
    "generation = rag_chain_both.invoke({\"context\": context, \"contract\": contract, \"new_query\": new_query})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt template\n",
    "template_doc = \"\"\"<|im_start|>system\n",
    "You are a legal AI specializing in contract evaluation. Your task is to analyze a given contract clause and determine whether it *complies with, violates, or lacks sufficient information* to conclude compliance based on the provided legal provisions.  \n",
    "\n",
    "### *Inputs:*  \n",
    "- *Laws:* Relevant legal provisions related to the contract.  \n",
    "- *Contract Clause:* The specific contract segment to be evaluated.  \n",
    "\n",
    "### *Evaluation Criteria:*  \n",
    "1. *Compliance:* The contract fully adheres to the relevant legal provisions.  \n",
    "2. *Violation:* The contract contradicts or does not meet the legal requirements.  \n",
    "3. *Insufficient Information:* The provided contract clause lacks enough details to determine compliance or violation.  \n",
    "\n",
    "### *Response Format (in Japanese, JSON format):*  \n",
    "```json\n",
    "{{\n",
    "  \"evaluation\": \"compliance\" | \"violation\" | \"insufficient_information\",\n",
    "  \"explanation\": \"A concise explanation of why the contract complies, violates, or lacks sufficient information.\"\n",
    "}}\n",
    "<|im_end|>\n",
    "\n",
    "<|im_start|>user\n",
    "Laws: [{context}].\n",
    "Contract Segment: [{contract}]\n",
    "\n",
    "<|im_end|>\n",
    "\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "# Create prompt\n",
    "prompt_doc = PromptTemplate.from_template(template_doc)\n",
    "\n",
    "rag_chain_doc = prompt_doc | vllm_model | RunnableLambda(extract_json_from_string)\n",
    "\n",
    "# Example contract and laws\n",
    "contract = \"\"\"第5条（労働時間および休憩時間）\n",
    "\n",
    "従業員の労働時間は、1日8時間、週40時間を超えないものとする。\"\"\"\n",
    "context = \"\"\"労働基準法 (労基法) 第32条 - 労働時間の制限\n",
    "使用者は、労働者に対し、1週間について40時間を超えて労働させてはならない。\n",
    "使用者は、労働者に対し、1日について8時間を超えて労働させてはならない。\n",
    "労働基準法 (労基法) 第34条 - 休憩時間\n",
    "使用者は、労働時間が6時間を超える場合は少なくとも45分、8時間を超える場合は少なくとも1時間の休憩時間を労働者に与えなければならない。\n",
    "休憩時間は、労働時間の途中に与えなければならず、自由に利用させなければならない。\"\"\"\n",
    "\n",
    "# Run inference\n",
    "generation = rag_chain_doc.invoke({\"context\": context, \"contract\": contract})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### websearch grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retrieval Grader \n",
    "\n",
    "websearch_grader_template=\"\"\"<|im_start|>system\n",
    "You are an evaluator assessing whether a retrieved reference can serve as **direct legal evidence** to determine if a given contract segment complies with legal standards.\n",
    "\n",
    "### **Evaluation Criteria:**\n",
    "- Answer **\"yes\"** **only if** the retrieved reference is a law, regulation, or legally binding document that provides **direct evidence** for assessing the contract segment’s compliance or violation.\n",
    "- Answer **\"no\"** in all other cases, including:\n",
    "  - The reference is **not an actual law** (e.g., a general explanation, opinion, or guideline).\n",
    "  - The reference **does not directly address** the criteria mentioned in the contract segment.\n",
    "  - The reference is **too vague or general** to be used as clear legal evidence.\n",
    "\n",
    "Your response should be **strictly based on whether the reference can serve as legal proof**.\n",
    "\n",
    "### **Reasoning Steps:**\n",
    "Follow these steps to evaluate the relevance of the reference context:\n",
    "\n",
    "**Step 1: Understand the Contract Segment**\n",
    "- Read the contract segment carefully to identify its key terms, obligations, rights, or conditions.\n",
    "- Highlight the main requirements or claims in the segment. For example:\n",
    "  - What is being promised or agreed upon?\n",
    "  - Are there specific conditions, deadlines, or obligations?\n",
    "  - What legal standards or regulations might apply?\n",
    "\n",
    "**Step 2: Understand the Reference Context**\n",
    "- Read the reference context carefully to understand the information it provides.\n",
    "- Identify any legal provisions, rules, or facts that relate to the contract segment. For example:\n",
    "  - Does the reference context mention laws, regulations, or standards that apply to the contract segment?\n",
    "  - Does it provide evidence or context that supports or challenges the segment's compliance?\n",
    "\n",
    "**Step 3: Compare and Analyze**\n",
    "- Compare the reference context with the contract segment. Look for:\n",
    "  - **Alignment**: Does the reference context confirm or support the contract segment's compliance?\n",
    "  - **Contradiction**: Does the reference context contradict or disprove the contract segment's compliance?\n",
    "  - **Gaps**: Is the reference context insufficient or irrelevant to the contract segment?\n",
    "- Evaluate whether the reference context provides **direct and actionable evidence** to determine compliance or violation.\n",
    "\n",
    "### Expected Output:\n",
    "Provide your response in the following JSON format:\n",
    "{{\n",
    "  \"analysis\": \"your step-by-step reasoning and analysis\",\n",
    "  \"binary_score\": \"yes\" | \"no\"\n",
    "}}\n",
    "\n",
    "### Examples:\n",
    "#### Example 1:\n",
    "- Reference Context: \"According to Article 12 of the Labor Law, employees must be provided with a minimum of 12 days of paid annual leave.\"\n",
    "- Contract Segment: \"The company agrees to provide employees with 10 days of paid annual leave.\"\n",
    "- Analysis: \"The reference context states that the Labor Law requires a minimum of 12 days of paid annual leave, while the contract segment provides only 10 days. This directly contradicts the legal requirement, proving non-compliance.\"\n",
    "- Binary Score: \"yes\"\n",
    "\n",
    "#### Example 2:\n",
    "- Reference Context: \"According to Article 15 of the Tax Code, businesses must file their tax returns by March 31st each year.\"\n",
    "- Contract Segment: \"The company agrees to deliver the product by January 15, 2024.\"\n",
    "- Analysis: \"The reference context discusses tax filing deadlines, which is unrelated to the contract segment about product delivery. There is no direct link between the two.\"\n",
    "- Binary Score: \"no\"\n",
    "\n",
    "#### Example 3:\n",
    "- Reference Context: \"According to Article 20 of the Consumer Protection Law, sellers must provide a 30-day return policy for defective products.\"\n",
    "- Contract Segment: \"The seller agrees to provide a 14-day return policy for defective products.\"\n",
    "- Analysis: \"The reference context states that the Consumer Protection Law requires a 30-day return policy, while the contract segment provides only 14 days. This directly contradicts the legal requirement, proving non-compliance.\"\n",
    "- Binary Score: \"yes\"\n",
    "\n",
    "#### Example 4:\n",
    "- Reference Context: \"According to Article 5 of the Environmental Protection Act, companies must reduce carbon emissions by 20% by 2030.\"\n",
    "- Contract Segment: \"The company agrees to reduce carbon emissions by 15% by 2030.\"\n",
    "- Analysis: \"The reference context states that the Environmental Protection Act requires a 20% reduction in carbon emissions, while the contract segment commits to only 15%. This directly contradicts the legal requirement, proving non-compliance.\"\n",
    "- Binary Score: \"yes\"\n",
    "\n",
    "#### Example 5:\n",
    "- Reference Context: \"According to Article 8 of the Data Privacy Law, companies must obtain explicit consent before collecting personal data.\"\n",
    "- Contract Segment: \"The company agrees to deliver the product within 7 days of purchase.\"\n",
    "- Analysis: \"The reference context discusses data privacy requirements, which is unrelated to the contract segment about product delivery. There is no direct link between the two.\"\n",
    "- Binary Score: \"no\"\n",
    "\n",
    "<|im_end|>\n",
    "\n",
    "<|im_start|>user\n",
    "Reference Context: [{context}].\n",
    "Contract Segment: [{contract}].\n",
    "<|im_end|>\n",
    "\n",
    "<|im_start|>assistant\"\"\"\n",
    "\n",
    "websearch_grader_prompt = PromptTemplate.from_template(websearch_grader_template)\n",
    "\n",
    "# Chain\n",
    "websearch_grader = websearch_grader_prompt | vllm_model |RunnableLambda(extract_json_from_string)\n",
    "\n",
    "\n",
    "# generation = retrieval_grader.invoke({\"context\": \"\"\"この法律で「労働者」とは、職業の種類を問わず、事業又は事務所（以下「事業」という。）に使用される者で、賃金を支払われる者をいう。\n",
    "# \"\"\", \"contract\": \"\"\"**労働者の定義**\n",
    "\n",
    "# 本契約において、「労働者」とは、第2章に規定する手続きに従って入社した者であり、特定の定義に基づいて分類される者を指します。\"\"\"})\n",
    "generation = websearch_grader.batch([{\"context\": \"\"\"メニュー 会員登録 気になる 閲覧履歴 検索 転職情報を収集する 求人と出合う 企業と出合う 会員限定メニュー 【Career Pedia】転職実用辞典「キャリペディア」 内定取り消しとは？違法・認められる場合・対処法・判断基準を解説 更新日：2025年02月17日 監修者谷所\\u3000健一郎 キャリア・デベロップメント・アドバイザー（CDA）／有限会社キャリアドメイン\\u3000代表取締役 転職先から内定をもらったとしても、その後、内定取り消しにならないか不安になる方もいるのではないでしょうか。 内定取り消しになると転職活動の再開や生活設計の見直しが必要になるなど、内定者には大きな負担がかかります。 内定取り消しとはどのようなものか、違法性の有無、取り消しとなった場合の対処法について解説します。 内定取り消しとは 内定取り消しとは、企業側が一度出した内定を取り消すことです。 内定は、企業が求職者を正式に採用するための約束であり、企業が出した内定に求職者が合意し、内定が成立したタイミングで、企業と求職者の間に労働契約が成立します。 入社までにやむをえない事由が発生した場合、内定を取り消すことがあるといった条件付きの始期付解約権留保付労働契約が成立しますが、内定取り消しは事実上「解雇」と同じ意味を持ちます。 求職者を守るために、「解雇」の条件は法律で制限が設けられており、内定取り消しも同様です。 そのため、内定取り消しは企業側にも一定のリスクと義務が伴うため、簡単にできることではありません。 完全在宅勤務・フルリモートワークの求人\"\"\",\n",
    "                                      \"contract\": \"\"\"採用内定者が次の各号のいずれかに該当する場合は、内定を取り消し、採用しない。\\n採用内定後に傷病に罹患し、または採用選考時に過去の病歴、持病、および現在の治療中の傷病について健康告知を怠り、健康状態が業務に耐えられないと会社が判断したとき\"\"\"}]*10)\n",
    "print(generation)\n",
    "# print(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### retrieval grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retrieval Grader \n",
    "\n",
    "retrieval_grader_template=\"\"\"<|im_start|>system\n",
    "You are an evaluator assessing the relevance of a retrieved law to a given contract segment.\n",
    "Evaluation Criteria:\n",
    "Answer \"yes\" only if the retrieved law can be used as direct evidence to determine whether the contract segment complies with the law or violates the law.\n",
    "In all other cases, including where the law is not directly applicable or does not provide clear evidence of compliance or non-compliance, answer \"no\".\n",
    "The law must have a direct and actionable link to the contract segment's compliance or violation of legal standards.\n",
    "\n",
    "Your response should focus on whether the law can serve as direct evidence of the contract’s legal compliance or non-compliance.\n",
    "\n",
    "Provide a binary score to indicate relevance:\n",
    "\"yes\": The law provides direct evidence of the contract segment’s compliance or violation of the law.\n",
    "\"no\": The law does not provide direct evidence regarding compliance or violation.\n",
    "\n",
    "The response should be returned in following JSON format:\n",
    "{{\n",
    "  \"binary_score\": \"yes\" |\"no\"\n",
    "}}\n",
    "<|im_end|>\n",
    "\n",
    "<|im_start|>user\n",
    "Law: [{context}].\n",
    "Contract Segment: [{contract}]\n",
    "\n",
    "<|im_end|>\n",
    "\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "retrieval_grader_prompt = PromptTemplate.from_template(retrieval_grader_template)\n",
    "\n",
    "# Chain\n",
    "retrieval_grader = retrieval_grader_prompt | vllm_model |RunnableLambda(extract_json_from_string)\n",
    "\n",
    "\n",
    "# generation = retrieval_grader.invoke({\"context\": \"\"\"この法律で「労働者」とは、職業の種類を問わず、事業又は事務所（以下「事業」という。）に使用される者で、賃金を支払われる者をいう。\n",
    "# \"\"\", \"contract\": \"\"\"**労働者の定義**\n",
    "\n",
    "# 本契約において、「労働者」とは、第2章に規定する手続きに従って入社した者であり、特定の定義に基づいて分類される者を指します。\"\"\"})\n",
    "generation = retrieval_grader.batch([{\"context\": \"\"\"この法律で「労働者」とは、職業の種類を問わず、事業又は事務所（以下「事業」という。）に使用される者で、賃金を支払われる者をいう。\n",
    "\"\"\", \"contract\": \"\"\"**労働者の定義**\n",
    "\n",
    "本契約において、「労働者」とは、第2章に規定する手続きに従って入社した者であり、特定の定義に基づいて分類される者を指します。\"\"\"}]*50)\n",
    "\n",
    "print(generation)\n",
    "# print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hallucination grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retrieval Grader \n",
    "\n",
    "# hallucination_grader_template=\"\"\"<|im_start|>system\n",
    "# The input consists of laws and a generated explanation.\n",
    "# Your task is to determine whether the generated explanation is supported by the information in the laws.\n",
    "\n",
    "# ### Evaluation criteria:\n",
    "# - If the explanation is directly supported by the laws, return \"yes\".\n",
    "# - If the explanation is not supported by the laws, return \"no\".\n",
    "\n",
    "# The response must be returned in the following JSON format:\n",
    "# {{\"analysis\": \"A brief explanation of the relationship between the texts, must not include `\"`.\",\n",
    "#   \"is_supported\": \"yes\" or \"no\"\n",
    "# }}\n",
    "# <|im_end|>\n",
    "\n",
    "# <|im_start|>user\n",
    "# laws: [{laws}]  \n",
    "# generated explanation: [{explanation}]  \n",
    "# <|im_end|> \n",
    "# <|im_start|>assistant\n",
    "# \"\"\"\n",
    "# hallucination_grader_template=\"\"\"<|im_start|>system\n",
    "# The input consists of laws,contract and a generated explanation.\n",
    "# Your task is to determine whether the generated explanation strictly relies on the given laws for its content.\n",
    "\n",
    "# ### Evaluation criteria:\n",
    "# - If the explanation is fully supported by and directly derived from the laws, return \"yes\".\n",
    "# - If the explanation includes information not explicitly stated in the laws, return \"no\".\n",
    "\n",
    "# The response must be returned in the following JSON format:\n",
    "# {{\"analysis\": \"A brief explanation of whether the explanation strictly follows the laws\",\n",
    "#   \"is_supported\": \"yes\" or \"no\"\n",
    "# }}\n",
    "# <|im_end|>\n",
    "\n",
    "# <|im_start|>user\n",
    "# laws: [{laws}]  \n",
    "# contract: [{contract}]\n",
    "# generated explanation: [{explanation}]  \n",
    "# <|im_end|> \n",
    "# <|im_start|>assistant\n",
    "# \"\"\"\n",
    "hallucination_grader_template=\"\"\"<|im_start|>system\n",
    "The input consists of laws, a contract, and a generated explanation.  \n",
    "Your task is to determine whether the generated explanation strictly relies on the provided laws and contract without introducing new context.  \n",
    "\n",
    "### Evaluation criteria:  \n",
    "- If the explanation only uses information directly from the laws and contract, return \"yes\".  \n",
    "- The explanation may include a conclusion (e.g., stating whether the contract is correct, incorrect, or lacks enough information), and this is **acceptable** as long as the reasoning **before** the conclusion strictly follows the laws and contract.  \n",
    "- Return \"no\" **only if** the explanation introduces new context, makes assumptions, or includes information not explicitly stated in the laws and contract.  \n",
    "\n",
    "### Examples:\n",
    "\n",
    "#### Example 1 \n",
    "**Laws:** \"Employers must give at least 30 days’ notice before termination.\"  \n",
    "**Contract:** \"Employees must be given at least 40 days notice before termination.\"  \n",
    "**Generated Explanation:**  \n",
    "\"The contract states that employees must be given at least 40 days' notice before termination, which is stricter than the law requiring 30 days. This means the contract is legally valid but more protective than the minimum legal requirement.\"  \n",
    "\n",
    "*Analysis: The explanation strictly follows the laws and contract without adding new information. The conclusion is based entirely on the provided text.*  \n",
    "*is_supported: yes\n",
    "Your response must be in the following JSON format:  \n",
    "{{\n",
    "  \"analysis\": \"A brief explanation of whether the generated explanation strictly follows the laws and contract.\",\n",
    "  \"is_supported\": \"yes\" or \"no\"\n",
    "}}\n",
    "<|im_end|>\n",
    "\n",
    "<|im_start|>user\n",
    "laws: [{laws}]  \n",
    "contract: [{contract}]  \n",
    "generated explanation: [{explanation}]  \n",
    "<|im_end|>  \n",
    "<|im_start|>assistant\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "hallucination_grader_prompt = PromptTemplate.from_template(hallucination_grader_template)\n",
    "\n",
    "# Chain\n",
    "hallucination_grader = hallucination_grader_prompt | vllm_model|RunnableLambda(extract_json_from_string)\n",
    "\n",
    "contract = \"\"\"使用者は、従業員を解雇する場合、解雇の少なくとも40日前にその旨を通知しなければならない。\"\"\"\n",
    "\n",
    "explanation = \"\"\"本契約条項は、労働基準法第二十条に基づいて、雇用主が従業員を解雇する場合、少なくとも30日前に通知する必要があると規定しているが、本契約条項では40日前に通知する必要があると規定しているため、誤りである。\"\"\"\n",
    "\n",
    "laws=\"\"\"昭和二十二年法律第四十九号\\u3000労働基準法/第二章\\u3000労働契約/第二十条\\u3000解雇の予告/使用者は、労働者を解雇しようとする場合においては、少くとも三十日前にその予告をしなければならない。三十日前に予告をしない使用者は、三十日分以上の平均賃金を支払わなければならない。但し、天災事変その他やむを得ない事由のために事業の継続が不可能となつた場合又は労働者の責に帰すべき事由に基いて解雇する場合においては、この限りでない。\\n前項の予告の日数は、一日について平均賃金を支払つた場合においては、その日数を短縮することができる。\\n前条第二項の規定は、第一項但書の場合にこれを準用する。\"\"\"\n",
    "\n",
    "generation = hallucination_grader.batch([{\"laws\": laws,\"contract\":contract, \"explanation\": explanation}]*10)\n",
    "print(generation)\n",
    "# print(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### answer grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_grader_template=\"\"\"<|im_start|>system\n",
    "The input consists of a contract and a generated explanation.\n",
    "Your task is to determine whether the generated explanation truly explains the contract.\n",
    "\n",
    "### Evaluation criteria:\n",
    "- If the explanation correctly interprets or clarifies the contract, return \"yes\".\n",
    "- If the explanation does not accurately explain the contract, return \"no\".\n",
    "\n",
    "The response must be returned in the following JSON format:\n",
    "{{\n",
    "  \"is_valid_explanation\": \"yes\" or \"no\",\n",
    "}}\n",
    "<|im_end|>\n",
    "\n",
    "<|im_start|>user\n",
    "contract: [{contract}]  \n",
    "generated explanation: [{explanation}]  \n",
    "<|im_end|> \n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "answer_grader_prompt = PromptTemplate.from_template(answer_grader_template)\n",
    "\n",
    "# Chain\n",
    "answer_grader = answer_grader_prompt | vllm_model|RunnableLambda(extract_json_from_string)\n",
    "explanation = \"\"\"契約条項によると、労働時間は1日8時間、週40時間以内と定められています。  \n",
    "また、休憩時間については、労働時間が6時間を超え8時間以下の場合、本契約では具体的な時間が記載されておらず、後で通知されることになっています。  \n",
    "そのため、この範囲の労働時間に関する休憩時間は現時点では未定ですが、契約上は通知されることが明示されています。  \n",
    "さらに、8時間を超える場合には1時間の休憩があると規定されています。\n",
    "\"\"\"\n",
    "contract =\"\"\"本契約に基づき、従業員の労働時間は1日8時間、週40時間以内とする。  \n",
    "ただし、休憩時間については、労働時間が6時間を超え8時間以下の場合は後で通知される。  \n",
    "8時間を超える場合、休憩時間は1時間とする。\"\"\"\n",
    "\n",
    "generation = answer_grader.batch([{\"contract\": contract, \"explanation\": explanation}]*3)\n",
    "print(generation)\n",
    "# print(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### query transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kết hợp paraphrase + Granularity Adjustment + query expansion\n",
    "query_transformation_template=\"\"\"<|im_start|>system\n",
    "You are a natural language processing expert. Your task is to transform the input query into different variations based on the following criteria:  \n",
    "### Instructions:  \n",
    "1. **Paraphrasing (1 variations):** Rewrite the query in a different way while keeping the meaning unchanged.  \n",
    "2. **Granularity Adjustment - More Detailed (1 variations):** Rewrite the query with a higher level of detail by adding specific elements without changing its meaning.  \n",
    "3. **Granularity Adjustment - Less Detailed (1 variations):** Rewrite the query in a more generalized way by removing details while maintaining the overall meaning.  \n",
    "4. **Query Expansion (1 variations):** Expand the query by adding relevant information to make it more comprehensive.  \n",
    "\n",
    "The response must be returned in the following JSON format:\n",
    "{{\n",
    "  \"paraphrased_queries\": [\n",
    "    \"Paraphrased query\"\n",
    "  ],\n",
    "  \"detailed_queries\": [\n",
    "    \"More detailed query\"\n",
    "  ],\n",
    "  \"generalized_queries\": [\n",
    "    \"More generalized query\"\n",
    "  ],\n",
    "  \"expanded_queries\": [\n",
    "    \"Expanded query\"\n",
    "  ]\n",
    "}}\n",
    "<|im_end|>\n",
    "\n",
    "<|im_start|>user\n",
    "Input Query:{query}  \n",
    "<|im_end|> \n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "query_transformation_prompt = PromptTemplate.from_template(query_transformation_template)\n",
    "\n",
    "# Chain\n",
    "query_transformation_generator = query_transformation_prompt | vllm_model|RunnableLambda(extract_json_from_string)\n",
    "\n",
    "test =[{'query': '変更事項は以下の書類の記載事項や次の事項を含む：①履歴書・職務経歴書、②入社誓約書、③個人情報保護法に基づく誓約書、④住民票記載事項証明書、⑤給与所得者の扶養控除等（異動）申告書、⑥源泉徴収票、⑦雇用契約書、⑧年金手帳の写し、⑨通勤手当申請書、⑩住宅手当申請書、⑪給与振込先申請書、⑫身元保証書、⑬雇用保険被保険者証の写し、⑭マイナンバーカード又はマイナンバー通知書の写し、⑮資格免許証の写し、⑯健康診断結果、⑰学生証の写し、⑱パスポートの写し、⑲その他会社が必要と認めたもの、氏名。'}, {'query': '変更事項には、1 履歴書・職務経歴書、2 入社誓約書、3 個人情報保護法に基づく誓約書、4 住民票記載事項証明書、5 給与所得者の扶養控除等（異動）申告書、6 源泉徴収票、7 雇用契約書、8 年金手帳の写し、9 通勤手当申請書、10 住宅手当申請書、11 給与振込先申請書、12 身元保証書、13 雇用保険被保険者証の写し、14 マイナンバーカード又はマイナンバー通知書の写し、15 資格免許証の写し、16 健康診断結果、17 学生証の写し、18 パスポートの写し、19 その他会社が必要と認めたもの、学歴、資格、免許が含まれる。'}]\n",
    "# generation = query_transformation_generator.invoke({\"query\": \"勤務地限定社員とは契約社員、アルバイト以外の労働者で期間の定めがなく勤務地が正社員と比べて限定され雇用される労働者を指す。\"})\n",
    "generation = query_transformation_generator.batch(test)\n",
    "print(generation)\n",
    "# print(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### question re-writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "question_re_writer_template=\"\"\"<|im_start|>system\n",
    "You are an expert in query rewriting for improving information retrieval. Your task is to rewrite the given user query to make it more effective for retrieving relevant documents from a vectorstore or search engine.\n",
    "\n",
    "Guidelines:\n",
    "Clarify vague terms: If the original query is too broad or ambiguous, make it more specific.\n",
    "Expand missing context: If the query lacks key details, infer and add relevant information based on intent.\n",
    "Use precise terminology: Replace general words with domain-specific terms when applicable.\n",
    "Avoid over-restricting: Ensure the rewritten query does not remove useful flexibility in retrieval.\n",
    "\n",
    "The response must be returned in the following JSON format:\n",
    "{{\n",
    "\"rewritten_query\": str\n",
    "}}\n",
    "\n",
    "\n",
    "<|im_end|>\n",
    "\n",
    "<|im_start|>user\n",
    "\"original_query\": {original_query},\n",
    "<|im_end|> \n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "question_re_writer_prompt = PromptTemplate.from_template(question_re_writer_template)\n",
    "\n",
    "# Chain\n",
    "question_rewriter = question_re_writer_prompt | vllm_model|RunnableLambda(extract_json_from_string)\n",
    "retrieved_results=[]\n",
    "generation = question_rewriter.invoke({\"original_query\": contract, \"retrieved_results\": json.dumps(retrieved_results)})\n",
    "print(generation)\n",
    "# print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Web Search Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Search\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "web_search_tool = TavilySearchResults(max_results=1,include_images=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {'query': 'この規則は、第2章に定める手続きにより入社し、次の通り定義される労働者に適用する。\\n勤務地限定社員・・・契約社員・アルバイト以外の労働者で期間の定めがなく勤務地が正社員と比べ限定され雇用される労働者\\n短時間正社員・・・・契約社員・嘱託社員・パートタイマー・アルバイト・派遣社員以外の労働者で期間の定めがなくフルタイム勤務で働く正社員に比べ短い労働時間で雇用される労働者\\n正社員・・・・・・・契約社員・嘱託社員・パートタイマー・アルバイト・派遣社員以外の労働者で期間の定めがなく雇用される基幹業務に従事する労働者\\n職務限定社員・・・・契約社員・アルバイト以外の労働'}\n",
    "print(len(query[\"query\"]))\n",
    "web_search_tool.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "# Load lại Embedding function\n",
    "embedding_function = HuggingFaceEmbeddings(model_name=\"intfloat/multilingual-e5-large\")\n",
    "\n",
    "# Load Vector Store từ thư mục đã lưu\n",
    "vector_store = Chroma(persist_directory=\"/home/thanhnguyen/Data/code/RAG_techniques/vectorDB\", embedding_function=embedding_function)\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 4})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_documents = len(vector_store)\n",
    "# print(f\"Total documents in vector store: {num_documents}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### dense retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### init\n",
    "\n",
    "# import json\n",
    "# import json\n",
    "# import hashlib\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# with open(\"/home/thanhnguyen/Data/code/RAG_techniques/Phuc/full_corpus_110225_with_metadata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     data_docs = json.load(f)\n",
    "\n",
    "# from langchain_core.documents import Document\n",
    "\n",
    "# docs = []\n",
    "# for doc in data_docs:\n",
    "#     docs.append(\n",
    "#         Document(\n",
    "#             page_content=doc[\"content\"],\n",
    "#             metadata=doc[\"metadata\"]\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "# unique_documents = []\n",
    "# seen = set()\n",
    "\n",
    "# for document in docs:\n",
    "#     # Truy cập metadata và content từ đối tượng Document\n",
    "#     metadata = document.metadata\n",
    "#     content = document.page_content\n",
    "    \n",
    "#     # Chuyển đổi metadata và content thành chuỗi JSON để so sánh\n",
    "#     doc_str = json.dumps({\"metadata\": metadata, \"content\": content}, sort_keys=True)\n",
    "    \n",
    "#     # Tạo một hash MD5 cho chuỗi JSON này\n",
    "#     doc_hash = hashlib.md5(doc_str.encode('utf-8')).hexdigest()\n",
    "    \n",
    "#     # Nếu tài liệu chưa gặp, thêm vào list unique_documents\n",
    "#     if doc_hash not in seen:\n",
    "#         unique_documents.append(document)\n",
    "#         seen.add(doc_hash)\n",
    "\n",
    "# # Kết quả là danh sách không có phần tử lặp\n",
    "# print(\"len unique_documents\",len(unique_documents))\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=30)\n",
    "# chunks = text_splitter.split_documents(unique_documents)\n",
    "# embedding_function = HuggingFaceEmbeddings(model_name=\"intfloat/multilingual-e5-large\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### using dense retrieval\n",
    "# import numpy as np\n",
    "# import json\n",
    "# import torch\n",
    "# import pickle\n",
    "# from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "\n",
    "# class RetrievalData:\n",
    "#     def __init__(self, legal_dict_json, model_paths, legal_data_path, weighted, top_n=20, range_score=0.2, encode_legal_data_flag=False):\n",
    "#         self.legal_dict_json = legal_dict_json\n",
    "#         self.legal_data_path = legal_data_path\n",
    "#         self.weighted = weighted\n",
    "#         self.top_n = top_n\n",
    "#         self.range_score = range_score\n",
    "#         self.models = self.load_models(model_paths)\n",
    "\n",
    "#         if encode_legal_data_flag:\n",
    "#             self.emb_legal_data = self.encode_legal_data()\n",
    "#             with open(self.legal_data_path, \"wb\") as f:\n",
    "#                 pickle.dump(self.emb_legal_data, f)\n",
    "#         else:\n",
    "#             with open(self.legal_data_path, \"rb\") as f:\n",
    "#                 self.emb_legal_data = pickle.load(f)\n",
    "\n",
    "#     def load_models(self, model_paths):\n",
    "#         return [SentenceTransformer(model_path) for model_path in model_paths]\n",
    "\n",
    "#     def encode_legal_data(self):\n",
    "#         doc_data = json.load(open(self.legal_dict_json, encoding='utf-8'))\n",
    "#         list_emb_models = []\n",
    "#         for model in self.models:\n",
    "#             emb2_list = [model.encode(doc[\"content\"]) for doc in doc_data]\n",
    "#             list_emb_models.append(np.array(emb2_list))\n",
    "#         return list_emb_models\n",
    "\n",
    "#     def encode_question(self, question_data):\n",
    "#         return [model.encode(question_data) for model in self.models]\n",
    "\n",
    "#     def calculate_cosine_similarity(self, question_embs, list_embed=None):\n",
    "#         cos_sim = []\n",
    "#         if list_embed is not None:\n",
    "#             print(\"vao no embed\")\n",
    "#             for idx, emb_legal in enumerate(list_embed):\n",
    "#                 scores = util.cos_sim(torch.tensor(question_embs[idx]), torch.tensor(emb_legal))\n",
    "#                 cos_sim.append(self.weighted[idx] * scores)\n",
    "#         else:\n",
    "#             print(\"vao yes embed\")\n",
    "#             for idx, emb_legal in enumerate(self.emb_legal_data):\n",
    "#                 scores = util.cos_sim(torch.tensor(question_embs[idx]), torch.tensor(emb_legal))\n",
    "#                 cos_sim.append(self.weighted[idx] * scores)\n",
    "#         cos_sim = torch.cat(cos_sim, dim=0)\n",
    "#         # Dùng flatten() thay vì squeeze(0) để đảm bảo kết quả là 1-d array.\n",
    "#         cos_sim = torch.sum(cos_sim, dim=0).flatten().numpy()\n",
    "#         return cos_sim\n",
    "\n",
    "#     def process_predictions(self, cos_sim):\n",
    "#         # Đảm bảo cos_sim có dạng ít nhất là 1-d array\n",
    "#         cos_sim = np.atleast_1d(cos_sim)\n",
    "#         max_score = np.max(cos_sim)\n",
    "#         # Sử dụng số lượng phần tử tối đa là min(top_n, len(cos_sim))\n",
    "#         k = min(self.top_n, len(cos_sim))\n",
    "#         predictions = np.argpartition(cos_sim, len(cos_sim) - k)[-k:]\n",
    "#         new_scores = cos_sim[predictions]\n",
    "#         new_predictions = np.where(new_scores >= (max_score - self.range_score))[0]\n",
    "#         map_ids = predictions[new_predictions]\n",
    "#         new_scores = new_scores[new_scores >= (max_score - self.range_score)]\n",
    "#         if new_scores.shape[0] > k:\n",
    "#             predictions_2 = np.argpartition(new_scores, len(new_scores) - k)[-k:]\n",
    "#             map_ids = map_ids[predictions_2]\n",
    "#         return map_ids, new_scores\n",
    "\n",
    "#     def display_results(self, map_ids):\n",
    "#         dup_ans = []\n",
    "#         with open(self.legal_dict_json, 'r', encoding='utf-8') as file:\n",
    "#             doc_data = json.load(file)\n",
    "#         for idx_pred in map_ids:\n",
    "#             law_id_article = doc_data[idx_pred][\"id\"]\n",
    "#             content = doc_data[idx_pred][\"content\"]\n",
    "#             if law_id_article not in dup_ans:\n",
    "#                 dup_ans.append(law_id_article)\n",
    "#                 print(f'Law and Article ID: {law_id_article}')\n",
    "#                 print(f'Text: {content}')\n",
    "#                 print(\"=\" * 80)\n",
    "\n",
    "#     def inference(self, question, list_embed=None):\n",
    "#         question_embs = self.encode_question(question)\n",
    "#         cos_sim = self.calculate_cosine_similarity(question_embs, list_embed)\n",
    "#         map_ids, new_scores = self.process_predictions(cos_sim)\n",
    "#         return map_ids[:5], new_scores[:5]\n",
    "#         # self.display_results(map_ids)\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     model_paths = [\"/home/hoangphuc/Nhap/RAG_Pb/saved_model_finetune_mlm_lms_round2\"]\n",
    "#     legal_dict_json = \"/home/hoangphuc/Nhap/RAG_Pb/generated_data/full_corpus_110225_with_metadata_new.json\"\n",
    "#     legal_data_path = \"/home/hoangphuc/Nhap/RAG_Pb/generated_data/vectorDB_full_corpus_110225.pkl\"\n",
    "#     # weighted = [0.3, 0.2, 0.5]\n",
    "#     weighted = [1.0]\n",
    "\n",
    "#     retrieval = RetrievalData(legal_dict_json, model_paths, legal_data_path, weighted, encode_legal_data_flag = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def display_results(map_ids, new_scores):\n",
    "#     dup_ans = []\n",
    "#     with open(legal_dict_json, 'r', encoding='utf-8') as file:\n",
    "#         doc_data = json.load(file)\n",
    "#     for idx_pred in map_ids:\n",
    "#         law_id_article = doc_data[idx_pred][\"id\"]\n",
    "#         content = doc_data[idx_pred][\"content\"]\n",
    "#         if law_id_article not in dup_ans:\n",
    "#             dup_ans.append(law_id_article)\n",
    "#             print(f'Law and Article ID: {law_id_article}')\n",
    "#             print(f'Text: {content}')\n",
    "#             print(\"=\" * 80)\n",
    "# def return_documents(map_ids, new_scores):\n",
    "#     dup_ans = []\n",
    "#     with open(legal_dict_json, 'r', encoding='utf-8') as file:\n",
    "#         doc_data = json.load(file)\n",
    "#     test_docs = []\n",
    "#     for idx_pred in map_ids:\n",
    "#         law_id_article = doc_data[idx_pred][\"id\"]\n",
    "#         content = doc_data[idx_pred][\"content\"]\n",
    "#         metadata = doc_data[idx_pred][\"metadata\"]\n",
    "\n",
    "#         if law_id_article not in dup_ans:\n",
    "#             test_docs.append(\n",
    "#                 Document(\n",
    "#                     page_content=content,\n",
    "#                     metadata=metadata\n",
    "#                 )\n",
    "#             )\n",
    "#             dup_ans.append(law_id_article)\n",
    "#     return test_docs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # question = input(\"\\nEnter a contract sentence: \").strip()\n",
    "# question = \"雇用主が従業員を解雇する場合、雇用契約で定められている通知期間は、40日以上である必要があるのか？\"\n",
    "# map_ids, new_scores = retrieval.inference(question)\n",
    "# documents = return_documents(map_ids, new_scores)\n",
    "# # display_results(map_ids, new_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langchain_core.documents import Document\n",
    "from typing import List, Dict, Union, Tuple\n",
    "\n",
    "class Proposition(TypedDict):\n",
    "    proposition_content: str # sửa khi trích metadata\n",
    "    documents: List[Document]  # Sử dụng Document thay vì List[str]\n",
    "    filtered_documents: List[Document]\n",
    "\n",
    "class reference(TypedDict):\n",
    "    text: str\n",
    "    index_list: List[int]\n",
    "    \n",
    "class Question(TypedDict):\n",
    "    question_content: Document\n",
    "    rewrite_question_content: str # rewrite with ref\n",
    "    propositions: List[Proposition]\n",
    "    ref_propositions: List[Proposition]\n",
    "    direct_reference:List[Document]\n",
    "    direct_reference_text:List[reference]\n",
    "    direct_reference_index: List[int]\n",
    "    merged_reference_text: str\n",
    "    generation: str\n",
    "    all_docs: List[Document] \n",
    "    web_search_docs: List[Document]\n",
    "    filtered_web_search_docs: List[Document]\n",
    "    # regen_count: int\n",
    "\n",
    "\n",
    "class propositions_to_rewrite(TypedDict):\n",
    "    question_index: int\n",
    "    proposition_index: int\n",
    "    ref_propositions_index: int\n",
    "    original_proposition: Proposition\n",
    "    \n",
    "class RootText(TypedDict):\n",
    "    Chapter: Dict[int, str]\n",
    "    Article: Dict[int, str]\n",
    "    clause: Dict[Tuple[int, int], str]\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    questions: List[Question]  # Lưu danh sách các câu hỏi thay vì chỉ một câu hỏi\n",
    "    processed_question_index: List[int]\n",
    "    current_question_index: List[int]\n",
    "    propositions_to_rewrite : List[propositions_to_rewrite]\n",
    "    rewrite_count :int # rewrite proposition\n",
    "    regen_count :int #\n",
    "    question_to_regen_index: List[int]#\n",
    "    root_text: RootText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from langchain.schema import Document\n",
    "from pathlib import Path\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### direct reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def direct_reference(state):\n",
    "#     print(\"---DIRECT REFERENCE---\")\n",
    "#     \"\"\"\n",
    "#     Lặp qua các proposition để trích xuất phần reference, sau đó lọc các proposition \n",
    "#     thật sự liên quan bằng LLM, rồi cập nhật lại state với thông tin tham chiếu.\n",
    "#     \"\"\"\n",
    "#     batch_inputs = []\n",
    "#     batch_map = []  # Mỗi phần tử là (q_idx, p_idx)\n",
    "#     new_graph_state = {\"questions\": []}\n",
    "#     # Tạo batch_inputs cho LLM\n",
    "#     for q_idx, question in enumerate(state[\"questions\"]):\n",
    "#         question_content = question[\"question_content\"]\n",
    "#         # Lấy metadata từ đối tượng, nếu không có thì trả về None\n",
    "#         question_metadata = getattr(question_content, \"metadata\", None)\n",
    "#         for p_idx, proposition in enumerate(question[\"propositions\"]):\n",
    "#             prop_content = proposition[\"proposition_content\"]\n",
    "#             prop_metadata = getattr(prop_content, \"metadata\", None)\n",
    "#             metadata_parts = []\n",
    "#             # Sử dụng getattr để lấy thuộc tính nếu tồn tại\n",
    "#             chapter = getattr(question_metadata, \"chapter_number\", None) if question_metadata else None\n",
    "#             if chapter:\n",
    "#                 metadata_parts.append(f\"current chapter: {chapter}\")\n",
    "#             article = getattr(question_metadata, \"article_number\", None) if question_metadata else None\n",
    "#             if article:\n",
    "#                 metadata_parts.append(f\"current article: {article}\")\n",
    "#             clause = getattr(question_metadata, \"clause_number\", None) if question_metadata else None\n",
    "#             if clause:\n",
    "#                 metadata_parts.append(f\"current clause: {clause}\")\n",
    "#             sub_clause = getattr(prop_metadata, \"sub_clause_number\", None) if prop_metadata else None\n",
    "#             if sub_clause:\n",
    "#                 metadata_parts.append(f\"current sub-clause: {sub_clause}\")\n",
    "#             entry = {\"document\": prop_content}\n",
    "#             entry[\"metadata\"] = \", \".join(metadata_parts)\n",
    "#             batch_inputs.append(entry)\n",
    "#             batch_map.append((q_idx, p_idx))\n",
    "#     # Gọi LLM để trích xuất reference\n",
    "#     responses = extract_reference_generator.batch(batch_inputs) if batch_inputs else []\n",
    "#     # Tạo state mới\n",
    "#     for q_idx, question in enumerate(state[\"questions\"]):\n",
    "#         new_question = {\n",
    "#             \"question_content\": question[\"question_content\"],\n",
    "#             \"propositions\": []\n",
    "#         }\n",
    "#         for p_idx, proposition in enumerate(question[\"propositions\"]):\n",
    "#             new_proposition = proposition.copy()\n",
    "#             new_proposition[\"direct_reference\"] = []\n",
    "#             new_proposition[\"semantic_reference\"] = []\n",
    "#             # Xử lý phản hồi từ LLM\n",
    "#             response = responses[batch_map.index((q_idx, p_idx))]\n",
    "#             if response['has_reference'] == \"yes\":\n",
    "#                 print(\"=========================\")\n",
    "#                 print(f\"response {response}\")\n",
    "#                 print(f\"proposition {proposition}\")\n",
    "#                 print(f\"\"\"question {question[\"question_content\"]}\"\"\")\n",
    "#             if response['has_reference'] == \"yes\" and response[\"is_extractable\"] == \"yes\":\n",
    "#                 for reference in response[\"references\"]:\n",
    "#                     if reference[\"resolved\"][\"Chapter\"]==-1 and reference[\"resolved\"][\"Article\"]==-1 and reference[\"resolved\"][\"clause\"]==-1 and reference[\"resolved\"][\"sub_clause\"]==-1:\n",
    "#                         continue\n",
    "#                     for ref_q in state[\"questions\"]:\n",
    "#                             if (reference[\"resolved\"][\"Chapter\"]==-1 or ref_q[\"question_content\"].metadata[\"chapter_number\"] == reference[\"resolved\"][\"Chapter\"]) and \\\n",
    "#                             (reference[\"resolved\"][\"Article\"]==-1 or ref_q[\"question_content\"].metadata[\"article_number\"] == reference[\"resolved\"][\"Article\"]) and \\\n",
    "#                             (reference[\"resolved\"][\"clause\"]==-1 or ref_q[\"question_content\"].metadata[\"clause_number\"] == reference[\"resolved\"][\"clause\"]):\n",
    "#                                 for ref_p in ref_q[\"propositions\"]:\n",
    "#                                     if reference[\"resolved\"][\"sub_clause\"] in ref_p[\"proposition_content\"].metadata[\"sub_clause_number\"] or reference[\"resolved\"][\"sub_clause\"]==-1:\n",
    "#                                             ref_p[\"proposition_content\"].metadata[\"chapter_number\"]=ref_q[\"question_content\"].metadata[\"chapter_number\"]\n",
    "#                                             ref_p[\"proposition_content\"].metadata[\"chapter_title\"]=ref_q[\"question_content\"].metadata[\"chapter_title\"]\n",
    "#                                             ref_p[\"proposition_content\"].metadata[\"article_number\"]=ref_q[\"question_content\"].metadata[\"article_number\"]\n",
    "#                                             ref_p[\"proposition_content\"].metadata[\"article_title\"]=ref_q[\"question_content\"].metadata[\"article_title\"]\n",
    "#                                             ref_p[\"proposition_content\"].metadata[\"clause_number\"]=ref_q[\"question_content\"].metadata[\"clause_number\"]\n",
    "#                                             new_proposition[\"direct_reference\"].append(ref_p[\"proposition_content\"])\n",
    "                            \n",
    "#             new_question[\"propositions\"].append(new_proposition)\n",
    "#         new_graph_state[\"questions\"].append(new_question)\n",
    "#     return new_graph_state\n",
    "\n",
    "def direct_reference(state):\n",
    "    level_order = [\"Chapter\", \"Article\", \"clause\", \"sub_clause\"]\n",
    "    level_dict={\"Chapter\":\"chapter_number\",\"Article\":\"article_number\",\"clause\":\"clause_number\",\"sub_clause\":\"sub_clause_number\"}\n",
    "\n",
    "    def get_min_level(metadata):\n",
    "        \"\"\"Xác định mức chỉ số nhỏ nhất khác None trong metadata.\"\"\"\n",
    "        # Duyệt ngược từ mức cao nhất xuống mức thấp nhất\n",
    "        # print(\"get_min_level\")\n",
    "        for level in reversed(level_order):\n",
    "            if str(metadata[level]) !='-1':\n",
    "                # print(f\"level: {level}\")\n",
    "                return level\n",
    "        return \"Chapter\"\n",
    "    def find_sub_refer(refer):\n",
    "        # print(f\"refer {refer}\")\n",
    "        min_level = get_min_level(refer)\n",
    "        next_min_level=level_order[level_order.index(min_level)+1]\n",
    "        # print(f\"min_level {min_level}\")\n",
    "        # print(f\"next_min_level {next_min_level}\")\n",
    "        sub_refer = set()\n",
    "        for doc in state[\"questions\"]:\n",
    "            # kiểm tra lọc doc từ chapter đến min_level\n",
    "            check=True\n",
    "            if str(refer['Article'])!='-1':\n",
    "                for level in level_order[1:level_order.index(min_level)+1]:\n",
    "                    # print(f\"level_dict[level] {level_dict[level]}\")\n",
    "                    # print(f\"\"\"doc[\"question_content\"].metadata {doc[\"question_content\"].metadata}\"\"\")\n",
    "                    # print(f\"\"\"doc[\"question_content\"].metadata[level_dict[level]] {doc[\"question_content\"].metadata[level_dict[level]]}\"\"\")\n",
    "                    # print(f\"\"\"refer[level] {refer[level]}\"\"\")\n",
    "                    # print(f\"\"\"level_dict[level] in doc[\"question_content\"].metadata {level_dict[level] in doc[\"question_content\"].metadata}\"\"\")\n",
    "                    # print(f\"\"\"doc[\"question_content\"].metadata[level_dict[level]] != refer[level] {doc[\"question_content\"].metadata[level_dict[level]] != str(refer[level])}\"\"\")\n",
    "                    if level_dict[level] in doc[\"question_content\"].metadata and doc[\"question_content\"].metadata[level_dict[level]] != str(refer[level]):\n",
    "                        check=False\n",
    "                        break\n",
    "            else:\n",
    "                for level in level_order[:level_order.index(min_level)+1]:\n",
    "                    # print(f\"level_dict[level] {level_dict[level]}\")\n",
    "                    # print(f\"\"\"doc[\"question_content\"].metadata {doc[\"question_content\"].metadata}\"\"\")\n",
    "                    # print(f\"\"\"doc[\"question_content\"].metadata[level_dict[level]] {doc[\"question_content\"].metadata[level_dict[level]]}\"\"\")\n",
    "                    # print(f\"\"\"refer[level] {refer[level]}\"\"\")\n",
    "                    # print(f\"\"\"level_dict[level] in doc[\"question_content\"].metadata {level_dict[level] in doc[\"question_content\"].metadata}\"\"\")\n",
    "                    # print(f\"\"\"doc[\"question_content\"].metadata[level_dict[level]] != refer[level] {doc[\"question_content\"].metadata[level_dict[level]] != str(refer[level])}\"\"\")\n",
    "                    if level_dict[level] in doc[\"question_content\"].metadata and doc[\"question_content\"].metadata[level_dict[level]] != str(refer[level]):\n",
    "                        check=False\n",
    "                        break\n",
    "            if check:\n",
    "                # append vào sub_refer từ chapter đến next_min_level\n",
    "                dict_refer = {}\n",
    "                # print(f\"next_min_level {next_min_level}\")\n",
    "                for level in level_order[:level_order.index(next_min_level)+1]:\n",
    "                    # print(f\"level {level}\")\n",
    "                    dict_refer[level]=doc[\"question_content\"].metadata[level_dict[level]]\n",
    "                # các phần tử sau gán = -1\n",
    "                for level in level_order[level_order.index(next_min_level)+1:]:\n",
    "                    dict_refer[level]=-1\n",
    "                sub_refer.add(tuple(dict_refer.items()))\n",
    "            else:\n",
    "                pass\n",
    "                # sub_refer.append({\"chapter\":doc.metadata[\"chapter_number\"],\"article\":doc.metadata[\"article_number\"],\"clause\":doc.metadata[\"clause_number\"],\"sub_clause\":doc.metadata[\"sub_clause_number\"]})\n",
    "        # chuyển sub_refer từ set sang list\n",
    "        sub_refer = list(sub_refer)\n",
    "        # chuyển từng phần tử trong list thành dict\n",
    "        for i in range(len(sub_refer)):\n",
    "            sub_refer[i]=dict(sub_refer[i])\n",
    "        # print(f\"sub_refer {sub_refer}\")\n",
    "        return sub_refer\n",
    "    def refer2refer_text(refer):\n",
    "        min_level=get_min_level(refer)\n",
    "        refer_text = \"\"\n",
    "        refer_obj={}\n",
    "        if min_level==\"Chapter\":\n",
    "            refer_obj=state[\"root_text\"][min_level][str(refer[min_level])]\n",
    "            refer_obj[\"full_text\"]=refer_obj[\"chapter\"]+refer_obj[\"text\"]\n",
    "        elif  min_level==\"Article\":\n",
    "            refer_obj=state[\"root_text\"][min_level][str(refer[min_level])]\n",
    "            refer_obj[\"full_text\"]=refer_obj[\"chapter\"]+refer_obj[\"article\"]+refer_obj[\"text\"]\n",
    "        elif min_level==\"clause\":\n",
    "            if str(refer[\"Article\"])=='-1':\n",
    "                print(\"---CAN'T FIND CLAUSE---\")\n",
    "            else :\n",
    "                refer_obj=state[\"root_text\"][min_level][(str(refer[\"Article\"]),str(refer[min_level]))]\n",
    "                refer_obj[\"full_text\"]=refer_obj[\"chapter\"]+refer_obj[\"article\"]+refer_obj[\"clause\"]+refer_obj[\"text\"]\n",
    "        else:\n",
    "            for ref_q_index,ref_q in enumerate(state[\"questions\"]):\n",
    "                if str(refer[\"Article\"])=='-1' or str(refer[\"clause\"])=='-1' :\n",
    "                    print(\"---CAN'T FIND SUB_CLAUSE---\")\n",
    "                    break\n",
    "                else:    \n",
    "                    if (ref_q[\"question_content\"].metadata[\"article_number\"] == str(refer[\"Article\"])) and \\\n",
    "                    (ref_q[\"question_content\"].metadata[\"clause_number\"] == str(refer[\"clause\"])) and \\\n",
    "                    (ref_q[\"question_content\"].metadata[\"sub_clause_number\"] == str(refer[\"sub_clause\"])) :\n",
    "                        refer_obj={\n",
    "                            \"chapter\":f\"\"\"第{ref_q[\"question_content\"].metadata[\"chapter_number\"]}章　{ref_q[\"question_content\"].metadata[\"chapter_title\"]} \\n \"\"\",\n",
    "                            \"article\":f\"\"\"第{ref_q[\"question_content\"].metadata[\"article_number\"]}条（{ref_q[\"question_content\"].metadata[\"article_title\"]}）\\n \"\"\",\n",
    "                            \"clause\":f\"\"\"{ref_q[\"question_content\"].metadata[\"clause_number\"]}．\"\"\",\n",
    "                            \"sub_clause\":f\"\"\"{ref_q[\"question_content\"].page_content}\"\"\",\n",
    "                            \"text\":\"\"}\n",
    "                        refer_obj[\"full_text\"]=refer_obj[\"chapter\"]+refer_obj[\"article\"]+refer_obj[\"clause\"]+refer_obj[\"sub_clause\"]+refer_obj[\"text\"]\n",
    "        return refer_obj\n",
    "\n",
    "    def get_list_index_from_refer(refer):\n",
    "        # print(refer)\n",
    "        direct_reference_index=[]\n",
    "        for ref_q_index,ref_q in enumerate(state[\"questions\"]):\n",
    "            if str(refer[\"Article\"])!='-1':\n",
    "                if (str(refer[\"Article\"])=='-1' or ref_q[\"question_content\"].metadata[\"article_number\"] == str(refer[\"Article\"])) and \\\n",
    "                (str(refer[\"clause\"])=='-1' or ref_q[\"question_content\"].metadata[\"clause_number\"] == str(refer[\"clause\"])) and \\\n",
    "                (str(refer[\"sub_clause\"])=='-1' or ref_q[\"question_content\"].metadata[\"sub_clause_number\"] == str(refer[\"sub_clause\"])) :\n",
    "                    direct_reference_index.append(ref_q_index)\n",
    "            else:\n",
    "                if (str(refer[\"Chapter\"])=='-1' or ref_q[\"question_content\"].metadata[\"chapter_number\"] == str(refer[\"Chapter\"])) and \\\n",
    "                (str(refer[\"Article\"])=='-1' or ref_q[\"question_content\"].metadata[\"article_number\"] == str(refer[\"Article\"])) and \\\n",
    "                (str(refer[\"clause\"])=='-1' or ref_q[\"question_content\"].metadata[\"clause_number\"] == str(refer[\"clause\"])) and \\\n",
    "                (str(refer[\"sub_clause\"])=='-1' or ref_q[\"question_content\"].metadata[\"sub_clause_number\"] == str(refer[\"sub_clause\"])) :\n",
    "                    direct_reference_index.append(ref_q_index)\n",
    "        return direct_reference_index\n",
    "    \n",
    "    def get_reftext(refer):\n",
    "        max_length = 512\n",
    "        refer_obj = refer2refer_text(refer)\n",
    "        index_list=get_list_index_from_refer(refer)\n",
    "        if len(refer_obj[\"full_text\"]) < max_length:\n",
    "            refer_obj[\"index_list\"]=index_list\n",
    "            return [refer_obj]  # Trả về dưới dạng danh sách\n",
    "        \n",
    "        sub_refers = find_sub_refer(refer)\n",
    "        # print(f\"sub_refers {sub_refers}\")\n",
    "        if not sub_refers:\n",
    "            refer_obj[\"index_list\"]=index_list\n",
    "            return [refer_obj]  # Trả về dưới dạng danh sách\n",
    "        ref_texts = []\n",
    "        for sub_refer in sub_refers:\n",
    "            indexes= get_list_index_from_refer(sub_refer)\n",
    "            ref_texts.extend(get_reftext(sub_refer))\n",
    "        return ref_texts\n",
    "\n",
    "    print(\"---DIRECT REFERENCE---\")\n",
    "    \"\"\"\n",
    "    Lặp qua các proposition để trích xuất phần reference, sau đó lọc các proposition \n",
    "    thật sự liên quan bằng LLM, rồi cập nhật lại state với thông tin tham chiếu.\n",
    "    đã bỏ qua sự cần thiết của chương bởi chương có thể bị sai, nên nếu có điều thì không cần xét chương\n",
    "    \"\"\"\n",
    "    #Kiểm tra độ dài tham chiếu nếu lớn hơn max_length thì kiểm tra cấp độ nhỏ hơn, nếu không có thì trả về.Ngược lại độ dài bé hơn max_length thì trả về \n",
    "    batch_inputs = []\n",
    "    # batch_map = []  # Mỗi phần tử là (q_idx, p_idx)\n",
    "    # new_graph_state = {\"questions\": []}\n",
    "    new_graph_state = deepcopy(state)\n",
    "    # Tạo batch_inputs cho LLM\n",
    "    for q_idx, question in enumerate(new_graph_state[\"questions\"]):\n",
    "        question_content = question[\"question_content\"]\n",
    "        # Lấy metadata từ đối tượng, nếu không có thì trả về None\n",
    "        question_metadata = getattr(question_content, \"metadata\", None)\n",
    "        # print(f\"question_metadata {question_metadata}\")\n",
    "        metadata_parts = []\n",
    "        # Sử dụng getattr để lấy thuộc tính nếu tồn tại\n",
    "        chapter = question_metadata[\"chapter_number\"] if question_metadata[\"chapter_number\"] else None\n",
    "        if chapter:\n",
    "                metadata_parts.append(f\"current chapter: {chapter}\")\n",
    "            else:\n",
    "                metadata_parts.append(f'current chapter: \"\"')\n",
    "        article = question_metadata[\"article_number\"] if question_metadata[\"article_number\"] else None\n",
    "        if article:\n",
    "            metadata_parts.append(f\"current article: {article}\")\n",
    "        clause = question_metadata[\"clause_number\"] if question_metadata[\"clause_number\"] else None\n",
    "        if clause:\n",
    "            metadata_parts.append(f\"current clause: {clause}\")\n",
    "        sub_clause = question_metadata[\"sub_clause_number\"] if question_metadata[\"sub_clause_number\"] else None\n",
    "        if sub_clause:\n",
    "            metadata_parts.append(f\"current sub-clause: {sub_clause}\")\n",
    "        entry = {\"document\": question_content.page_content}\n",
    "        entry[\"metadata\"] = \", \".join(metadata_parts)\n",
    "        # print(entry)\n",
    "        batch_inputs.append(entry)\n",
    "    responses = extract_reference_generator.batch(batch_inputs) if batch_inputs else []\n",
    "    responses =retry_failed_batches(batch_inputs,responses,extract_reference_generator)\n",
    "    \n",
    "    # new_graph_state = {\"questions\": []}\n",
    "    count=0\n",
    "    questions=[]\n",
    "    # Tạo state mới\n",
    "    for q_idx, question in enumerate(new_graph_state[\"questions\"]):\n",
    "            new_question = question.copy()\n",
    "            new_question[\"direct_reference\"]=[]\n",
    "            new_question[\"direct_reference_index\"]=[]\n",
    "            new_question[\"direct_reference_text\"]=[]\n",
    "            response = responses[q_idx]\n",
    "            # print(f\"response {response}\")\n",
    "            if response['has_reference'] == \"yes\" and response[\"is_extractable\"] == \"yes\":\n",
    "                # print(\"=========================\")\n",
    "                # print(f\"response {response}\")\n",
    "                # print(f\"\"\"question {question}\"\"\")\n",
    "                for reference in response[\"references\"]:\n",
    "                    if reference[\"resolved\"][\"Chapter\"]==-1 and reference[\"resolved\"][\"Article\"]==-1 and reference[\"resolved\"][\"clause\"]==-1 and reference[\"resolved\"][\"sub_clause\"]==-1:\n",
    "                        continue\n",
    "                    count+=1\n",
    "                    # teee=\"No\"\n",
    "\n",
    "                    #####\n",
    "               \n",
    "                   \n",
    "                    resolved = reference[\"resolved\"]\n",
    "                    metadata = question[\"question_content\"].metadata\n",
    "\n",
    "                    if resolved[\"sub_clause\"] != -1:\n",
    "                        resolved[\"clause\"] = resolved[\"clause\"] if resolved[\"clause\"] != -1 else metadata[\"clause_number\"]\n",
    "                        resolved[\"Article\"] = resolved[\"Article\"] if resolved[\"Article\"] != -1 else metadata[\"article_number\"]\n",
    "\n",
    "                    if resolved[\"clause\"] != -1 and resolved[\"Article\"] == -1:\n",
    "                        resolved[\"Article\"] = metadata[\"article_number\"]\n",
    "                    # print(f\"resolved {resolved}\")\n",
    "                    for ref_q_index,ref_q in enumerate(state[\"questions\"]):\n",
    "                        if reference[\"resolved\"][\"Article\"]!=-1:\n",
    "                            if (reference[\"resolved\"][\"Article\"]==-1 or ref_q[\"question_content\"].metadata[\"article_number\"] == str(reference[\"resolved\"][\"Article\"])) and \\\n",
    "                            (reference[\"resolved\"][\"clause\"]==-1 or ref_q[\"question_content\"].metadata[\"clause_number\"] == str(reference[\"resolved\"][\"clause\"])) and \\\n",
    "                            (reference[\"resolved\"][\"sub_clause\"]==-1 or ref_q[\"question_content\"].metadata[\"sub_clause_number\"] == str(reference[\"resolved\"][\"sub_clause\"])) :\n",
    "                                new_question[\"direct_reference\"].append(ref_q[\"question_content\"])\n",
    "                                new_question[\"direct_reference_index\"].append(ref_q_index)\n",
    "                                teee=\"yes\"\n",
    "                        else:\n",
    "                            if (reference[\"resolved\"][\"Chapter\"]==-1 or ref_q[\"question_content\"].metadata[\"chapter_number\"] == str(reference[\"resolved\"][\"Chapter\"])) and \\\n",
    "                            (reference[\"resolved\"][\"Article\"]==-1 or ref_q[\"question_content\"].metadata[\"article_number\"] == str(reference[\"resolved\"][\"Article\"])) and \\\n",
    "                            (reference[\"resolved\"][\"clause\"]==-1 or ref_q[\"question_content\"].metadata[\"clause_number\"] == str(reference[\"resolved\"][\"clause\"])) and \\\n",
    "                            (reference[\"resolved\"][\"sub_clause\"]==-1 or ref_q[\"question_content\"].metadata[\"sub_clause_number\"] == str(reference[\"resolved\"][\"sub_clause\"])) :\n",
    "                                new_question[\"direct_reference\"].append(ref_q[\"question_content\"])\n",
    "                                new_question[\"direct_reference_index\"].append(ref_q_index)\n",
    "                                teee=\"yes\"\n",
    "                    if new_question[\"direct_reference\"]:\n",
    "                    # print(f\"\"\"batch_inputs: {batch_inputs[q_idx]}\"\"\")\n",
    "                    # print(f\"\"\"reference[\"resolved\"] {reference[\"resolved\"]}\"\"\")\n",
    "                        direct_reference_text=get_reftext(reference[\"resolved\"])\n",
    "                        new_question[\"direct_reference_text\"]=direct_reference_text\n",
    "\n",
    "                    # if teee==\"No\":\n",
    "                    #     print(\"|||||||||||||||||||||\")\n",
    "                    #     print(f\"response {response}\")\n",
    "                    #     print(f\"\"\"question {question}\"\"\")\n",
    "            questions.append(new_question)\n",
    "            new_graph_state[\"processed_question_index\"]=[]\n",
    "    new_graph_state[\"questions\"]=questions\n",
    "    return new_graph_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### grade_direct_reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def nested_defaultdict():\n",
    "    return {\"values\": [], \"sub\": defaultdict(nested_defaultdict)}\n",
    "\n",
    "def merge_filtered_list(filtered_list):\n",
    "    merged_dict = defaultdict(nested_defaultdict)\n",
    "\n",
    "    for item in filtered_list:\n",
    "        chapter = item[\"chapter\"]\n",
    "        article = item[\"article\"] if item[\"article\"] != \"-1\" else None\n",
    "        clause = item[\"clause\"] if item[\"clause\"] != \"-1\" else None\n",
    "        sub_clause = item[\"sub_clause\"] if item[\"sub_clause\"] != \"-1\" else None\n",
    "        value = item[\"text\"].strip()\n",
    "\n",
    "        if article is None:\n",
    "            merged_dict[chapter][\"values\"].append(value)\n",
    "        elif clause is None:\n",
    "            merged_dict[chapter][\"sub\"][article][\"values\"].append(value)\n",
    "        elif sub_clause is None:\n",
    "            merged_dict[chapter][\"sub\"][article][\"sub\"][clause][\"values\"].append(value)\n",
    "        else:\n",
    "            merged_dict[chapter][\"sub\"][article][\"sub\"][clause][\"sub\"][sub_clause][\"values\"].append(value)\n",
    "\n",
    "    # Chuyển về định dạng văn bản\n",
    "    def format_dict(d):\n",
    "        result = []\n",
    "        if \"values\" in d and d[\"values\"]:\n",
    "            result.extend(v for v in d[\"values\"])\n",
    "        for key, sub_d in d[\"sub\"].items():\n",
    "            result.append(key)\n",
    "            result.extend(format_dict(sub_d))\n",
    "        return result\n",
    "\n",
    "    merged_result = []\n",
    "    for chapter, content in merged_dict.items():\n",
    "        merged_result.append(chapter)\n",
    "        merged_result.extend(format_dict(content))\n",
    "\n",
    "    return \"\\n\".join(merged_result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_direct_reference(state):\n",
    "    \"\"\"\n",
    "    Kiểm tra direct_reference của mỗi câu hỏi:\n",
    "    - Chỉ thực hiện nếu ít nhất một câu hỏi có hơn 1 direct_reference.\n",
    "    - Dùng model để kiểm tra độ liên quan của direct_reference.\n",
    "    - Cập nhật lại graph state với direct_reference đã được lọc.\n",
    "    \"\"\"\n",
    "    print(\"---GRADE REFERENCE---\")\n",
    "    new_graph_state = deepcopy(state)  # Tạo bản sao để tránh sửa đổi trực tiếp\n",
    "    \n",
    "    batch_inputs = []\n",
    "    batch_indicates = []\n",
    "    has_multiple_references = False  # Cờ kiểm tra có câu hỏi nào có >1 reference không\n",
    "    question_have_ref_count=0\n",
    "    # Thu thập dữ liệu batch\n",
    "    temp =False\n",
    "    for q_idx, question in enumerate(state[\"questions\"]):\n",
    "        if len(question.get(\"direct_reference_text\", [])) > 1:\n",
    "            has_multiple_references = True\n",
    "            for r_idx, direct_reference_text in enumerate(question[\"direct_reference_text\"]):\n",
    "                batch_inputs.append({\n",
    "                    \"main_text\": question[\"question_content\"].page_content,\n",
    "                    \"referenced_text\": direct_reference_text[\"full_text\"]\n",
    "                })\n",
    "                if not temp:\n",
    "                    print(f\"\"\"\n",
    "                        \"main_text\": {question[\"question_content\"].page_content},\n",
    "                        \"referenced_text\": {direct_reference_text[\"full_text\"]}\n",
    "                    \"\"\")\n",
    "                    temp=True\n",
    "                batch_indicates.append((q_idx, r_idx))\n",
    "        elif len(question.get(\"direct_reference_text\", [])) == 1:\n",
    "            question[\"merged_reference_text\"]=question[\"direct_reference_text\"][0]\n",
    "        else:\n",
    "            question[\"merged_reference_text\"]=\"\"\n",
    "        if len(question.get(\"direct_reference_text\", [])) >= 1:\n",
    "            question_have_ref_count+=1\n",
    "    # Nếu không có câu hỏi nào có >1 direct_reference, thoát sớm\n",
    "    print(f\"question_have_ref_count {question_have_ref_count}\")\n",
    "    if not has_multiple_references:\n",
    "        print(\"NOTHING TO GRADE\")\n",
    "        return state\n",
    "\n",
    "    # Chạy mô hình kiểm tra tính liên quan\n",
    "    responses =sematic_reference_generator.batch(batch_inputs) if batch_inputs else []\n",
    "    responses =retry_failed_batches(batch_inputs,responses,sematic_reference_generator)\n",
    "    # Ánh xạ kết quả vào new_graph_state\n",
    "    result_map = {batch_indicates[i]: responses[i][\"provide_additional_meaning\"].strip().lower() == \"yes\"\n",
    "                  for i in range(len(responses))}\n",
    "    question_have_ref_count=0\n",
    "    for q_idx, question in enumerate(new_graph_state[\"questions\"]):\n",
    "        if len(question.get(\"direct_reference_text\", [])) > 1:\n",
    "            # Lọc lại danh sách direct_reference dựa trên kết quả kiểm tra\n",
    "            question[\"direct_reference_text\"] = [\n",
    "                ref for r_idx, ref in enumerate(question[\"direct_reference_text\"])\n",
    "                if result_map.get((q_idx, r_idx), False)  # Chỉ giữ lại ref nếu is_related == \"yes\"\n",
    "            ]\n",
    "        if len(question.get(\"direct_reference_text\", [])) != 0:\n",
    "            question[\"merged_reference_text\"]=merge_filtered_list(question[\"direct_reference_text\"])\n",
    "            question_have_ref_count+=1\n",
    "        else:\n",
    "            question[\"merged_reference_text\"]=\"\"\n",
    "            \n",
    "    print(f\"question_have_ref_count {question_have_ref_count}\")\n",
    "          \n",
    "    for q_idx, question in enumerate(new_graph_state[\"questions\"]):\n",
    "        new_index_list = []\n",
    "        for direct_reference_text in question[\"direct_reference_text\"]:\n",
    "            new_index_list.extend(direct_reference_text[\"index_list\"])\n",
    "        question[\"direct_reference_index\"]=new_index_list\n",
    "    \n",
    "    return new_graph_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rewrite_question_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def rewrite_question_content(state):\n",
    "    \"\"\"\n",
    "    Lặp qua từng câu hỏi và tạo batch_inputs để cập nhật lại nội dung câu hỏi:\n",
    "    - Với mỗi câu hỏi, tạo batch_inputs chứa {\"main_section\": question[\"question_content\"], \"refer_sections\": direct_reference_text}.\n",
    "    - Chạy batch qua mô hình để tạo câu hỏi rewrite.\n",
    "    - Lưu kết quả vào question[\"rewrite_question_content\"], là một list.\n",
    "    \"\"\"\n",
    "    print(\"---REWRITE QUESTION CONTENT---\")\n",
    "    new_graph_state = deepcopy(state)  # Sao chép tránh sửa đổi trực tiếp\n",
    "    \n",
    "    batch_inputs = []\n",
    "    batch_indicates = []\n",
    "    \n",
    "    # Thu thập dữ liệu batch\n",
    "    for q_idx, question in enumerate(state[\"questions\"]):\n",
    "        new_graph_state[\"questions\"][q_idx][\"rewrite_question_content\"] = \"\"\n",
    "        if question[\"merged_reference_text\"] !=\"\":\n",
    "            batch_inputs.append({\n",
    "                \"main_section\": question[\"question_content\"].page_content,\n",
    "                \"refer_sections\": question[\"merged_reference_text\"]\n",
    "            })\n",
    "            batch_indicates.append(q_idx)\n",
    "    # print(batch_inputs)\n",
    "    # Nếu không có dữ liệu nào để xử lý, thoát sớm\n",
    "    if not batch_inputs:\n",
    "        print(\"NO QUESTIONS TO REWRITE\")\n",
    "        return state\n",
    "    \n",
    "    # Chạy mô hình sinh câu hỏi rewrite\n",
    "    responses = rewrite_clause_generator.batch(batch_inputs) if batch_inputs else []\n",
    "    responses =retry_failed_batches(batch_inputs,responses,rewrite_clause_generator)\n",
    "    \n",
    "    # print(f\"responses {responses}\")\n",
    "    # Cập nhật kết quả vào new_graph_state\n",
    "    for i, q_idx in enumerate(batch_indicates):\n",
    "        new_graph_state[\"questions\"][q_idx][\"rewrite_question_content\"]=(responses[i][\"generated_section\"])\n",
    "    return new_graph_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loopback_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loopback_node(graph_state):\n",
    "    print(\"--- LOOPBACK NODE ---\")\n",
    "    \n",
    "    new_graph_state = deepcopy(graph_state)  # Tạo bản sao sâu để tránh sửa đổi trực tiếp vào state gốc\n",
    "    processed_question_index = graph_state[\"processed_question_index\"]\n",
    "    next_batch = []\n",
    "    current_batch =[]\n",
    "    direct_count=0\n",
    "    ref_count=0\n",
    "    for index, question in enumerate(graph_state[\"questions\"]):\n",
    "        if index not in processed_question_index:\n",
    "            if len(question[\"direct_reference_text\"])==0:\n",
    "                current_batch.append(index)\n",
    "                direct_count+=1\n",
    "            elif all(ref_q in processed_question_index for ref_q in question[\"direct_reference_index\"]):\n",
    "                current_batch.append(index)\n",
    "                ref_count+=1\n",
    "            else:\n",
    "                next_batch.append(index)  # Chưa đủ điều kiện chạy\n",
    "                \n",
    "            # if len(question[\"direct_reference_text\"])!=0 and (not all(ref_q in processed_question_index for ref_q in question[\"direct_reference_index\"])):\n",
    "            #     next_batch.append(index)  # Chưa đủ điều kiện chạy\n",
    "            # else:\n",
    "            #     current_batch.append(index)\n",
    "    print(f\"direct count {direct_count}\")\n",
    "    print(f\"ref count {ref_count}\")\n",
    "    new_graph_state[\"current_question_index\"]=current_batch\n",
    "    new_graph_state[\"question_to_regen_index\"]=[]\n",
    "    new_graph_state[\"regen_count\"]=0\n",
    "    \n",
    "    return new_graph_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create_propositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_propositions(state):\n",
    "    # print(\"---CREATE PROPOSITIONS---\")\n",
    "#     # question = state[\"question\"]\n",
    "#     new_questions=[]\n",
    "#     questions = state['questions']\n",
    "\n",
    "#     # Assume we have a function to create propositions from the question\n",
    "#     proposition_responses = proposition_generator.batch([\n",
    "#         {\"document\": question['question_content'].page_content} \n",
    "#         for question in questions])\n",
    "#     for index, proposition_response in enumerate(proposition_responses):\n",
    "#         propositions = [\n",
    "#             {\n",
    "#                 \"proposition_content\": Document(\n",
    "#                 page_content=prop[\"proposition\"],\n",
    "#                 metadata={\"sub_clause_number\":prop[\"sub-clause\"]}\n",
    "#                 ),\n",
    "#                 \"documents\": [],  # Chưa có document nào, sẽ lấy ở retrieval step\n",
    "#                 \"filtered_documents\": [],\n",
    "#                 \"generation\": \"\"  # Chưa có kết quả generation\n",
    "#             }\n",
    "#             for prop in proposition_response['propositions']\n",
    "#         ]\n",
    "#         new_questions.append({\n",
    "#             \"question_content\": questions[index][\"question_content\"],\n",
    "#             \"propositions\": propositions\n",
    "#         })\n",
    "\n",
    "#     return {\"questions\": new_questions}\n",
    "\n",
    "\n",
    "\n",
    "def create_propositions(graph_state):\n",
    "    def question2text(question):\n",
    "        return f\"\"\"第{question.metadata[\"chapter_number\"]}章　{question.metadata[\"chapter_title\"]} \\n 第{question.metadata[\"article_number\"]}条（{question.metadata[\"article_title\"]}）\\n{question.page_content}\"\"\"\n",
    "    \n",
    "    print(\"---CREATE PROPOSITIONS---\")\n",
    "    new_graph_state = deepcopy(graph_state)  # Tạo bản sao sâu để tránh sửa đổi trực tiếp vào state gốc\n",
    "    direct_batch_inputs = []\n",
    "    ref_batch_inputs = []\n",
    "    direct_batch_map = []\n",
    "    ref_batch_map = []\n",
    "    print(f\"\"\"len(graph_state[\"current_question_index\"]) {len(graph_state[\"current_question_index\"])}\"\"\")\n",
    "    count=0\n",
    "    for index, question in enumerate(graph_state[\"questions\"]):\n",
    "        if index in graph_state[\"current_question_index\"]:\n",
    "            if len(question[\"direct_reference_text\"])==0:\n",
    "                # count+=1\n",
    "                direct_batch_inputs.append({\"document\": question[\"question_content\"].page_content})  # Chạy hàm proposition_gen\n",
    "                direct_batch_map.append(index)\n",
    "            elif  all(ref_q in graph_state[\"processed_question_index\"] for ref_q in question[\"direct_reference_index\"]):\n",
    "                count+=1\n",
    "                ref_batch_inputs.append({\"document\": question[\"rewrite_question_content\"]})\n",
    "                ref_batch_map.append(index)\n",
    "            else:\n",
    "                print(\"HAVE SOME TROUBLES\")\n",
    "    print(f\"len(direct ) {len(direct_batch_inputs)}\")\n",
    "    print(f\"len(ref ) {len(ref_batch_inputs)}\")\n",
    "    print(f\"count {count}\")\n",
    "    # Chạy batch proposition\n",
    "    direct_proposition_responses = proposition_generator.batch(direct_batch_inputs)\n",
    "    direct_proposition_responses =retry_failed_batches(direct_batch_inputs,direct_proposition_responses,proposition_generator)\n",
    "\n",
    "    ref_proposition_responses = proposition_generator.batch(ref_batch_inputs)\n",
    "    ref_proposition_responses =retry_failed_batches(ref_batch_inputs,ref_proposition_responses,proposition_generator)\n",
    "    # print(f\"direct_proposition_responses {direct_proposition_responses}\")\n",
    "    for i, proposition_response in enumerate(direct_proposition_responses):\n",
    "        propositions = [\n",
    "            {\n",
    "                \"proposition_content\": prop[\"proposition\"],\n",
    "                \"documents\": [],\n",
    "                \"filtered_documents\": []\n",
    "            }\n",
    "            for prop in proposition_response['propositions']\n",
    "        ]\n",
    "        propositions.append({\n",
    "            \"proposition_content\":question2text(new_graph_state[\"questions\"][direct_batch_map[i]][\"question_content\"]),\n",
    "            \"documents\": [],\n",
    "            \"filtered_documents\": []\n",
    "        })\n",
    "        new_graph_state[\"questions\"][direct_batch_map[i]][\"propositions\"] = propositions\n",
    "        new_graph_state[\"processed_question_index\"].append(direct_batch_map[i])\n",
    "    for i, proposition_response in enumerate(ref_proposition_responses):\n",
    "        propositions = [\n",
    "            {\n",
    "                \"proposition_content\": prop[\"proposition\"],\n",
    "                \"documents\": [],\n",
    "                \"filtered_documents\": []\n",
    "            }\n",
    "            for prop in proposition_response['propositions']\n",
    "        ]\n",
    "        new_graph_state[\"questions\"][ref_batch_map[i]][\"ref_propositions\"] = propositions\n",
    "        new_graph_state[\"processed_question_index\"].append(ref_batch_map[i])\n",
    "\n",
    "    return new_graph_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def retrieve(state):\n",
    "    print(\"---RETRIEVE---\")\n",
    "    new_graph_state = deepcopy(state)  # Tạo bản sao sâu để tránh sửa đổi trực tiếp vào state gốc\n",
    "    count=0\n",
    "    for index, question in enumerate(new_graph_state[\"questions\"]):\n",
    "    # for question in new_graph_state[\"questions\"]:\n",
    "        if index in new_graph_state[\"current_question_index\"]:\n",
    "            for p_idx, proposition in enumerate(question[\"propositions\"]):\n",
    "                retrieved_docs = retriever.invoke(proposition[\"proposition_content\"])\n",
    "                question[\"propositions\"][p_idx][\"documents\"] = retrieved_docs  # Cập nhật trực tiếp vào bản sao mới\n",
    "                count+=1\n",
    "            for p_idx, proposition in enumerate(question[\"ref_propositions\"]):\n",
    "                retrieved_docs = retriever.invoke(proposition[\"proposition_content\"])\n",
    "                question[\"ref_propositions\"][p_idx][\"documents\"]= retrieved_docs\n",
    "                count+=1\n",
    "    print(f\"---have retrieved for {count} propositions---\")\n",
    "    new_graph_state[\"rewrite_count\"]=0\n",
    "    return new_graph_state\n",
    "\n",
    "# def dense_retrieve(state):\n",
    "#     print(\"---RETRIEVE---\")\n",
    "#     # question = state[\"question\"]\n",
    "\n",
    "#     # Retrieval\n",
    "#     # documents = retriever.invoke(question)\n",
    "#     # return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "#     new_graph_state = {\"questions\": []}\n",
    "\n",
    "#     for question in state[\"questions\"]:\n",
    "#         new_propositions = []\n",
    "#         for proposition in question[\"propositions\"]:\n",
    "#             # retrieved_docs = retriever.invoke(proposition[\"proposition_content\"])\n",
    "#             map_ids, new_scores = retrieval.inference(proposition[\"proposition_content\"])\n",
    "#             retrieved_docs = return_documents(map_ids, new_scores)\n",
    "#             new_propositions.append({\n",
    "#                 \"proposition_content\": proposition[\"proposition_content\"],\n",
    "#                 \"documents\": retrieved_docs,  # Cập nhật danh sách documents\n",
    "#                 \"filtered_documents\": [],\n",
    "#                 \"generation\": proposition[\"generation\"]\n",
    "#             })\n",
    "        \n",
    "#         new_graph_state[\"questions\"].append({\n",
    "#             \"question_content\": question[\"question_content\"],\n",
    "#             \"propositions\": new_propositions\n",
    "#         })\n",
    "\n",
    "#     return new_graph_state  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### grade_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def grade_documents(state):\n",
    "#     \"\"\"\n",
    "#     Đánh giá và lọc bỏ các tài liệu không liên quan cho những proposition có filtered_documents == [].\n",
    "#     Những proposition đã có filtered_documents (không rỗng) sẽ được giữ nguyên.\n",
    "\n",
    "#     Args:\n",
    "#         state (Dict): Graph state chứa danh sách các câu hỏi và propositions.\n",
    "    \n",
    "#     Returns:\n",
    "#         Dict: Graph state mới với các proposition đã được cập nhật filtered_documents.\n",
    "#     \"\"\"\n",
    "#     print(\"---GRADE DOCUMENTS---\")\n",
    "    \n",
    "#     # Chuẩn bị danh sách đầu vào cho batch processing cùng với mapping\n",
    "#     batch_inputs = []\n",
    "#     batch_map = []  # Mỗi phần tử là (q_idx, p_idx, doc_idx)\n",
    "    \n",
    "#     # Duyệt qua state để thu thập các input cần xử lý (chỉ với proposition chưa được đánh giá)\n",
    "#     for q_idx, question in enumerate(state[\"questions\"]):\n",
    "#         for p_idx, proposition in enumerate(question[\"propositions\"]):\n",
    "#             # Nếu proposition đã có filtered_documents không rỗng, bỏ qua\n",
    "#             if proposition.get(\"filtered_documents\", []) != []:\n",
    "#                 continue\n",
    "#             for doc_idx, doc in enumerate(proposition[\"documents\"]):\n",
    "#                 batch_inputs.append({\n",
    "#                     \"context\": doc.page_content,\n",
    "#                     \"contract\": proposition[\"proposition_content\"]\n",
    "#                 })\n",
    "#                 batch_map.append((q_idx, p_idx,0, doc_idx))\n",
    "#         for ref_p_idx, proposition in enumerate(question[\"ref_propositions\"]):\n",
    "#             # Nếu proposition đã có filtered_documents không rỗng, bỏ qua\n",
    "#             if proposition.get(\"filtered_documents\", []) != []:\n",
    "#                 continue\n",
    "#             for doc_idx, doc in enumerate(proposition[\"documents\"]):\n",
    "#                 batch_inputs.append({\n",
    "#                     \"context\": doc.page_content,\n",
    "#                     \"contract\": proposition[\"proposition_content\"]\n",
    "#                 })\n",
    "#                 batch_map.append((q_idx, 0,ref_p_idx, doc_idx))\n",
    "    \n",
    "#     # Gọi batch processing nếu có input; nếu không có, responses là danh sách rỗng\n",
    "#     responses = retrieval_grader.batch(batch_inputs) if batch_inputs else []\n",
    "    \n",
    "#     # Tạo new_graph_state, cập nhật các proposition đã được đánh giá\n",
    "#     new_graph_state = {\"questions\": []}\n",
    "    \n",
    "#     # Duyệt qua state để xây dựng new_graph_state\n",
    "#     for q_idx, question in enumerate(state[\"questions\"]):\n",
    "#         new_props = []\n",
    "#         for p_idx, proposition in enumerate(question[\"propositions\"]):\n",
    "#             # Nếu proposition đã có filtered_documents (không cần đánh giá lại), giữ nguyên\n",
    "#             if proposition.get(\"filtered_documents\", []) != []:\n",
    "#                 new_props.append(proposition)\n",
    "#             else:\n",
    "#                 # Lấy các document được đánh giá cho proposition hiện tại dựa trên batch_map\n",
    "#                 filtered_docs = []\n",
    "#                 # Duyệt qua tất cả mapping, nếu vị trí khớp, lấy kết quả đánh giá\n",
    "#                 for map_idx, (mq, mp, md) in enumerate(batch_map):\n",
    "#                     if mq == q_idx and mp == p_idx:\n",
    "#                         response = responses[map_idx][\"binary_score\"].strip() if responses else \"\"\n",
    "#                         if response.lower() == \"yes\":\n",
    "#                             filtered_docs.append(proposition[\"documents\"][md])\n",
    "#                 # Cập nhật proposition với filtered_documents mới\n",
    "#                 updated_prop = proposition.copy()\n",
    "#                 updated_prop[\"filtered_documents\"] = filtered_docs\n",
    "#                 new_props.append(updated_prop)\n",
    "#         new_graph_state[\"questions\"].append({\n",
    "#             \"question_content\": question[\"question_content\"],\n",
    "#             \"propositions\": new_props\n",
    "#         })\n",
    "    \n",
    "#     return new_graph_state\n",
    "from copy import deepcopy\n",
    "\n",
    "def grade_documents(state):\n",
    "    def question2text(question):\n",
    "        return f\"\"\"第{question.metadata[\"chapter_number\"]}章　{question.metadata[\"chapter_title\"]} \\n 第{question.metadata[\"article_number\"]}条（{question.metadata[\"article_title\"]}）\\n{question.page_content}\"\"\"\n",
    "   \n",
    "    \"\"\"\n",
    "    Đánh giá và lọc bỏ các tài liệu không liên quan cho những proposition có filtered_documents == [].\n",
    "    Những proposition đã có filtered_documents (không rỗng) sẽ được giữ nguyên.\n",
    "\n",
    "    Args:\n",
    "        state (Dict): Graph state chứa danh sách các câu hỏi và propositions.\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Graph state mới với các proposition đã được cập nhật filtered_documents.\n",
    "    \"\"\"\n",
    "    print(\"---GRADE DOCUMENTS---\")\n",
    "    \n",
    "    new_graph_state = deepcopy(state)  # Tạo bản sao sâu để tránh sửa đổi trực tiếp vào state gốc\n",
    "    batch_inputs = []\n",
    "    batch_map = []  # Mỗi phần tử là (q_idx, p_idx, is_ref, doc_idx)\n",
    "    # if new_graph_state[\"rewrite_count\"]!=0:\n",
    "    #     for proposition in new_graph_state[\"propositions_to_rewrite\"]:\n",
    "    #         if proposition[\"proposition_index\"]!=-1:\n",
    "    #             for doc_idx,doc in new_graph_state[\"questions\"][proposition[\"question_index\"]][\"propositions\"][proposition[\"proposition_index\"]][\"documents\"]:\n",
    "    #                 batch_inputs.append({\n",
    "    #                     \"context\": doc.page_content,\n",
    "    #                     \"contract\": proposition[\"original_proposition\"][\"proposition_content\"]\n",
    "    #                 })\n",
    "    #                 batch_map.append((proposition[\"question_index\"],proposition[\"propositions_index\"],-1,doc_idx))\n",
    "    #         else:\n",
    "    #             for doc_idx,doc in new_graph_state[\"questions\"][proposition[\"question_index\"]][\"ref_propositions\"][proposition[\"ref_propositions_index\"]][\"documents\"]:\n",
    "    #                 batch_inputs.append({\n",
    "    #                     \"context\": doc.page_content,\n",
    "    #                     \"contract\": proposition[\"original_proposition\"][\"proposition_content\"]\n",
    "    #                 })\n",
    "    #                 batch_map.append((proposition[\"question_index\"],-1,proposition[\"ref_propositions_index\"],doc_idx))\n",
    "    \n",
    "    # Duyệt qua state để thu thập các input cần xử lý (chỉ với proposition chưa được đánh giá)\n",
    "    # else:\n",
    "    for q_idx, question in enumerate(new_graph_state[\"questions\"]):\n",
    "        # Xử lý propositions thông thường\n",
    "        if q_idx in new_graph_state[\"current_question_index\"]:\n",
    "            if question[\"rewrite_question_content\"]!=\"\":\n",
    "               contract= question[\"rewrite_question_content\"]\n",
    "            else:\n",
    "               contract= question2text(question[\"question_content\"])\n",
    "            for p_idx, proposition in enumerate(question[\"propositions\"]):\n",
    "                if proposition.get(\"filtered_documents\", []):\n",
    "                    continue\n",
    "                for doc_idx, doc in enumerate(proposition[\"documents\"]):\n",
    "                    \n",
    "                    batch_inputs.append({\n",
    "                        \"context\": doc.page_content,\n",
    "                        # \"contract\": proposition[\"proposition_content\"]\n",
    "                        \"contract\": contract\n",
    "                    })\n",
    "                    batch_map.append((q_idx, p_idx, -1, doc_idx))  # is_ref = 0\n",
    "            \n",
    "            for ref_p_idx, proposition in enumerate(question.get(\"ref_propositions\", [])):\n",
    "                if proposition.get(\"filtered_documents\", []):\n",
    "                    continue\n",
    "                for doc_idx, doc in enumerate(proposition[\"documents\"]):\n",
    "                    batch_inputs.append({\n",
    "                        \"context\": doc.page_content,\n",
    "                        # \"contract\": proposition[\"proposition_content\"]\n",
    "                        \"contract\": contract\n",
    "                        \n",
    "                    })\n",
    "                    batch_map.append((q_idx, -1, ref_p_idx, doc_idx))  # is_ref = 1\n",
    "        \n",
    "    # Gọi batch processing nếu có input; nếu không có, responses là danh sách rỗng\n",
    "    responses = retrieval_grader.batch(batch_inputs) if batch_inputs else []\n",
    "    responses =retry_failed_batches(batch_inputs,responses,retrieval_grader)\n",
    "    count=0\n",
    "    # Cập nhật new_graph_state với kết quả đánh giá\n",
    "    for q_idx, question in enumerate(new_graph_state[\"questions\"]):\n",
    "        if q_idx in new_graph_state[\"current_question_index\"]:\n",
    "            for p_idx, proposition in enumerate(question[\"propositions\"]):\n",
    "                if proposition.get(\"filtered_documents\", []):\n",
    "                    continue\n",
    "                filtered_docs = []\n",
    "                for map_idx, (mq, mp, m_ref, md) in enumerate(batch_map):\n",
    "                    if mq == q_idx and mp == p_idx and m_ref == -1:\n",
    "                        response = responses[map_idx][\"binary_score\"].strip() if responses else \"\"\n",
    "                        if response.lower() == \"yes\":\n",
    "                            filtered_docs.append(proposition[\"documents\"][md])\n",
    "                proposition[\"filtered_documents\"] = filtered_docs\n",
    "                count+= len(filtered_docs)\n",
    "            for ref_p_idx, proposition in enumerate(question.get(\"ref_propositions\", [])):\n",
    "                if proposition.get(\"filtered_documents\", []):\n",
    "                    continue\n",
    "                filtered_docs = []\n",
    "                for map_idx, (mq, mp, m_ref, md) in enumerate(batch_map):\n",
    "                    if mq == q_idx and mp == -1 and ref_p_idx == mp:\n",
    "                        response = responses[map_idx][\"binary_score\"].strip() if responses else \"\"\n",
    "                        if response.lower() == \"yes\":\n",
    "                            filtered_docs.append(proposition[\"documents\"][md])\n",
    "                proposition[\"filtered_documents\"] = filtered_docs\n",
    "                count+= len(filtered_docs)\n",
    "    print(f\"---have filtered and have {count} docs left---\")\n",
    "    return new_graph_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check_empty_propositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_empty_propositions(state):\n",
    "    print(\"---CHECK EMPTY PROPOSITIONS---\")\n",
    "    new_graph_state = deepcopy(state)  # Tạo bản sao sâu để tránh sửa đổi trực tiếp vào state gốc\n",
    "\n",
    "    # new_graph_state = {\"questions\": [],\"propositions_to_rewrite\":[]}\n",
    "    propositions_to_rewrite = []  # Danh sách propositions cần rewrite\n",
    "    count = 0\n",
    "    \n",
    "    for q_idx, question in enumerate(state[\"questions\"]):\n",
    "        # new_propositions = []\n",
    "        if q_idx in new_graph_state[\"current_question_index\"]:\n",
    "            for p_idx, proposition in enumerate(question[\"propositions\"]):\n",
    "                if not proposition[\"filtered_documents\"]:  # Nếu danh sách documents rỗng\n",
    "                    count+=1\n",
    "                    propositions_to_rewrite.append({\n",
    "                        \"question_index\": q_idx,  # Lưu vị trí question\n",
    "                        \"proposition_index\": p_idx,  # Lưu vị trí proposition\n",
    "                        \"ref_propositions_index\": -1,  # Lưu vị trí proposition\n",
    "                        \"original_proposition\": proposition,  # Lưu thông tin gốc\n",
    "                    })\n",
    "            for ref_p_idx, proposition in enumerate(question[\"ref_propositions\"]):\n",
    "                if not proposition[\"filtered_documents\"]:  # Nếu danh sách documents rỗng\n",
    "                    count+=1\n",
    "                    propositions_to_rewrite.append({\n",
    "                        \"question_index\": q_idx,  # Lưu vị trí question\n",
    "                        \"proposition_index\": -1,  # Lưu vị trí proposition\n",
    "                        \"ref_propositions_index\": ref_p_idx,  # Lưu vị trí proposition\n",
    "                        \"original_proposition\": proposition,  # Lưu thông tin gốc\n",
    "                    })        \n",
    "                # else:\n",
    "                # new_propositions.append(proposition)  # Giữ lại propositions hợp lệ\n",
    "            \n",
    "            # new_graph_state[\"questions\"].append({\n",
    "            #     \"question_content\": question[\"question_content\"],\n",
    "            #     \"propositions\": new_propositions\n",
    "            # })\n",
    "            # new_graph_state[\"propositions_to_rewrite\"]=propositions_to_rewrite\n",
    "    print(f\"---{count}--- propositions that can not find documents\")\n",
    "    new_graph_state[\"propositions_to_rewrite\"]=propositions_to_rewrite\n",
    "    return new_graph_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### query_transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def query_transformation(state):\n",
    "    print(\"---QUERY TRANSFORMATION---\")\n",
    "    # print(f\"graph_state {state}\")\n",
    "    propositions_to_recheck = []\n",
    "    count = 0\n",
    "    # 1. Chuẩn bị input cho query transformation\n",
    "    query_transformation_input = [\n",
    "        {\"query\": prop_data[\"original_proposition\"][\"proposition_content\"]}\n",
    "        for prop_data in state[\"propositions_to_rewrite\"]\n",
    "    ]\n",
    "\n",
    "    # 2. Gọi query transformation\n",
    "    transformed_queries = query_transformation_generator.batch(query_transformation_input)\n",
    "    transformed_queries =retry_failed_batches(query_transformation_input,transformed_queries,query_transformation_generator)\n",
    "    print(\"---trieve---\")\n",
    "    # 3. Xử lý retrieval từng query một\n",
    "    prop_documents = {\n",
    "        prop_data[\"original_proposition\"][\"proposition_content\"]: {}\n",
    "        for prop_data in state[\"propositions_to_rewrite\"]\n",
    "    }\n",
    "    \n",
    "    for i, transformed_query in enumerate(transformed_queries):\n",
    "        original_prop = query_transformation_input[i][\"query\"]\n",
    "        queries = (\n",
    "            transformed_query[\"paraphrased_queries\"]\n",
    "            + transformed_query[\"detailed_queries\"]\n",
    "            + transformed_query[\"generalized_queries\"]\n",
    "            + transformed_query[\"expanded_queries\"]\n",
    "        )\n",
    "        for query in queries:\n",
    "            retrieved_docs = retriever.invoke(query)  # Chạy tuần tự từng query\n",
    "\n",
    "            for doc in retrieved_docs:\n",
    "                # Dùng doc.page_content làm key, và lưu đối tượng document đầy đủ\n",
    "                prop_documents[original_prop][doc.page_content] = doc\n",
    "\n",
    "    # 4. Cập nhật lại graph_state với các documents mới\n",
    "    updated_state = deepcopy(state)\n",
    "    old_docs_keys = set()\n",
    "    for prop_data in state[\"propositions_to_rewrite\"]:\n",
    "        for d in prop_data[\"original_proposition\"].get('documents', []):\n",
    "            old_docs_keys.add(d.page_content)\n",
    "\n",
    "    for prop_data in updated_state[\"propositions_to_rewrite\"]:\n",
    "        q_idx = prop_data[\"question_index\"]\n",
    "        p_idx = prop_data[\"proposition_index\"]\n",
    "        ref_p_idx = prop_data[\"ref_propositions_index\"]\n",
    "        proposition_text = prop_data[\"original_proposition\"][\"proposition_content\"]\n",
    "\n",
    "        new_docs_dict = {\n",
    "            key: doc for key, doc in prop_documents[proposition_text].items()\n",
    "            if key not in old_docs_keys\n",
    "        }\n",
    "        new_documents = list(new_docs_dict.values())\n",
    "        count+=len(new_documents)\n",
    "        # print(\"len new docs\",len(new_documents))\n",
    "        if new_documents:\n",
    "            if p_idx!=-1:\n",
    "                updated_state[\"questions\"][q_idx][\"propositions\"][p_idx][\"documents\"] = new_documents\n",
    "            else:\n",
    "                updated_state[\"questions\"][q_idx][\"ref_propositions\"][ref_p_idx][\"documents\"] = new_documents\n",
    "            # print(updated_state[\"questions\"][q_idx][\"propositions\"][p_idx])\n",
    "        else:\n",
    "            if p_idx!=-1:\n",
    "                propositions_to_recheck.append(updated_state[\"questions\"][q_idx][\"propositions\"][p_idx])\n",
    "            else:\n",
    "                propositions_to_recheck.append(updated_state[\"questions\"][q_idx][\"ref_propositions\"][ref_p_idx])\n",
    "    if not updated_state.get(\"rewrite_count\", 0):  # Mặc định là 0 nếu không có\n",
    "        updated_state[\"rewrite_count\"] = 1\n",
    "    else:\n",
    "        updated_state[\"rewrite_count\"] += 1\n",
    "    updated_state[\"propositions_to_rewrite\"]=[]\n",
    "    print(f\"---new docs num: {count}\")\n",
    "    return updated_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generate websearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def websearch_generate(state):\n",
    "#     def docs2text(docs):\n",
    "        \n",
    "#         return '\\n'.join([doc.page_content for doc in docs])\n",
    "        \n",
    "#     def question2text(question):\n",
    "#         return f\"\"\"第{question.metadata[\"chapter_number\"]}章　{question.metadata[\"chapter_title\"]} \\n 第{question.metadata[\"article_number\"]}条（{question.metadata[\"article_title\"]}）\\n{question.page_content}\"\"\"\n",
    "    \n",
    "#     print(\"---WEBSEARCH GENERATE---\")\n",
    "#     new_graph_state = deepcopy(state)\n",
    "#     batch_doc_inputs = []\n",
    "#     batch_both_inputs = []\n",
    "#     batch_doc_indices = []\n",
    "#     batch_both_indices = []\n",
    "#     print(f\"\"\"current_question_index {new_graph_state[\"current_question_index\"]}\"\"\")\n",
    "#     # if new_graph_state[\"needed_to_regen\"] and new_graph_state[\"regen_count\"]<=2:\n",
    "#     #     for q_idx\n",
    "#     for q_idx, question in enumerate(new_graph_state[\"questions\"]):\n",
    "#         if q_idx in new_graph_state[\"current_question_index\"]:\n",
    "#             if new_graph_state[\"needed_to_regen\"] and new_graph_state[\"regen_count\"]<=2:\n",
    "            \n",
    "#             if len(question[\"direct_reference_text\"])!=0:\n",
    "#                 # print(\"OH NOOOOOOOOOOOOOOO\")tered_documents\"]))\n",
    "#                 if question[\"filtered_web_search_docs\"]:\n",
    "#                     batch_both_inputs.append({\n",
    "#                         \"context\": docs2text(question[\"filtered_web_search_docs\"]),\n",
    "#                         \"contract\": question2text(question[\"question_content\"]),\n",
    "#                         \"new_query\": question[\"rewrite_question_content\"]\n",
    "#                     })\n",
    "#                     batch_both_indices.append(q_idx)\n",
    "#                 else:\n",
    "#                     question[\"generation\"] = \"\"\"{\"evaluation\":\"insufficient_information\",\n",
    "#                     \"explanation\":\"CANNOT FIND ANY RELEVANT DOCUMENT\"}\"\"\"\n",
    "#             else:\n",
    "#                 if question[\"filtered_web_search_docs\"]:\n",
    "#                     batch_doc_inputs.append({\n",
    "#                         \"context\": docs2text(question[\"filtered_web_search_docs\"]),\n",
    "#                         \"contract\": question2text(question[\"question_content\"])\n",
    "#                     })\n",
    "#                     batch_doc_indices.append(q_idx)\n",
    "#                 else:\n",
    "#                     question[\"generation\"] = \"\"\"{\"evaluation\":\"insufficient_information\",\n",
    "#                     \"explanation\":\"CANNOT FIND ANY RELEVANT DOCUMENT\"}\"\"\"\n",
    "#     print(\"start both\")\n",
    "#     both_responses = rag_chain_both.batch(batch_both_inputs)\n",
    "#     both_responses = retry_failed_batches(batch_both_inputs, both_responses, rag_chain_both)\n",
    "#     print(\"end both\")\n",
    "#     print(\"start doc\")\n",
    "    \n",
    "#     doc_responses = rag_chain_doc.batch(batch_doc_inputs)\n",
    "#     doc_responses = retry_failed_batches(batch_doc_inputs, doc_responses, rag_chain_doc)\n",
    "#     print(\"end doc\")\n",
    "    \n",
    "#     for idx, response in enumerate(both_responses):\n",
    "#         q_idx = batch_both_indices[idx]\n",
    "#         new_graph_state[\"questions\"][q_idx][\"generation\"] = response\n",
    "    \n",
    "#     for idx, response in enumerate(doc_responses):\n",
    "#         q_idx = batch_doc_indices[idx]\n",
    "#         new_graph_state[\"questions\"][q_idx][\"generation\"] = response\n",
    "\n",
    "#     ######## halluciantion\n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "#     return new_graph_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate(state):\n",
    "    def docs2text(docs):\n",
    "        \n",
    "        return '\\n'.join([doc.page_content for doc in docs])\n",
    "        \n",
    "    def question2text(question):\n",
    "        return f\"\"\"第{question.metadata[\"chapter_number\"]}章　{question.metadata[\"chapter_title\"]} \\n 第{question.metadata[\"article_number\"]}条（{question.metadata[\"article_title\"]}）\\n{question.page_content}\"\"\"\n",
    "    \n",
    "    print(\"---GENERATE---\")\n",
    "    new_graph_state = deepcopy(state)\n",
    "    batch_doc_inputs = []\n",
    "    batch_both_inputs = []\n",
    "    batch_doc_indices = []\n",
    "    batch_both_indices = []\n",
    "    print(f\"\"\"current_question_index {new_graph_state[\"current_question_index\"]}\"\"\")\n",
    "\n",
    "    if len(new_graph_state[\"question_to_regen_index\"])!=0 and new_graph_state[\"regen_count\"]<=2:\n",
    "        for q_idx, question in enumerate(new_graph_state[\"questions\"]):\n",
    "            if q_idx in new_graph_state[\"question_to_regen_index\"]:\n",
    "                if question[\"all_docs\"]!=[]:\n",
    "                    if len(question[\"direct_reference_text\"])!=0:\n",
    "                        batch_both_inputs.append({\n",
    "                                \"context\": docs2text(question[\"all_docs\"]),\n",
    "                                \"contract\": question2text(question[\"question_content\"]),\n",
    "                                \"new_query\": question[\"rewrite_question_content\"]\n",
    "                            })\n",
    "                        batch_both_indices.append(q_idx)\n",
    "                    else:\n",
    "                        batch_doc_inputs.append({\n",
    "                                \"context\": docs2text(question[\"all_docs\"]),\n",
    "                                \"contract\": question2text(question[\"question_content\"])\n",
    "                            })\n",
    "                        batch_doc_indices.append(q_idx)\n",
    "                elif question[\"filtered_web_search_docs\"]!= []:\n",
    "                    if len(question[\"direct_reference_text\"])!=0:\n",
    "                        batch_both_inputs.append({\n",
    "                                \"context\": docs2text(question[\"filtered_web_search_docs\"]),\n",
    "                                \"contract\": question2text(question[\"question_content\"]),\n",
    "                                \"new_query\": question[\"rewrite_question_content\"]\n",
    "                            })\n",
    "                        batch_both_indices.append(q_idx)\n",
    "                    else:\n",
    "                        batch_doc_inputs.append({\n",
    "                                \"context\": docs2text(question[\"filtered_web_search_docs\"]),\n",
    "                                \"contract\": question2text(question[\"question_content\"])\n",
    "                            })\n",
    "                        batch_doc_indices.append(q_idx)\n",
    "                else:\n",
    "                    question[\"generation\"] = \"\"\"{\"evaluation\":\"insufficient_information\",\n",
    "                            \"explanation\":\"CANNOT FIND ANY RELEVANT DOCUMENT\"}\"\"\"\n",
    "                    \n",
    "            # if q_idx in new_graph_state[\"question_to_regen_index\"]:\n",
    "            #     doc_list = set()  # Dùng set để loại bỏ tài liệu trùng lặp\n",
    "            #     for p_idx, proposition in enumerate(question[\"propositions\"]):\n",
    "            #         if proposition[\"filtered_documents\"]:\n",
    "            #             doc_list.update(str(proposition[\"filtered_documents\"]))\n",
    "                \n",
    "            #     if len(question[\"direct_reference_text\"])!=0:\n",
    "            #         # print(\"OH NOOOOOOOOOOOOOOO\")\n",
    "            #         for ref_p_idx, proposition in enumerate(question[\"ref_propositions\"]):\n",
    "            #             if proposition[\"filtered_documents\"]:\n",
    "            #                 doc_list.update(str(proposition[\"filtered_documents\"]))\n",
    "            #         doc_list=[Document(doc) for doc in doc_list]\n",
    "            #         if doc_list:\n",
    "            #             batch_both_inputs.append({\n",
    "            #                 \"context\": docs2text(doc_list),\n",
    "            #                 \"contract\": question2text(question[\"question_content\"]),\n",
    "            #                 \"new_query\": question[\"rewrite_question_content\"]\n",
    "            #             })\n",
    "            #             batch_both_indices.append(q_idx)\n",
    "            #         else:\n",
    "            #             question[\"generation\"] = \"\"\"{\"evaluation\":\"insufficient_information\",\n",
    "            #             \"explanation\":\"CANNOT FIND ANY RELEVANT DOCUMENT\"}\"\"\"\n",
    "            #     else:\n",
    "            #         doc_list=[Document(doc) for doc in doc_list]\n",
    "            #         if doc_list:\n",
    "            #             batch_doc_inputs.append({\n",
    "            #                 \"context\": docs2text(doc_list),\n",
    "            #                 \"contract\": question2text(question[\"question_content\"])\n",
    "            #             })\n",
    "            #             batch_doc_indices.append(q_idx)\n",
    "            #         else:\n",
    "            #             question[\"generation\"] = \"\"\"{\"evaluation\":\"insufficient_information\",\n",
    "            #             \"explanation\":\"CANNOT FIND ANY RELEVANT DOCUMENT\"}\"\"\"\n",
    "        # new_graph_state[\"question_to_regen_index\"]=[]\n",
    "        new_graph_state[\"regen_count\"]+=1\n",
    "    else:\n",
    "        # for q_idx, question in enumerate(new_graph_state[\"questions\"]):\n",
    "        #     if q_idx in new_graph_state[\"current_question_index\"]:\n",
    "        #         doc_list = set()  # Dùng set để loại bỏ tài liệu trùng lặp\n",
    "        #         for p_idx, proposition in enumerate(question[\"propositions\"]):\n",
    "        #             if proposition[\"filtered_documents\"]:\n",
    "        #                 doc_list.update(str(proposition[\"filtered_documents\"]))\n",
    "                \n",
    "        #         if len(question[\"direct_reference_text\"])!=0:\n",
    "        #             # print(\"OH NOOOOOOOOOOOOOOO\")\n",
    "        #             for ref_p_idx, proposition in enumerate(question[\"ref_propositions\"]):\n",
    "        #                 if proposition[\"filtered_documents\"]:\n",
    "        #                     doc_list.update(str(proposition[\"filtered_documents\"]))\n",
    "        #             doc_list=[Document(doc) for doc in doc_list]\n",
    "        #             if doc_list:\n",
    "        #                 batch_both_inputs.append({\n",
    "        #                     \"context\": docs2text(doc_list),\n",
    "        #                     \"contract\": question2text(question[\"question_content\"]),\n",
    "        #                     \"new_query\": question[\"rewrite_question_content\"]\n",
    "        #                 })\n",
    "        #                 batch_both_indices.append(q_idx)\n",
    "        #             else:\n",
    "        #                 question[\"generation\"] = \"\"\"{\"evaluation\":\"insufficient_information\",\n",
    "        #                 \"explanation\":\"CANNOT FIND ANY RELEVANT DOCUMENT\"}\"\"\"\n",
    "        #         else:\n",
    "        #             doc_list=[Document(doc) for doc in doc_list]\n",
    "        #             if doc_list:\n",
    "        #                 batch_doc_inputs.append({\n",
    "        #                     \"context\": docs2text(doc_list),\n",
    "        #                     \"contract\": question2text(question[\"question_content\"])\n",
    "        #                 })\n",
    "        #                 batch_doc_indices.append(q_idx)\n",
    "        #             else:\n",
    "        #                 question[\"generation\"] = \"\"\"{\"evaluation\":\"insufficient_information\",\n",
    "        #                 \"explanation\":\"CANNOT FIND ANY RELEVANT DOCUMENT\"}\"\"\"\n",
    "        for q_idx, question in enumerate(new_graph_state[\"questions\"]):\n",
    "            if q_idx in new_graph_state[\"current_question_index\"]:\n",
    "                if question[\"all_docs\"]!=[]:\n",
    "                    if len(question[\"direct_reference_text\"])!=0:\n",
    "                        batch_both_inputs.append({\n",
    "                                \"context\": docs2text(question[\"all_docs\"]),\n",
    "                                \"contract\": question2text(question[\"question_content\"]),\n",
    "                                \"new_query\": question[\"rewrite_question_content\"]\n",
    "                            })\n",
    "                        batch_both_indices.append(q_idx)\n",
    "                    else:\n",
    "                        batch_doc_inputs.append({\n",
    "                                \"context\": docs2text(question[\"all_docs\"]),\n",
    "                                \"contract\": question2text(question[\"question_content\"])\n",
    "                            })\n",
    "                        batch_doc_indices.append(q_idx)\n",
    "                elif question[\"filtered_web_search_docs\"]!= []:\n",
    "                    if len(question[\"direct_reference_text\"])!=0:\n",
    "                        batch_both_inputs.append({\n",
    "                                \"context\": docs2text(question[\"filtered_web_search_docs\"]),\n",
    "                                \"contract\": question2text(question[\"question_content\"]),\n",
    "                                \"new_query\": question[\"rewrite_question_content\"]\n",
    "                            })\n",
    "                        batch_both_indices.append(q_idx)\n",
    "                    else:\n",
    "                        batch_doc_inputs.append({\n",
    "                                \"context\": docs2text(question[\"filtered_web_search_docs\"]),\n",
    "                                \"contract\": question2text(question[\"question_content\"])\n",
    "                            })\n",
    "                        batch_doc_indices.append(q_idx)\n",
    "                else:\n",
    "                    question[\"generation\"] = \"\"\"{\"evaluation\":\"insufficient_information\",\n",
    "                            \"explanation\":\"CANNOT FIND ANY RELEVANT DOCUMENT\"}\"\"\"\n",
    "               \n",
    "    print(\"start both\")\n",
    "    both_responses = rag_chain_both.batch(batch_both_inputs)\n",
    "    both_responses = retry_failed_batches(batch_both_inputs, both_responses, rag_chain_both)\n",
    "    print(\"end both\")\n",
    "    print(\"start doc\")\n",
    "    \n",
    "    doc_responses = rag_chain_doc.batch(batch_doc_inputs)\n",
    "    doc_responses = retry_failed_batches(batch_doc_inputs, doc_responses, rag_chain_doc)\n",
    "    print(\"end doc\")\n",
    "    \n",
    "    for idx, response in enumerate(both_responses):\n",
    "        q_idx = batch_both_indices[idx]\n",
    "        new_graph_state[\"questions\"][q_idx][\"generation\"] = response\n",
    "    \n",
    "    for idx, response in enumerate(doc_responses):\n",
    "        q_idx = batch_doc_indices[idx]\n",
    "        new_graph_state[\"questions\"][q_idx][\"generation\"] = response\n",
    "    return new_graph_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### web_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_search(state):\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    new_graph_state = deepcopy(state)\n",
    "    input_batch = []\n",
    "    batch_indices=[]\n",
    "    # for q_idx,question in enumerate(new_graph_state[\"questions\"]):\n",
    "    #     if q_idx in new_graph_state[\"current_question_index\"]:\n",
    "    #         input_batch.append({\"query\": question[\"question_content\"].page_content})\n",
    "    #         batch_indices.append(q_idx)\n",
    "    # # Web search\n",
    "    # responses = web_search_tool.batch(input_batch)\n",
    "    # for idx, response in enumerate(responses):\n",
    "    #     print(f\"\"\"len(response) {len(response)}\"\"\")\n",
    "    #     print(f\"response {response}\")\n",
    "    #     q_idx = batch_indices[idx]\n",
    "    #     for d in response:\n",
    "    #         new_graph_state[\"questions\"][q_idx][\"web_search_docs\"].append(Document(page_content=d[\"content\"]))\n",
    "    #     text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=1024, chunk_overlap=30)\n",
    "    #     new_graph_state[\"questions\"][q_idx][\"web_search_docs\"] = text_splitter.split_documents(new_graph_state[\"questions\"][q_idx][\"web_search_docs\"])\n",
    "    \n",
    "    search_count=0\n",
    "    for q_idx, question in enumerate(new_graph_state[\"questions\"]):\n",
    "            if q_idx in new_graph_state[\"current_question_index\"]:\n",
    "                # if question[\"all_docs\"]==[]:\n",
    "                #     pass\n",
    "                doc_list = set()  # Dùng set để loại bỏ tài liệu trùng lặp\n",
    "                for p_idx, proposition in enumerate(question[\"propositions\"]):\n",
    "                    if proposition[\"filtered_documents\"]:\n",
    "                        doc_list.update(str(proposition[\"filtered_documents\"]))\n",
    "                \n",
    "                if len(question[\"direct_reference_text\"])!=0:\n",
    "                    # print(\"OH NOOOOOOOOOOOOOOO\")\n",
    "                    for ref_p_idx, proposition in enumerate(question[\"ref_propositions\"]):\n",
    "                        if proposition[\"filtered_documents\"]:\n",
    "                            doc_list.update(str(proposition[\"filtered_documents\"]))\n",
    "                doc_list=[Document(doc) for doc in doc_list]\n",
    "                if doc_list:\n",
    "                    question[\"all_docs\"]=doc_list\n",
    "                else:\n",
    "                    search_count+=1\n",
    "                    if question[\"rewrite_question_content\"]!=\"\":\n",
    "                        input_batch.append({\"query\": question[\"rewrite_question_content\"][:100]})\n",
    "                    else:\n",
    "                        input_batch.append({\"query\": question[\"question_content\"].page_content[:100]})\n",
    "                    batch_indices.append(q_idx)\n",
    "    responses = web_search_tool.batch(input_batch)\n",
    "    print(f\"websearch input {input_batch}\")\n",
    "    for idx, response in enumerate(responses):\n",
    "        # print(f\"\"\"len(response) {len(response)}\"\"\")\n",
    "        # print(f\"response {response}\")\n",
    "        q_idx = batch_indices[idx]\n",
    "        for d in response:\n",
    "            new_graph_state[\"questions\"][q_idx][\"web_search_docs\"].append(Document(page_content=d[\"content\"]))\n",
    "        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=512, chunk_overlap=30)\n",
    "        new_graph_state[\"questions\"][q_idx][\"web_search_docs\"] = text_splitter.split_documents(new_graph_state[\"questions\"][q_idx][\"web_search_docs\"])\n",
    "    print(f\"HAVE SEARCH DOCS FOR: {search_count} CHUNKS\")\n",
    "    return new_graph_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### grade_web_search_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_web_search_docs(state):\n",
    "    def question2text(question):\n",
    "        return f\"\"\"第{question.metadata[\"chapter_number\"]}章　{question.metadata[\"chapter_title\"]} \\n 第{question.metadata[\"article_number\"]}条（{question.metadata[\"article_title\"]}）\\n{question.page_content}\"\"\"\n",
    "   \n",
    "    print(\"---GRADE WEB DOCUMENTS---\")\n",
    "    \n",
    "    new_graph_state = deepcopy(state)  # Tạo bản sao để tránh sửa đổi trực tiếp vào state gốc\n",
    "    batch_inputs = []\n",
    "    batch_map = []  # Mỗi phần tử là (q_idx, doc_idx)\n",
    "\n",
    "    for q_idx, question in enumerate(new_graph_state[\"questions\"]):\n",
    "        if q_idx in new_graph_state[\"current_question_index\"]:\n",
    "            contract= \"\"\n",
    "            if question[\"rewrite_question_content\"]!=\"\":\n",
    "                contract=question[\"rewrite_question_content\"]\n",
    "            else: \n",
    "                contract= question2text(question[\"question_content\"])\n",
    "            for doc_idx, doc in enumerate(question[\"web_search_docs\"]):  # Sửa lỗi unpacking\n",
    "                batch_inputs.append({\n",
    "                    \"context\": doc.page_content,\n",
    "                    \"contract\": contract\n",
    "                })\n",
    "                batch_map.append((q_idx, doc_idx))\n",
    "    print(f\"len(batch_map) {len(batch_map)}\")\n",
    "    # Xử lý batch nếu có dữ liệu\n",
    "    responses = retrieval_grader.batch(batch_inputs) if batch_inputs else []\n",
    "    responses = retry_failed_batches(batch_inputs, responses, retrieval_grader)\n",
    "\n",
    "    count = 0  # Đếm số tài liệu hợp lệ còn lại\n",
    "\n",
    "    # Cập nhật new_graph_state với kết quả đánh giá\n",
    "    for map_idx, (q_idx, doc_idx) in enumerate(batch_map):\n",
    "        response = responses[map_idx][\"binary_score\"].strip().lower() if responses else \"\"\n",
    "        if response == \"yes\":\n",
    "            # Chỉ giữ lại tài liệu được chấp nhận\n",
    "            new_graph_state[\"questions\"][q_idx][\"filtered_web_search_docs\"].append(new_graph_state[\"questions\"][q_idx][\"web_search_docs\"][doc_idx])\n",
    "            count += 1  # Tăng biến đếm số tài liệu hợp lệ\n",
    "\n",
    "    print(f\"---Have filtered and have {count} docs left---\")\n",
    "    return new_graph_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### grade answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_grade_node(state):\n",
    "    def docs2text(docs):\n",
    "        return '\\n'.join([doc.page_content for doc in docs])\n",
    "    def question2text(question):\n",
    "        return f\"\"\"第{question.metadata[\"chapter_number\"]}章　{question.metadata[\"chapter_title\"]} \\n 第{question.metadata[\"article_number\"]}条（{question.metadata[\"article_title\"]}）\\n{question.page_content}\"\"\"\n",
    "    \n",
    "    print(\"---HALLUCINATION GRADE---\")\n",
    "    new_graph_state = deepcopy(state)\n",
    "    batch_inputs = []\n",
    "    batch_indices = []\n",
    "    for q_idx, question in enumerate(new_graph_state[\"questions\"]):\n",
    "        if len(new_graph_state[\"question_to_regen_index\"])!=0:\n",
    "            if q_idx in new_graph_state[\"question_to_regen_index\"]:\n",
    "                doc_list = set()  # Dùng set để loại bỏ tài liệu trùng lặp\n",
    "                for p_idx, proposition in enumerate(question[\"propositions\"]):\n",
    "                    if proposition[\"filtered_documents\"]:\n",
    "                        doc_list.update(str(proposition[\"filtered_documents\"]))\n",
    "                if len(question[\"direct_reference_text\"])!=0:\n",
    "                    # print(\"OH NOOOOOOOOOOOOOOO\")\n",
    "                    for ref_p_idx, proposition in enumerate(question[\"ref_propositions\"]):\n",
    "                        if proposition[\"filtered_documents\"]:\n",
    "                            doc_list.update(str(proposition[\"filtered_documents\"]))\n",
    "                doc_list=[Document(doc) for doc in doc_list]\n",
    "                batch_inputs.append({\"laws\":docs2text(doc_list),\"contract\":question2text(question[\"question_content\"]),\"explanation\":question[\"generation\"]})\n",
    "                batch_indices.append(q_idx)\n",
    "        else:\n",
    "            if q_idx in new_graph_state[\"current_question_index\"] and question[\"generation\"]!=\"\"\"{\"evaluation\":\"insufficient_information\",\n",
    "                        \"explanation\":\"CANNOT FIND ANY RELEVANT DOCUMENT\"}\"\"\":\n",
    "            # print(f\"\"\"============== {question[\"generation\"]}\"\"\")\n",
    "                doc_list = set()  # Dùng set để loại bỏ tài liệu trùng lặp\n",
    "                for p_idx, proposition in enumerate(question[\"propositions\"]):\n",
    "                    if proposition[\"filtered_documents\"]:\n",
    "                        doc_list.update(str(proposition[\"filtered_documents\"]))\n",
    "                if len(question[\"direct_reference_text\"])!=0:\n",
    "                    # print(\"OH NOOOOOOOOOOOOOOO\")\n",
    "                    for ref_p_idx, proposition in enumerate(question[\"ref_propositions\"]):\n",
    "                        if proposition[\"filtered_documents\"]:\n",
    "                            doc_list.update(str(proposition[\"filtered_documents\"]))\n",
    "                doc_list=[Document(doc) for doc in doc_list]\n",
    "                batch_inputs.append({\"laws\":docs2text(doc_list),\"contract\":question2text(question[\"question_content\"]),\"explanation\":question[\"generation\"]})\n",
    "                batch_indices.append(q_idx)\n",
    "    responses = hallucination_grader.batch(batch_inputs)\n",
    "    responses = retry_failed_batches(batch_inputs, responses, hallucination_grader)\n",
    "    new_graph_state[\"question_to_regen_index\"]=[]\n",
    "    for idx, response in enumerate(responses):\n",
    "        q_idx = batch_indices[idx]\n",
    "        if response[\"is_supported\"].lower()==\"no\":\n",
    "            new_graph_state[\"question_to_regen_index\"].append(q_idx)\n",
    "    print(\"---GRADE ANSWER---\")\n",
    "    \n",
    "    grade_batch_inputs =[]\n",
    "    grade_batch_indicates =[]\n",
    "    for q_idx, question in enumerate(new_graph_state[\"questions\"]):\n",
    "        if q_idx in new_graph_state[\"current_question_index\"]:\n",
    "            if q_idx not in new_graph_state[\"question_to_regen_index\"] and question[\"generation\"]!=\"\"\"{\"evaluation\":\"insufficient_information\",\n",
    "                        \"explanation\":\"CANNOT FIND ANY RELEVANT DOCUMENT\"}\"\"\":\n",
    "                grade_batch_inputs.append({\"contract\":question2text(question[\"question_content\"]),\"explanation\":question[\"generation\"]})\n",
    "                grade_batch_indicates.append(q_idx)\n",
    "    grade_responses = answer_grader.batch(grade_batch_inputs)\n",
    "    grade_responses = retry_failed_batches(grade_batch_inputs, grade_responses, answer_grader)\n",
    "    for idx, response in enumerate(grade_responses):\n",
    "        q_idx = grade_batch_indicates[idx]\n",
    "        if response[\"is_valid_explanation\"].lower()==\"no\":\n",
    "            new_graph_state[\"question_to_regen_index\"].append(q_idx)\n",
    "    return new_graph_state\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rewrite_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_query(state):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_or_update_index(collection_name: str, embedding_model, persist_directory: str, \n",
    "                           all_docs: List[Document], clear_persist_folder: bool = False):\n",
    "    \"\"\"\n",
    "    Tạo hoặc cập nhật index vector.\n",
    "    - Nếu `clear_persist_folder=True`, sẽ xóa thư mục lưu trữ trước khi tạo lại từ đầu.\n",
    "    - Nếu `clear_persist_folder=False`, sẽ giữ dữ liệu cũ và chỉ cập nhật tài liệu mới.\n",
    "    \"\"\"\n",
    "    pf = Path(persist_directory)\n",
    "    if clear_persist_folder:\n",
    "        if pf.exists() and pf.is_dir():\n",
    "            print(f\"Deleting the content of: {pf}\")\n",
    "            shutil.rmtree(pf)\n",
    "        pf.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"Recreated the directory at: {pf}\")\n",
    "\n",
    "    print(\"\\nGenerating and persisting the embeddings..\")\n",
    "    print(\"Persist Directory:\", persist_directory)\n",
    "    \n",
    "    vectordb = Chroma.from_documents(\n",
    "        collection_name=collection_name,\n",
    "        documents=all_docs, \n",
    "        embedding=embedding_model, \n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "    vectordb.persist()\n",
    "    \n",
    "    # Đóng connection để tránh lỗi \"readonly database\"\n",
    "    # del vectordb\n",
    "    # gc.collect()\n",
    "    return vectordb\n",
    "\n",
    "def sematic_reference(state):\n",
    "    \"\"\"\n",
    "    Thực hiện embedding giữa các câu trong cùng một điều rồi lấy ra các câu có embedding gần nhau nhất, \n",
    "    sau đó đưa cho LLM để đánh giá có phải là câu liên quan trực tiếp không.\n",
    "    Sử dụng ChromaDB để lấy ra các câu có embedding gần nhau nhất.\n",
    "    \"\"\"\n",
    "    print(\"---SEMANTIC REFERENCE---\")\n",
    "\n",
    "    # Tạo một bản sao của state để tránh ghi đè\n",
    "    new_state = deepcopy(state)\n",
    "\n",
    "    for key, value in new_state.items():\n",
    "        print(key, value)\n",
    "\n",
    "    questions = new_state[\"questions\"]\n",
    "    \n",
    "    # tạo ra một list chứa các câu trong cùng một điều\n",
    "    question_same_dieu_dict = {}\n",
    "    for question in questions:\n",
    "        dieu_id = question[\"question_content\"].metadata[\"article_title\"] + \"_\" + question[\"question_content\"].metadata[\"article_number\"]\n",
    "        if dieu_id in question_same_dieu_dict:\n",
    "            question_same_dieu_dict[dieu_id].append(question)\n",
    "        else:\n",
    "            question_same_dieu_dict[dieu_id] = [question]\n",
    "\n",
    "    sematic_reference_dict = {}\n",
    "    # duyệt qua từng điều trong question_same_dieu_dict\n",
    "    for dieu, value in question_same_dieu_dict.items(): \n",
    "        print(\"dieu\", dieu)\n",
    "        print(\"value\", value)\n",
    "        new_value = [val[\"question_content\"] for val in value]\n",
    "        vectordb = create_or_update_index(\n",
    "            collection_name=\"same_dieu\",\n",
    "            embedding_model=embedding_function,\n",
    "            persist_directory=\"./same_article_db\",\n",
    "            all_docs=new_value,\n",
    "            clear_persist_folder=True  # Xóa dữ liệu cũ nếu có, tạo lại từ đầu\n",
    "        )\n",
    "        retriever_dieu = vectordb.as_retriever(search_kwargs={\"k\": 6})\n",
    "        # Duyệt qua từng câu trong value để tìm các câu có embedding gần nhau nhất\n",
    "        for question in value:\n",
    "            docs = retriever_dieu.get_relevant_documents(question[\"question_content\"].page_content)\n",
    "            docs = [doc for doc in docs if doc.page_content != question[\"question_content\"].page_content]\n",
    "            id_question = question[\"question_content\"].page_content\n",
    "            sematic_reference_dict[id_question] = docs\n",
    "\n",
    "    for question in questions:\n",
    "        id_question = question[\"question_content\"].page_content\n",
    "        question[\"sematic_reference\"] = sematic_reference_dict[id_question]\n",
    "\n",
    "    new_state[\"questions\"] = questions\n",
    "    return new_state  # Trả về state mới\n",
    "\n",
    "\n",
    "### Edges ###\n",
    "\n",
    "def route_question(state):\n",
    "    print(\"---ROUTE QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    source = question_router.invoke({\"question\": question})   \n",
    "    if source[\"datasource\"] == 'web_search':\n",
    "        print(\"---ROUTE QUESTION TO WEB SEARCH---\")\n",
    "        return \"web_search\"\n",
    "    elif source[\"datasource\"] == 'vectorstore':\n",
    "        print(\"---ROUTE QUESTION TO RAG---\")\n",
    "        return \"vectorstore\"\n",
    "\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    score = hallucination_grader.invoke({\"laws\": documents, \"explanation\": generation})\n",
    "    grade = score[\"is_supported\"]\n",
    "\n",
    "    # Check hallucination\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        # Check question-answering\n",
    "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "        return \"useful\"\n",
    "        # score = answer_grader.invoke({\"question\": question,\"generation\": generation})\n",
    "        # grade = score.binary_score\n",
    "        # if grade == \"yes\":\n",
    "        #     print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "        #     return \"useful\"\n",
    "        # else:\n",
    "        #     print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "        #     return \"not useful\"\n",
    "    else:\n",
    "        print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"not supported\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "def should_create_proposition(graph_state):\n",
    "    \"\"\"Kiểm tra xem có cần chạy create_proposition không.\"\"\"\n",
    "    if len(graph_state[\"current_question_index\"]) != 0:  # True nếu có câu hỏi có thể chạy\n",
    "        print(\"STILL HAVE INDEPENDENT CHUNK\")\n",
    "        return \"STILL HAVE INDEPENDENT CHUNK\"\n",
    "    else:\n",
    "        print(\"NO INDEPENDENT CHUNK\")\n",
    "        return \"NO INDEPENDENT CHUNK\"\n",
    "\n",
    "# # Define the nodes\n",
    "workflow.add_node(\"direct_reference\", direct_reference)\n",
    "workflow.add_node(\"grade_direct_reference\", grade_direct_reference)\n",
    "workflow.add_node(\"rewrite_question_content\", rewrite_question_content)\n",
    "workflow.add_node(\"loopback_node\",loopback_node) \n",
    "# workflow.add_node(\"sematic_reference\", sematic_reference) \n",
    "workflow.add_node(\"create_propositions\", create_propositions) \n",
    "# workflow.add_node(\"conditional_propositions\", conditional_propositions) \n",
    "workflow.add_node(\"retrieve\", retrieve) \n",
    "workflow.add_node(\"grade_documents\", grade_documents)\n",
    "workflow.add_node(\"generation\", generate)\n",
    "# workflow.add_node(\"hallucination\", hallucination)\n",
    "workflow.add_node(\"answer_grade_node\", answer_grade_node)\n",
    "\n",
    "workflow.add_node(\"query_transformation\", query_transformation)\n",
    "workflow.add_node(\"check_empty_propositions\", check_empty_propositions)\n",
    "workflow.add_node(\"web_search\", web_search)\n",
    "workflow.add_node(\"grade_web_search_docs\", grade_web_search_docs)\n",
    "\n",
    "\n",
    "workflow.set_entry_point(\"direct_reference\")\n",
    "workflow.add_edge(\"direct_reference\", \"grade_direct_reference\")\n",
    "workflow.add_edge(\"grade_direct_reference\", \"rewrite_question_content\")\n",
    "workflow.add_edge(\"rewrite_question_content\", \"loopback_node\")\n",
    "# workflow.add_edge(\"direct_reference\", \"grade_direct_reference\")\n",
    "# workflow.add_edge(\"sematic_reference\", \"grade_direct_reference\")\n",
    "workflow.add_conditional_edges( \"loopback_node\",\n",
    "                               should_create_proposition,\n",
    "                               {\n",
    "                                   \"STILL HAVE INDEPENDENT CHUNK\":\"create_propositions\",\n",
    "                                   \"NO INDEPENDENT CHUNK\":END\n",
    "                               })  \n",
    "workflow.add_edge(\"create_propositions\", \"retrieve\")  \n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_edge(\"grade_documents\",\"check_empty_propositions\")\n",
    "def check_related_laws(state):\n",
    "    if len(state[\"propositions_to_rewrite\"])!=0: \n",
    "        if state[\"rewrite_count\"]<1:\n",
    "            return \"CANNOT FIND RELATED LAWS\"\n",
    "        else: return \"web_search\"   ######################\n",
    "    else: return \"web_search\"\n",
    "workflow.add_conditional_edges(\n",
    "    \"check_empty_propositions\",\n",
    "    check_related_laws,\n",
    "    {\n",
    "        \"CANNOT FIND RELATED LAWS\":\"query_transformation\",\n",
    "        \"web_search\":\"web_search\"\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"web_search\",\"grade_web_search_docs\")\n",
    "workflow.add_edge(\"grade_web_search_docs\",\"generation\")\n",
    "\n",
    "# def check_hallucination(state):\n",
    "#     for question in state[\"questions\"]:\n",
    "#         if q_idx in new_graph_state[\"current_question_index\"]:\n",
    "#             if question[\"needed_to_regen\"] and question[\"regen_count\"]<=2:\n",
    "#                 return \"generation\"\n",
    "#     return \"answer_grader\"\n",
    "\n",
    "def check_answer(state):\n",
    "    if state[\"question_to_regen_index\"]!=[] and state[\"regen_count\"]<=1:\n",
    "            return \"UNSATISFACTORY ANSWER\"\n",
    "    return \"loopback_node\"\n",
    "\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"answer_grade_node\",\n",
    "    check_answer,\n",
    "    {\n",
    "        \"UNSATISFACTORY ANSWER\":\"generation\",\n",
    "        \"loopback_node\":\"loopback_node\"\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"query_transformation\",\"grade_documents\")\n",
    "workflow.add_edge(\"generation\", \"answer_grade_node\")\n",
    "\n",
    "# workflow.add_edge(\"hallucination\", \"loopback_node\")\n",
    "\n",
    "app = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# from langgraph.graph import END, StateGraph\n",
    "\n",
    "# websearch_workflow = StateGraph(GraphState)\n",
    "\n",
    "# def should_create_search_web(graph_state):\n",
    "#     \"\"\"Kiểm tra xem có cần chạy create_proposition không.\"\"\"\n",
    "#     if len(graph_state[\"current_question_index\"]) != 0:  # True nếu có câu hỏi có thể chạy\n",
    "#         print(\"STILL HAVE INDEPENDENT CHUNK\")\n",
    "#         return \"STILL HAVE INDEPENDENT CHUNK\"\n",
    "#     else:\n",
    "#         print(\"NO INDEPENDENT CHUNK\")\n",
    "#         return \"NO INDEPENDENT CHUNK\"\n",
    "\n",
    "# # # Define the nodes\n",
    "# websearch_workflow.add_node(\"direct_reference\", direct_reference)\n",
    "# websearch_workflow.add_node(\"grade_direct_reference\", grade_direct_reference)\n",
    "# websearch_workflow.add_node(\"rewrite_question_content\", rewrite_question_content)\n",
    "# websearch_workflow.add_node(\"loopback_node\",loopback_node) \n",
    "# websearch_workflow.add_node(\"web_search\", web_search) \n",
    "# websearch_workflow.add_node(\"grade_documents\", grade_documents)\n",
    "# websearch_workflow.add_node(\"generation\", generate)\n",
    "# # websearch_workflow.add_node(\"hallucination\", hallucination)\n",
    "\n",
    "# websearch_workflow.add_node(\"query_transformation\", query_transformation)\n",
    "# websearch_workflow.set_entry_point(\"direct_reference\")\n",
    "# websearch_workflow.add_edge(\"direct_reference\", \"grade_direct_reference\")\n",
    "# websearch_workflow.add_edge(\"grade_direct_reference\", \"rewrite_question_content\")\n",
    "# websearch_workflow.add_edge(\"rewrite_question_content\", \"loopback_node\")\n",
    "# websearch_workflow.add_conditional_edges( \"loopback_node\",\n",
    "#                                should_create_search_web,\n",
    "#                                {\n",
    "#                                    \"STILL HAVE INDEPENDENT CHUNK\":\"web_search\",\n",
    "#                                    \"NO INDEPENDENT CHUNK\":END\n",
    "#                                })  \n",
    "# # websearch_workflow.add_edge(\"create_propositions\", \"retrieve\")  \n",
    "# websearch_workflow.add_edge(\"web_search\", \"grade_documents\")\n",
    "# websearch_workflow.add_edge(\"grade_documents\",\"generation\")\n",
    "# # def decide_to_generate(state):\n",
    "# #     if len(state[\"propositions_to_rewrite\"])!=0: \n",
    "# #         if state[\"rewrite_count\"]<1:\n",
    "# #             return \"query_transformation\"\n",
    "# #         else: return \"generation\"   ######################\n",
    "# #     else: return \"generation\"\n",
    "# # websearch_workflow.add_conditional_edges(\n",
    "# #     \"grade_documents\",\n",
    "# #     decide_to_generate,\n",
    "# #     {\n",
    "# #         \"query_transformation\":\"query_transformation\",\n",
    "# #         \"generation\":\"generation\"\n",
    "# #     }\n",
    "# # )\n",
    "\n",
    "# # websearch_workflow.add_edge(\"query_transformation\",\"web_search\")\n",
    "# websearch_workflow.add_edge(\"generation\", \"loopback_node\")\n",
    "# # workflow.add_edge(\"hallucination\", \"loopback_node\")\n",
    "\n",
    "# websearch_app = websearch_workflow.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "websearch_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        websearch_app.get_graph().draw_mermaid_png(\n",
    "            draw_method=MermaidDrawMethod.API,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Toàn bộ pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "import time\n",
    "config = RunnableConfig(recursion_limit=200)\n",
    "\n",
    "start_time = time.time()  # Bắt đầu đo thời gian\n",
    "final_state = app.invoke(graph_state,config=config)\n",
    "end_time = time.time()  # Kết thúc đo thời gian\n",
    "\n",
    "print(f\"Thời gian chạy: {end_time - start_time:.6f} giây\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in final_state[\"questions\"]:\n",
    "    # if question[\"rewrite_question_content\"]!=\"\":\n",
    "        print(\"=======\")\n",
    "        print(question[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_state[\"questions\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "contract_terms = [\n",
    "    \"契約の目的: 本契約は、[A社]と[B社]の間で[取引・サービス]に関する権利義務を定めるものである。\",\n",
    "    \"契約期間: 本契約の有効期間は[開始日]から[終了日]までとする。ただし、当事者の合意により更新できるものとする。\",\n",
    "    \"支払条件: 甲は乙に対し、[金額]円を[支払期日]までに支払うものとする。\",\n",
    "    \"守秘義務: 当事者は、本契約に関連して知り得た機密情報を第三者に開示してはならない。\",\n",
    "    \"契約解除: 当事者のいずれかが本契約の条項に違反した場合、相手方は通知の上で契約を解除できる。\",\n",
    "    \"紛争解決: 本契約に関する紛争が生じた場合、[仲裁機関]において解決を図るものとする。\",\n",
    "    \"準拠法: 本契約は日本法に準拠し、解釈されるものとする。\",\n",
    "]\n",
    "start=time.time()\n",
    "for i in range(len(contract_terms)):\n",
    "    response= retriever.invoke(contract_terms[i])\n",
    "    print(response)\n",
    "    \n",
    "end=time.time()\n",
    "print(f\"thoi  gian chay tuan tu: {end-start}\")\n",
    "start=time.time()\n",
    "response =retriever.batch(contract_terms)\n",
    "print(response)\n",
    "end=time.time()\n",
    "print(f\"thoi  gian chay batch: {end-start}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for id, clause in final_state[\"root_text\"][\"clause\"].items():\n",
    "#     # print(id)\n",
    "#     # if id == ('9','1'):\n",
    "#     print(len(clause))\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Lấy độ dài của từng clause\n",
    "clause_lengths = [len(clause) for clause in final_state[\"root_text\"][\"clause\"].values()]\n",
    "\n",
    "# Đếm số lượng clause theo độ dài\n",
    "length_counts = Counter(clause_lengths)\n",
    "\n",
    "# Chuyển dữ liệu thành mảng numpy để vẽ biểu đồ\n",
    "x_values = np.array(list(length_counts.keys()))  # Độ dài của clause\n",
    "y_values = np.array(list(length_counts.values()))  # Số lượng clause có độ dài đó\n",
    "\n",
    "# Vẽ biểu đồ\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(x_values, y_values, color=\"skyblue\", edgecolor=\"black\")\n",
    "\n",
    "# Gắn nhãn trục\n",
    "plt.xlabel(\"Length of Clause\")\n",
    "plt.ylabel(\"Number of Clauses\")\n",
    "plt.title(\"Distribution of Clause Lengths\")\n",
    "\n",
    "# Hiển thị giá trị trên các cột\n",
    "for i in range(len(x_values)):\n",
    "    plt.text(x_values[i], y_values[i] + 0.5, str(y_values[i]), ha=\"center\", fontsize=10)\n",
    "\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "# Hiển thị biểu đồ\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_json = []\n",
    "\n",
    "for question in final_state[\"questions\"]:\n",
    "    docs = []\n",
    "    \n",
    "    for direct_proposition in question[\"propositions\"]:\n",
    "        for doc in direct_proposition[\"filtered_documents\"]:\n",
    "            doc_dict = {\n",
    "                \"metadata\": doc.metadata,\n",
    "                \"page_content\": doc.page_content\n",
    "            }\n",
    "            if doc_dict not in docs:  # Tránh trùng lặp\n",
    "                docs.append(doc_dict)\n",
    "\n",
    "    for ref_proposition in question[\"ref_propositions\"]:\n",
    "        for doc in ref_proposition[\"filtered_documents\"]:\n",
    "            doc_dict = {\n",
    "                \"metadata\": doc.metadata,\n",
    "                \"page_content\": doc.page_content\n",
    "            }\n",
    "            if doc_dict not in docs:  # Tránh trùng lặp\n",
    "                docs.append(doc_dict)\n",
    "    \n",
    "    response_json.append({\n",
    "        \"question\": {\n",
    "                \"metadata\": question[\"question_content\"].metadata,\n",
    "                \"page_content\": question[\"question_content\"].page_content\n",
    "            },\n",
    "        \"documents\": docs,\n",
    "        \"response\": json.loads(question[\"generation\"]) if type(question[\"generation\"])==str else question[\"generation\"] \n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lưu JSON vào file\n",
    "output_path = \"/home/thanhnguyen/Data/code/RAG_techniques/output_3chapters.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(response_json, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Đã lưu JSON vào: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "response_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Lưu vào tệp\n",
    "with open(\"/home/thanhnguyen/Data/code/RAG_techniques/output_3chapters.pkl\", \"wb\") as f:\n",
    "    pickle.dump(final_state, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đọc lại từ tệp\n",
    "import pickle\n",
    "\n",
    "with open(\"/home/thanhnguyen/Data/code/RAG_techniques/output_3chapters.pkl\", \"rb\") as f:\n",
    "    loaded_data = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test từng node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### separate chunks+ metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "def convert_japanese_numbers(text):\n",
    "    jp_numbers = \"０１２３４５６７８９\"\n",
    "\n",
    "    latin_numbers = \"0123456789\"\n",
    "    translation_table = str.maketrans(jp_numbers, latin_numbers)\n",
    "    return text.translate(translation_table)\n",
    "\n",
    "def get_list_number(paragraph):\n",
    "    \"\"\"Trích xuất số thứ tự của danh sách nếu có\"\"\"\n",
    "    ns = {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}\n",
    "    xml = paragraph._element\n",
    "    numPr = xml.find('.//w:numPr', ns)\n",
    "    \n",
    "    if numPr is not None:\n",
    "        numId = numPr.find('.//w:numId', ns)\n",
    "        ilvl = numPr.find('.//w:ilvl', ns)\n",
    "        \n",
    "        if numId is not None and ilvl is not None:\n",
    "            return f\"{int(ilvl.get('{http://schemas.openxmlformats.org/wordprocessingml/2006/main}val')) + 1}．\"\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "def extract_text_with_numbering(docx_path):\n",
    "    \"\"\"Trích xuất nội dung từ file .docx kèm theo số thứ tự danh sách\"\"\"\n",
    "    doc = Document(docx_path)\n",
    "    extracted_text = []\n",
    "\n",
    "    for index,para in enumerate(doc.paragraphs):\n",
    "        list_number = get_list_number(para)\n",
    "        text = para.text.strip()\n",
    "        # print(f\"index {index} \")\n",
    "        # print(f\"para {para.text} \")\n",
    "        if text:\n",
    "            if list_number:\n",
    "                extracted_text.append(f\"{list_number} {text}\")\n",
    "            else:\n",
    "                extracted_text.append(text)\n",
    "\n",
    "    return '@'.join(extracted_text)\n",
    "\n",
    "\n",
    "def circled_to_int(circled):\n",
    "    # Bản đồ chuyển các ký hiệu số tròn sang số nguyên.\n",
    "    mapping = {\n",
    "    # Số 1 đến 20: ① (U+2460) -> ⑳ (U+2473)\n",
    "    '①': \"1\",  '②': \"2\",  '③': \"3\",  '④': \"4\",  '⑤': \"5\",\n",
    "    '⑥': \"6\",  '⑦': \"7\",  '⑧': \"8\",  '⑨': \"9\",  '⑩': \"10\",\n",
    "    '⑪': \"11\", '⑫': \"12\", '⑬': \"13\", '⑭': \"14\", '⑮': \"15\",\n",
    "    '⑯': \"16\", '⑰': \"17\", '⑱': \"18\", '⑲': \"19\", '⑳': \"20\",\n",
    "    \n",
    "    # Số 21 đến 35: ㉑ (U+3251) -> ㉟ (U+325F)\n",
    "    '㉑': \"21\", '㉒': \"22\", '㉓': \"23\", '㉔': \"24\", '㉕': \"25\",\n",
    "    '㉖': \"26\", '㉗': \"27\", '㉘': \"28\", '㉙': \"29\", '㉚': \"30\",\n",
    "    '㉛': \"31\", '㉜': \"32\", '㉝': \"33\", '㉞': \"34\", '㉟': \"35\",\n",
    "    \n",
    "    # Số 36 đến 50: ㊱ (U+32B1) -> ㊿ (U+32BF)\n",
    "    '㊱': \"36\", '㊲': \"37\", '㊳': \"38\", '㊴': \"39\", '㊵': \"40\",\n",
    "    '㊶': \"41\", '㊷': \"42\", '㊸': \"43\", '㊹': \"44\", '㊺': \"45\",\n",
    "    '㊻': \"46\", '㊼': \"47\", '㊽': \"48\", '㊾': \"49\", '㊿': \"50\",\n",
    "}\n",
    "\n",
    "    return mapping.get(circled, None)\n",
    "\n",
    "def split_and_convert(text):\n",
    "    pattern = r'@((?:[①-⑳]|[㉑-㉟]|[㊱-㊿]|\\(\\d+\\)|（\\d+）))(.*?)(?=@(?:[①-⑳]|[㉑-㉟]|[㊱-㊿]|\\(\\d+\\)|（\\d+）)|$)'\n",
    "    matches = re.finditer(pattern, text, re.DOTALL)\n",
    "    # print(f\"text {text}\")\n",
    "    # print(f\"\"\"pre text '{text.split(\"①\")[0]}'\"\"\")\n",
    "    result = []\n",
    "    for m in matches:\n",
    "        marker = m.group(1).strip()  # Ký hiệu số (①, (2), （3）)\n",
    "        content = m.group(2).strip()  # Nội dung tương ứng\n",
    "        \n",
    "        # Xử lý marker thành số nguyên\n",
    "        if re.match(r'[（）()]\\d+[（）()]', marker):  # Nếu là dạng (1) hoặc （1）\n",
    "            number = marker.strip(\"（）()\")  # Loại bỏ dấu ngoặc\n",
    "        else:  # Nếu là dạng số tròn (①, ②, ...)\n",
    "            number = circled_to_int(marker)\n",
    "\n",
    "        result.append({\"sub_clause_number\": number, \"sub_clause_content\": content.replace(\"@\",\"\")})\n",
    "    \n",
    "    return result\n",
    "\n",
    "def extract_structure(docx_path):\n",
    "    # doc = Document(docx_path)\n",
    "    # full_text = ' '.join([para.text.strip() for para in doc.paragraphs if para.text.strip()])\n",
    "    full_text =extract_text_with_numbering(docx_path)\n",
    "    full_text = convert_japanese_numbers(full_text)  # Nếu có hàm này\n",
    "    full_text=full_text.replace(\")\",\"）\")\n",
    "    full_text=full_text.replace(\"(\",\"（\")\n",
    "    # print(f\"full text {full_text}\")\n",
    "    data = {\"chapters\": []}\n",
    "\n",
    "    # Tìm tất cả chương với nội dung tương ứng\n",
    "    chapters = re.finditer(r'第([0-9]+)章　(.*?)(?=第[0-9]+章　|\\Z)', full_text, re.DOTALL)\n",
    "    chapter_dict = {}\n",
    "    article_dict = {}\n",
    "    clause_dict = {}\n",
    "    for chapter in chapters:\n",
    "        chapter_num = chapter.group(1)\n",
    "        chapter_title = chapter.group(2).strip().split('@')[0]\n",
    "        chapter_content = chapter.group(0)\n",
    "        # print(f\"chapter_content {chapter_content}\")\n",
    "        # Tìm các điều trong chương\n",
    "        articles = re.finditer(r'@第([0-9]+)条\\s*（(.*?)）(.*?)(?=@第[0-9]+条\\s*（|\\Z)', chapter_content, re.DOTALL)\n",
    "        articles_data = []\n",
    "        for article in articles:\n",
    "            article_num = article.group(1)\n",
    "            # print(f\"article_num {article_num}\")\n",
    "            article_title = article.group(2).strip()\n",
    "            article_text = article.group(3).strip()\n",
    "            # print(f\"article_text {article_text}\")\n",
    "            article_contents = re.finditer(r'@([1-9]+)．(.*?)(?=@[1-9]．|\\Z)', article_text, re.DOTALL)\n",
    "            articles_content_data=[]\n",
    "            \n",
    "            for content in article_contents:\n",
    "                article_content_num = content.group(1)\n",
    "                article_content_text = content.group(2).strip()\n",
    "                # print(article_content_text)\n",
    "            # Tìm nội dung clause (bắt đầu bằng ①, ②, 1., - ...)\n",
    "                clause_match = re.split(r'(@①|@1\\.|@-|@（[0-9]+）)', article_content_text, maxsplit=1)\n",
    "                # clause_match = re.split(r'(①|②|③|1\\.|-|\\([0-9]+\\))', article_content_text, maxsplit=1, flags=re.MULTILINE)\n",
    "                clause_text_temp=article_content_text\n",
    "                parts=[]\n",
    "                if len(clause_match) > 1:\n",
    "                    article_content_text = clause_match[0].strip()  # Lấy phần trước danh sách\n",
    "                    clause_content = ''.join(clause_match[1:]).strip()  # Lấy phần còn lại\n",
    "                    \n",
    "                    parts=split_and_convert(clause_content)\n",
    "\n",
    "                else:\n",
    "                    clause_content = \"\"\n",
    "                # print(f\"parts {parts}\")\n",
    "                articles_content_data.append({\n",
    "                    \"clause_number\": article_content_num,\n",
    "                    \"clause_title\": article_content_text,\n",
    "                    # \"clause_content\": clause_content,\n",
    "                    \"sub_clauses\":parts\n",
    "                })\n",
    "                clause_dict[(article_num,article_content_num)] ={\n",
    "                    \"chapter\":f\"\"\"第{chapter_num.replace(\"@\",\"\")}章　{chapter_title.replace(\"@\",\"\")} \\n \"\"\",\n",
    "                    \"article\":f\"\"\"第{article_num.replace(\"@\",\"\")}条（{article_title.replace(\"@\",\"\")}）\\n \"\"\",\n",
    "                    \"clause\":f\"\"\"{article_content_num.replace(\"@\",\"\")}．\"\"\",\n",
    "                    \"sub_clause\":\"-1\",\n",
    "                    \"text\":f\"\"\"{clause_text_temp.replace(\"@\",\"\")}\"\"\"\n",
    "                } \n",
    "            articles_data.append({\n",
    "                    \"article_number\": article_num,\n",
    "                    \"article_title\": article_title,\n",
    "                    # \"article_text\":article_text,\n",
    "                    \"clauses\": articles_content_data,\n",
    "                    # \"clause_content\": clause_content\n",
    "                })\n",
    "            article_dict[article_num] = {\n",
    "                \"chapter\":f\"\"\"第{chapter_num.replace(\"@\",\"\")}章　{chapter_title.replace(\"@\",\"\")} \\n \"\"\",\n",
    "                \"article\":f\"\"\"第{article_num.replace(\"@\",\"\")}条（{article_title.replace(\"@\",\"\")}）\\n \"\"\",\n",
    "                \"clause\":\"-1\",\n",
    "                \"sub_clause\":\"-1\",\n",
    "                \"text\":f\"\"\"{article_text.replace(\"@\",\"\")}\"\"\"\n",
    "            }\n",
    "            \n",
    "            \n",
    "        data[\"chapters\"].append({\n",
    "            \"chapter_number\": chapter_num,\n",
    "            \"chapter_title\": chapter_title,\n",
    "            # \"chapter_text\": chapter_content,\n",
    "            \"articles\": articles_data\n",
    "        })\n",
    "    #     chapter_dict[chapter_num] =  f\"\"\"第{chapter_num}章　{chapter_title} \\n{chapter_content}\"\"\"\n",
    "        chapter_dict[chapter_num] =  {\n",
    "                \"chapter\":f\"\"\"第{chapter_num.replace(\"@\",\"\")}章　{chapter_title.replace(\"@\",\"\")} \\n \"\"\",\n",
    "                \"article\":\"-1\",\n",
    "                \"clause\":\"-1\",\n",
    "                \"sub_clause\":\"-1\",\n",
    "                \"text\":f\"\"\"{chapter.group(2).strip().split('@',1)[1].replace(\"@\",\"\")}\"\"\"\n",
    "      \n",
    "        } \n",
    "    root_text={\"Chapter\":chapter_dict, \"Article\":article_dict, \"clause\":clause_dict}\n",
    "    \n",
    "    return data,root_text\n",
    "\n",
    "def save_json(data, output_path):\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Sử dụng\n",
    "docx_path = '/home/thanhnguyen/Data/code/RAG_techniques/①就業規則_ひな形_20240513.docx'\n",
    "json_path = '①就業規則_ひな形_20240513.json'\n",
    "\n",
    "data,root_text = extract_structure(docx_path)\n",
    "save_json(data, json_path)\n",
    "\n",
    "normalized_data= my_utils.normalize_dict(data)\n",
    "useful_data = {'chapters':normalized_data['chapters'][0:2]}\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "docs_list = [\n",
    "    Document(\n",
    "        page_content=sub_clause.get(\"sub_clause_content\") if sub_clause else clause[\"clause_title\"],\n",
    "        metadata={\n",
    "            \"chapter_title\": chapter['chapter_title'].replace(\"@\",\"\"),\n",
    "            \"chapter_number\": chapter['chapter_number'].replace(\"@\",\"\"),\n",
    "            # \"chapter_text\": chapter['chapter_text'].replace(\"@\",\"\"),\n",
    "            \"article_title\": article['article_title'].replace(\"@\",\"\"), \n",
    "            \"article_number\": article['article_number'].replace(\"@\",\"\"),\n",
    "            # \"article_text\": article['article_text'].replace(\"@\",\"\"),\n",
    "            \"clause_number\": clause['clause_number'].replace(\"@\",\"\"),\n",
    "            \"clause_title\": clause[\"clause_title\"].replace(\"@\",\"\"),\n",
    "            # \"clause_text\": clause[\"clause_content\"].replace(\"@\",\"\"),\n",
    "            \"sub_clause_number\": sub_clause.get(\"sub_clause_number\").replace(\"@\",\"\") if sub_clause else \"\",\n",
    "            \"sub_clause_content\": sub_clause.get(\"sub_clause_content\").replace(\"@\",\"\") if sub_clause else \"\",\n",
    "\n",
    "            # \"clause_content\":  clause['clause_title']+\"\\n\" if clause['clause_content'] else \"\",\n",
    "            \n",
    "        }\n",
    "    )\n",
    "    for chapter in useful_data['chapters'] \n",
    "    for article in chapter['articles']\n",
    "    for clause in article['clauses']\n",
    "    for sub_clause in (clause[\"sub_clauses\"] or [None])  # Nếu rỗng thì tạo danh sách chứa None\n",
    "]\n",
    "\n",
    "docs_list\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=500, chunk_overlap=30\n",
    ")\n",
    "\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "for i, doc in enumerate(doc_splits):\n",
    "    doc.metadata['chunk_id'] = i+1 ### adding chunk id\n",
    "\n",
    "graph_state = {\"questions\":\n",
    "            [\n",
    "                {\"question_content\":Document(\n",
    "                    page_content=doc_split.metadata['clause_title'] +\"\\n\"+ doc_split.page_content if doc_split.metadata['sub_clause_content'] else doc_split.page_content,\n",
    "                    metadata={\n",
    "                        \"chapter_title\": doc_split.metadata['chapter_title'],\n",
    "                        \"chapter_number\": doc_split.metadata['chapter_number'],\n",
    "                        # \"chapter_text\": doc_split.metadata['chapter_text'],\n",
    "                        \"article_title\": doc_split.metadata['article_title'], \n",
    "                        \"article_number\": doc_split.metadata['article_number'],\n",
    "                        # \"article_text\": doc_split.metadata['article_text'],\n",
    "                        \"clause_number\": doc_split.metadata['clause_number'],\n",
    "                        \"clause_title\": doc_split.metadata['clause_title'],\n",
    "                        # \"clause_text\": doc_split.metadata['clause_text'],\n",
    "                        \"sub_clause_number\": doc_split.metadata['sub_clause_number'],\n",
    "                        \"sub_clause_content\": doc_split.metadata['sub_clause_content'],\n",
    "                    }),\n",
    "                \"propositions\":[],\n",
    "                \"ref_propositions\":[],\n",
    "                \"all_docs\":[],\n",
    "                \"web_search_docs\":[],\n",
    "                \"filtered_web_search_docs\": []\n",
    "                }\n",
    " for doc_split in doc_splits],\n",
    "                \"root_text\":root_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "config = RunnableConfig(recursion_limit=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_state = app.invoke(graph_state,config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "count=0\n",
    "for question in final_state[\"questions\"]:\n",
    "    if question.get(\"generation\", \"\") != \"\":\n",
    "        count+=1\n",
    "        print(question['generation'])\n",
    "print(f\"amount of processed question: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### direct Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test =deepcopy(graph_state)\n",
    "# Test[\"questions\"]=[{'question_content': Document(metadata={'chapter_title': '労働時間', 'chapter_number': '3', 'article_title': '労働時間の定義', 'article_number': '10', 'clause_number': '1', 'clause_title': 'この規則で労働時間とは、従業員が業務に従事する時間を指し、次の通り定義する。', 'sub_clause_number': '3', 'sub_clause_content': '所定労働時間・・・・・・1日8時間、1週間40時間を超えない範囲とする。'}, page_content='この規則で労働時間とは、従業員が業務に従事する時間を指し、次の通り定義する。\\n所定労働時間・・・・・・1日8時間、1週間40時間を超えない範囲とする。'),\n",
    "#    'propositions': [],\n",
    "#    'ref_propositions': [],\n",
    "#    'direct_reference': [],\n",
    "#    'direct_reference_index': [],\n",
    "#    'direct_reference_text': [],\n",
    "#    'ref_reference': [],}\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_graph_state=direct_reference(graph_state)\n",
    "# reference_Test=direct_reference(Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 0\n",
    "# for question in reference_graph_state['questions']:\n",
    "#     if question[\"direct_reference\"] != []:\n",
    "#         count+=1\n",
    "# print(f\"count {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "reference_graph_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grade reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# count = 0\n",
    "# for question in grade_direct_reference_graph_state[\"questions\"]:\n",
    "#     if question[\"direct_reference_text\"]:\n",
    "#         print(len(question[\"merged_reference_text\"]))\n",
    "#         count+=1\n",
    "# print(f\"count {count}\")\n",
    "#         # count_graded_text=0\n",
    "#         # for ref_text in question[\"direct_reference_text\"]:\n",
    "#             # count_graded_text+=len(ref_text[\"text\"])\n",
    "#         # print(count_graded_text)\n",
    "#         # if count_graded_text==901:\n",
    "#             # print(question)\n",
    "# # print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_direct_reference_graph_state=grade_direct_reference(reference_graph_state)\n",
    "# grade_direct_reference_Test=grade_direct_reference(reference_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grade_direct_reference_graph_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\"\"第1章\\u3000総則 \\n \\n  第2条（労働者の定義及び適用範囲）\\n \\n    2．\\n      この規則で労働者とは第2章に定める手続きにより入社した者で、次の通り定義する。\\n短時間正社員・・・・契約社員・嘱託社員・パートタイマー・アルバイト・派遣社員以外の労働@者で期間の定めがなくフルタイム勤務で働く正社員に比べ短い労働時間で@雇用される労働者\\n        + \\n      この規則で労働者とは第2章に定める手続きにより入社した者で、次の通り定義する。\\nパート/アルバイト 契約期間の有無にかかわらず、業務の一部や臨時的に業務を行う労働者\\n        + \\n      この規則で労働者とは第2章に定める手続きにより入社した者で、次の通り定義する。\\n派遣社員・・・・・・派遣元事業所より派遣され、派遣元の指揮命令を受け就業する労働者\\n        + \\n      この規則で労働者とは第2章に定める手続きにより入社した者で、次の通り定義する。\\n嘱託社員・・・・・・定年退職後に引き続き有期労働契約により雇用される労働者。原則1年ご@との更新とし、個別に定めるものとする。\\n        + \\n      この規則で労働者とは第2章に定める手続きにより入社した者で、次の通り定義する。\\n勤務地限定社員・・・契約社員・アルバイト以外の労働者で期間の定めがなく勤務地が正社員と@比べ限定され雇用される労働者\\n        + \\n      この規則で労働者とは第2章に定める手続きにより入社した者で、次の通り定義する。\\n職務限定社員・・・・契約社員・アルバイト以外の労働者で期間の定めがなく職務が正社員と比@べ限定され雇用される労働者\\n        + \\n      この規則で労働者とは第2章に定める手続きにより入社した者で、次の通り定義する。\\n正社員・・・・・・・契約社員・嘱託社員・パートタイマー・アルバイト・派遣社員以外の労働@者で期間の定めがなく雇用される基幹業務に従事する労働者\\n        + '},\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(grade_direct_reference_graph_state[\"questions\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewrite_graph_state=rewrite_question_content(grade_direct_reference_graph_state)\n",
    "# rewrite_Test=rewrite_question_content(grade_direct_reference_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "rewrite_graph_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewrite_graph_state[\"questions\"][10][\"web_search_docs\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loopback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loopback_graph_state=loopback_node(rewrite_graph_state)\n",
    "# loopback_Test=loopback_node(rewrite_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loopback_graph_state[\"questions\"][10][\"web_search_docs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(loopback_graph_state[\"current_question_index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(loopback_graph_state[\"questions\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create propositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "should_create_proposition(loopback_graph_state)\n",
    "# should_create_proposition(loopback_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_propositions_graph_state = create_propositions(loopback_graph_state)\n",
    "# create_propositions_Test = create_propositions(loopback_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_propositions_graph_state[\"questions\"][10][\"web_search_docs\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_graph_state=retrieve(create_propositions_graph_state)\n",
    "# retrieve_Test=retrieve(create_propositions_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_graph_state[\"questions\"][10][\"web_search_docs\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grade_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_documents_graph_state = grade_documents(retrieve_graph_state)\n",
    "# grade_documents_Test = grade_documents(retrieve_Test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_documents_graph_state[\"questions\"][10][\"web_search_docs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_documents_graph_state[\"rewrite_count\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check_empty_propositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_empty_graph_state=check_empty_propositions(grade_documents_graph_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "check_empty_graph_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### query transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_transformation_graph_state=query_transformation(check_empty_graph_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_transformation_graph_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grade_document again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_documents_graph_state_again = grade_documents(query_transformation_graph_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_documents_graph_state2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check empty proposition again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_empty_graph_state_again=check_empty_propositions(grade_documents_graph_state_again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "check_empty_graph_state_again[\"questions\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### web_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_search_graph_node=web_search(check_empty_graph_state_again)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grade_web_search_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_web_search_docs_graph_node=grade_web_search_docs(web_search_graph_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_graph_state=generate(grade_web_search_docs_graph_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "count=0\n",
    "for question in generate_graph_state[\"questions\"]:\n",
    "    if question.get(\"generation\", \"\") != \"\":\n",
    "        count+=1\n",
    "        print(question['generation'])\n",
    "print(f\"amount of processed question: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grade answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_answer_graph_state = answer_grade_node(generate_graph_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( 1 in generate_graph_state[\"question_to_regen_index\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_answer(grade_answer_graph_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_graph_state_again=generate(grade_answer_graph_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grade answer again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_answer_graph_state_again=generate(generate_graph_state_again)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loopback2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loopback2_graph_state = loopback_node(generate_graph_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### should_create_proposition 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "should_create_proposition(loopback2_graph_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create_propositions2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_propositions_graph_state2= create_propositions(loopback2_graph_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### retrieve2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_graph_state2=retrieve(create_propositions_graph_state2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_graph_state2[\"questions\"][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grade documents 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "grade_documents_graph_state2 = grade_documents(retrieve_graph_state2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check_empty_propositions2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_empty_propositions_graph_state2= check_empty_propositions(grade_documents_graph_state2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### query_transformation2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_transformation_graph_state2 =query_transformation(check_empty_propositions_graph_state2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grade_document 2 again "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_documents_graph_state2_again = grade_documents(query_transformation_graph_state2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check_empty_propositions 2 again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_empty_propositions_graph_state2_again = check_empty_propositions(grade_documents_graph_state2_again)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_graph_state2 = generate(check_empty_propositions_graph_state2_again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "count=0\n",
    "for question in final_state[\"questions\"]:\n",
    "    if question.get(\"generation\", \"\") != \"\":\n",
    "        count+=1\n",
    "        print(question['generation'])\n",
    "print(f\"amount of processed question: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loopback3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loopback_node_graph_state3 =loopback_node(generate_graph_state2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test từng node websearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## separate chunks + metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "def convert_japanese_numbers(text):\n",
    "    jp_numbers = \"０１２３４５６７８９\"\n",
    "\n",
    "    latin_numbers = \"0123456789\"\n",
    "    translation_table = str.maketrans(jp_numbers, latin_numbers)\n",
    "    return text.translate(translation_table)\n",
    "\n",
    "def get_list_number(paragraph):\n",
    "    \"\"\"Trích xuất số thứ tự của danh sách nếu có\"\"\"\n",
    "    ns = {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}\n",
    "    xml = paragraph._element\n",
    "    numPr = xml.find('.//w:numPr', ns)\n",
    "    \n",
    "    if numPr is not None:\n",
    "        numId = numPr.find('.//w:numId', ns)\n",
    "        ilvl = numPr.find('.//w:ilvl', ns)\n",
    "        \n",
    "        if numId is not None and ilvl is not None:\n",
    "            return f\"{int(ilvl.get('{http://schemas.openxmlformats.org/wordprocessingml/2006/main}val')) + 1}．\"\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "def extract_text_with_numbering(docx_path):\n",
    "    \"\"\"Trích xuất nội dung từ file .docx kèm theo số thứ tự danh sách\"\"\"\n",
    "    doc = Document(docx_path)\n",
    "    extracted_text = []\n",
    "\n",
    "    for index,para in enumerate(doc.paragraphs):\n",
    "        list_number = get_list_number(para)\n",
    "        text = para.text.strip()\n",
    "        # print(f\"index {index} \")\n",
    "        # print(f\"para {para.text} \")\n",
    "        if text:\n",
    "            if list_number:\n",
    "                extracted_text.append(f\"{list_number} {text}\")\n",
    "            else:\n",
    "                extracted_text.append(text)\n",
    "\n",
    "    return '@'.join(extracted_text)\n",
    "\n",
    "\n",
    "def circled_to_int(circled):\n",
    "    # Bản đồ chuyển các ký hiệu số tròn sang số nguyên.\n",
    "    mapping = {\n",
    "    # Số 1 đến 20: ① (U+2460) -> ⑳ (U+2473)\n",
    "    '①': \"1\",  '②': \"2\",  '③': \"3\",  '④': \"4\",  '⑤': \"5\",\n",
    "    '⑥': \"6\",  '⑦': \"7\",  '⑧': \"8\",  '⑨': \"9\",  '⑩': \"10\",\n",
    "    '⑪': \"11\", '⑫': \"12\", '⑬': \"13\", '⑭': \"14\", '⑮': \"15\",\n",
    "    '⑯': \"16\", '⑰': \"17\", '⑱': \"18\", '⑲': \"19\", '⑳': \"20\",\n",
    "    \n",
    "    # Số 21 đến 35: ㉑ (U+3251) -> ㉟ (U+325F)\n",
    "    '㉑': \"21\", '㉒': \"22\", '㉓': \"23\", '㉔': \"24\", '㉕': \"25\",\n",
    "    '㉖': \"26\", '㉗': \"27\", '㉘': \"28\", '㉙': \"29\", '㉚': \"30\",\n",
    "    '㉛': \"31\", '㉜': \"32\", '㉝': \"33\", '㉞': \"34\", '㉟': \"35\",\n",
    "    \n",
    "    # Số 36 đến 50: ㊱ (U+32B1) -> ㊿ (U+32BF)\n",
    "    '㊱': \"36\", '㊲': \"37\", '㊳': \"38\", '㊴': \"39\", '㊵': \"40\",\n",
    "    '㊶': \"41\", '㊷': \"42\", '㊸': \"43\", '㊹': \"44\", '㊺': \"45\",\n",
    "    '㊻': \"46\", '㊼': \"47\", '㊽': \"48\", '㊾': \"49\", '㊿': \"50\",\n",
    "}\n",
    "\n",
    "    return mapping.get(circled, None)\n",
    "\n",
    "def split_and_convert(text):\n",
    "    pattern = r'@((?:[①-⑳]|[㉑-㉟]|[㊱-㊿]|\\(\\d+\\)|（\\d+）))(.*?)(?=@(?:[①-⑳]|[㉑-㉟]|[㊱-㊿]|\\(\\d+\\)|（\\d+）)|$)'\n",
    "    matches = re.finditer(pattern, text, re.DOTALL)\n",
    "    # print(f\"text {text}\")\n",
    "    # print(f\"\"\"pre text '{text.split(\"①\")[0]}'\"\"\")\n",
    "    result = []\n",
    "    for m in matches:\n",
    "        marker = m.group(1).strip()  # Ký hiệu số (①, (2), （3）)\n",
    "        content = m.group(2).strip()  # Nội dung tương ứng\n",
    "        \n",
    "        # Xử lý marker thành số nguyên\n",
    "        if re.match(r'[（）()]\\d+[（）()]', marker):  # Nếu là dạng (1) hoặc （1）\n",
    "            number = marker.strip(\"（）()\")  # Loại bỏ dấu ngoặc\n",
    "        else:  # Nếu là dạng số tròn (①, ②, ...)\n",
    "            number = circled_to_int(marker)\n",
    "\n",
    "        result.append({\"sub_clause_number\": number, \"sub_clause_content\": content.replace(\"@\",\"\")})\n",
    "    \n",
    "    return result\n",
    "\n",
    "def extract_structure(docx_path):\n",
    "    # doc = Document(docx_path)\n",
    "    # full_text = ' '.join([para.text.strip() for para in doc.paragraphs if para.text.strip()])\n",
    "    full_text =extract_text_with_numbering(docx_path)\n",
    "    full_text = convert_japanese_numbers(full_text)  # Nếu có hàm này\n",
    "    full_text=full_text.replace(\")\",\"）\")\n",
    "    full_text=full_text.replace(\"(\",\"（\")\n",
    "    # print(f\"full text {full_text}\")\n",
    "    data = {\"chapters\": []}\n",
    "\n",
    "    # Tìm tất cả chương với nội dung tương ứng\n",
    "    chapters = re.finditer(r'第([0-9]+)章　(.*?)(?=第[0-9]+章　|\\Z)', full_text, re.DOTALL)\n",
    "    chapter_dict = {}\n",
    "    article_dict = {}\n",
    "    clause_dict = {}\n",
    "    for chapter in chapters:\n",
    "        chapter_num = chapter.group(1)\n",
    "        chapter_title = chapter.group(2).strip().split('@')[0]\n",
    "        chapter_content = chapter.group(0)\n",
    "        # print(f\"chapter_content {chapter_content}\")\n",
    "        # Tìm các điều trong chương\n",
    "        articles = re.finditer(r'@第([0-9]+)条\\s*（(.*?)）(.*?)(?=@第[0-9]+条\\s*（|\\Z)', chapter_content, re.DOTALL)\n",
    "        articles_data = []\n",
    "        for article in articles:\n",
    "            article_num = article.group(1)\n",
    "            # print(f\"article_num {article_num}\")\n",
    "            article_title = article.group(2).strip()\n",
    "            article_text = article.group(3).strip()\n",
    "            # print(f\"article_text {article_text}\")\n",
    "            article_contents = re.finditer(r'@([1-9]+)．(.*?)(?=@[1-9]．|\\Z)', article_text, re.DOTALL)\n",
    "            articles_content_data=[]\n",
    "            \n",
    "            for content in article_contents:\n",
    "                article_content_num = content.group(1)\n",
    "                article_content_text = content.group(2).strip()\n",
    "                # print(article_content_text)\n",
    "            # Tìm nội dung clause (bắt đầu bằng ①, ②, 1., - ...)\n",
    "                clause_match = re.split(r'(@①|@1\\.|@-|@（[0-9]+）)', article_content_text, maxsplit=1)\n",
    "                # clause_match = re.split(r'(①|②|③|1\\.|-|\\([0-9]+\\))', article_content_text, maxsplit=1, flags=re.MULTILINE)\n",
    "                clause_text_temp=article_content_text\n",
    "                parts=[]\n",
    "                if len(clause_match) > 1:\n",
    "                    article_content_text = clause_match[0].strip()  # Lấy phần trước danh sách\n",
    "                    clause_content = ''.join(clause_match[1:]).strip()  # Lấy phần còn lại\n",
    "                    \n",
    "                    parts=split_and_convert(clause_content)\n",
    "\n",
    "                else:\n",
    "                    clause_content = \"\"\n",
    "                # print(f\"parts {parts}\")\n",
    "                articles_content_data.append({\n",
    "                    \"clause_number\": article_content_num,\n",
    "                    \"clause_title\": article_content_text,\n",
    "                    # \"clause_content\": clause_content,\n",
    "                    \"sub_clauses\":parts\n",
    "                })\n",
    "                clause_dict[(article_num,article_content_num)] ={\n",
    "                    \"chapter\":f\"\"\"第{chapter_num.replace(\"@\",\"\")}章　{chapter_title.replace(\"@\",\"\")} \\n \"\"\",\n",
    "                    \"article\":f\"\"\"第{article_num.replace(\"@\",\"\")}条（{article_title.replace(\"@\",\"\")}）\\n \"\"\",\n",
    "                    \"clause\":f\"\"\"{article_content_num.replace(\"@\",\"\")}．\"\"\",\n",
    "                    \"sub_clause\":\"-1\",\n",
    "                    \"text\":f\"\"\"{clause_text_temp.replace(\"@\",\"\")}\"\"\"\n",
    "                } \n",
    "            articles_data.append({\n",
    "                    \"article_number\": article_num,\n",
    "                    \"article_title\": article_title,\n",
    "                    # \"article_text\":article_text,\n",
    "                    \"clauses\": articles_content_data,\n",
    "                    # \"clause_content\": clause_content\n",
    "                })\n",
    "            article_dict[article_num] = {\n",
    "                \"chapter\":f\"\"\"第{chapter_num.replace(\"@\",\"\")}章　{chapter_title.replace(\"@\",\"\")} \\n \"\"\",\n",
    "                \"article\":f\"\"\"第{article_num.replace(\"@\",\"\")}条（{article_title.replace(\"@\",\"\")}）\\n \"\"\",\n",
    "                \"clause\":\"-1\",\n",
    "                \"sub_clause\":\"-1\",\n",
    "                \"text\":f\"\"\"{article_text.replace(\"@\",\"\")}\"\"\"\n",
    "            }\n",
    "            \n",
    "            \n",
    "        data[\"chapters\"].append({\n",
    "            \"chapter_number\": chapter_num,\n",
    "            \"chapter_title\": chapter_title,\n",
    "            # \"chapter_text\": chapter_content,\n",
    "            \"articles\": articles_data\n",
    "        })\n",
    "    #     chapter_dict[chapter_num] =  f\"\"\"第{chapter_num}章　{chapter_title} \\n{chapter_content}\"\"\"\n",
    "        chapter_dict[chapter_num] =  {\n",
    "                \"chapter\":f\"\"\"第{chapter_num.replace(\"@\",\"\")}章　{chapter_title.replace(\"@\",\"\")} \\n \"\"\",\n",
    "                \"article\":\"-1\",\n",
    "                \"clause\":\"-1\",\n",
    "                \"sub_clause\":\"-1\",\n",
    "                \"text\":f\"\"\"{chapter.group(2).strip().split('@',1)[1].replace(\"@\",\"\")}\"\"\"\n",
    "      \n",
    "        } \n",
    "    root_text={\"Chapter\":chapter_dict, \"Article\":article_dict, \"clause\":clause_dict}\n",
    "    \n",
    "    return data,root_text\n",
    "\n",
    "def save_json(data, output_path):\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Sử dụng\n",
    "docx_path = '/home/thanhnguyen/Data/code/RAG_techniques/①就業規則_ひな形_20240513.docx'\n",
    "json_path = '①就業規則_ひな形_20240513.json'\n",
    "\n",
    "data,root_text = extract_structure(docx_path)\n",
    "save_json(data, json_path)\n",
    "\n",
    "normalized_data= my_utils.normalize_dict(data)\n",
    "useful_data = {'chapters':normalized_data['chapters'][0:2]}\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "docs_list = [\n",
    "    Document(\n",
    "        page_content=sub_clause.get(\"sub_clause_content\") if sub_clause else clause[\"clause_title\"],\n",
    "        metadata={\n",
    "            \"chapter_title\": chapter['chapter_title'].replace(\"@\",\"\"),\n",
    "            \"chapter_number\": chapter['chapter_number'].replace(\"@\",\"\"),\n",
    "            # \"chapter_text\": chapter['chapter_text'].replace(\"@\",\"\"),\n",
    "            \"article_title\": article['article_title'].replace(\"@\",\"\"), \n",
    "            \"article_number\": article['article_number'].replace(\"@\",\"\"),\n",
    "            # \"article_text\": article['article_text'].replace(\"@\",\"\"),\n",
    "            \"clause_number\": clause['clause_number'].replace(\"@\",\"\"),\n",
    "            \"clause_title\": clause[\"clause_title\"].replace(\"@\",\"\"),\n",
    "            # \"clause_text\": clause[\"clause_content\"].replace(\"@\",\"\"),\n",
    "            \"sub_clause_number\": sub_clause.get(\"sub_clause_number\").replace(\"@\",\"\") if sub_clause else \"\",\n",
    "            \"sub_clause_content\": sub_clause.get(\"sub_clause_content\").replace(\"@\",\"\") if sub_clause else \"\",\n",
    "\n",
    "            # \"clause_content\":  clause['clause_title']+\"\\n\" if clause['clause_content'] else \"\",\n",
    "            \n",
    "        }\n",
    "    )\n",
    "    for chapter in useful_data['chapters'] \n",
    "    for article in chapter['articles']\n",
    "    for clause in article['clauses']\n",
    "    for sub_clause in (clause[\"sub_clauses\"] or [None])  # Nếu rỗng thì tạo danh sách chứa None\n",
    "]\n",
    "\n",
    "docs_list\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=500, chunk_overlap=30\n",
    ")\n",
    "\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "for i, doc in enumerate(doc_splits):\n",
    "    doc.metadata['chunk_id'] = i+1 ### adding chunk id\n",
    "\n",
    "graph_state = {\"questions\":\n",
    "            [\n",
    "                {\"question_content\":Document(\n",
    "                    page_content=doc_split.metadata['clause_title'] +\"\\n\"+ doc_split.page_content if doc_split.metadata['sub_clause_content'] else doc_split.page_content,\n",
    "                    metadata={\n",
    "                        \"chapter_title\": doc_split.metadata['chapter_title'],\n",
    "                        \"chapter_number\": doc_split.metadata['chapter_number'],\n",
    "                        # \"chapter_text\": doc_split.metadata['chapter_text'],\n",
    "                        \"article_title\": doc_split.metadata['article_title'], \n",
    "                        \"article_number\": doc_split.metadata['article_number'],\n",
    "                        # \"article_text\": doc_split.metadata['article_text'],\n",
    "                        \"clause_number\": doc_split.metadata['clause_number'],\n",
    "                        \"clause_title\": doc_split.metadata['clause_title'],\n",
    "                        # \"clause_text\": doc_split.metadata['clause_text'],\n",
    "                        \"sub_clause_number\": doc_split.metadata['sub_clause_number'],\n",
    "                        \"sub_clause_content\": doc_split.metadata['sub_clause_content'],\n",
    "                    }),\n",
    "                \"propositions\":[],\n",
    "                \"ref_propositions\":[],\n",
    "                \"regen_count\":0,\n",
    "                \"web_search_docs\":[],\n",
    "                \"filtered_web_search_docs\":[],\n",
    "                 \n",
    "                }\n",
    " for doc_split in doc_splits],\n",
    "                \"root_text\":root_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## direct_reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_graph_state=direct_reference(graph_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "reference_graph_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## grade reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_direct_reference_graph_state=grade_direct_reference(reference_graph_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "grade_direct_reference_graph_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rewrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewrite_graph_state=rewrite_question_content(grade_direct_reference_graph_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "rewrite_graph_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loopback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loopback_graph_state=loopback_node(rewrite_graph_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "loopback_graph_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## web_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "should_create_search_web(loopback_graph_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "web_search_graph_state =web_search(loopback_graph_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = b\"ã\\x81\\x93ã\\x81®ã\\x83\\x9aã\\x83¼ã\\x82¸ã\\x81§ã\\x81¯JavaScriptã\\x82\\x92ä½¿ç\\x94¨ã\\x81\\x97ã\\x81¦ã\\x81\\x84ã\\x81¾ã\\x81\\x99ã\\x80\\x82JavaScriptã\\x82\\x92æ\\x9c\\x89å\\x8a¹ã\\x81«ã\\x81\\x97ã\"  # Dữ liệu bị lỗi\n",
    "# decoded_text = raw_text.decode(\"utf-8\", errors=\"ignore\")  # Cố gắng giải mã đúng encoding\n",
    "# print(decoded_text)\n",
    "raw_text.encode('latin1').decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_search_graph_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in web_search_graph_state[\"questions\"]:\n",
    "    # if question[\"web_search_docs\"]\n",
    "    print(len(question[\"web_search_docs\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## grade_websearch_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "grade_web_search_docs_graph_state= grade_web_search_docs(web_search_graph_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "count=0\n",
    "for q_idx,question in enumerate(grade_web_search_docs_graph_state[\"questions\"]):\n",
    "    if q_idx in grade_web_search_docs_graph_state[\"current_question_index\"]:\n",
    "        if len(question[\"filtered_web_search_docs\"])==0:\n",
    "            count+=1\n",
    "        else: \n",
    "            if len(question[\"filtered_web_search_docs\"])==6:\n",
    "                print(question)\n",
    "            print(len(question[\"filtered_web_search_docs\"]))\n",
    "            # print(question)\n",
    "print(f\"none {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "grade_web_search_docs_graph_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websearch_generate_graph_state= websearch_generate(grade_web_search_docs_graph_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "websearch_generate_graph_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,question in enumerate(websearch_generate_graph_state[\"questions\"]):\n",
    "    if index in websearch_generate_graph_state[\"current_question_index\"]:\n",
    "        print(f\"\"\"{index} {question[\"generation\"]}\"\"\")\n",
    "        print(f\"\"\"question[\"question_content\"].page_content {question[\"question_content\"].page_content}\"\"\")\n",
    "        print(f\"\"\"question[\"filtered_web_search_docs\"] {question[\"filtered_web_search_docs\"]}\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "vllm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
