{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ModuleRAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "from my_utils import extract_json_from_string\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "\n",
    "class ModuleRAG: \n",
    "    def __init__(self, vllm_model):\n",
    "        self.vllm_model = vllm_model\n",
    "        self.runnable = RunnableLambda(extract_json_from_string)\n",
    "        self.extract_reference_generator = self.create_extract_reference_generator()\n",
    "        self.sematic_reference_generator = self.create_sematic_reference_generator()\n",
    "        self.rewrite_clause_generator = self.create_rewrite_clause_generator()\n",
    "        self.proposition_generator = self.create_proposition_generator()\n",
    "        self.retrieval_grader = self.create_retrieval_grader()\n",
    "        self.rag_chain_both = self.create_rag_chain_both()\n",
    "        self.rag_chain_doc = self.create_rag_chain_doc()\n",
    "        self.query_transformation_generator = self.create_query_transformation_generator()\n",
    "        self.hallucination_grader = self.create_hallucination_grader()\n",
    "        self.answer_grader = self.create_answer_grader()\n",
    "        self.web_search_tool = self.create_web_search_tool()\n",
    "        self.grade_web_search_docs = self.create_grade_web_search_docs()\n",
    "\n",
    "    def create_extract_reference_generator(self):\n",
    "        extract_reference_template=\"\"\"<|im_start|>system\n",
    "You will receive:\n",
    "1. A passage from a contract that may include references to other sections, articles, clauses, or parts of the contract (e.g., “as specified in Section 2”, “according to the clause above”, “refer to Article 3 below”).\n",
    "2. Metadata providing context for the current passage, including details such as the current Chapter, Article, and Clause.\n",
    "\n",
    "### Task:\n",
    "#### Step 1: Extract References\n",
    "- Identify and extract all phrases in the contract text that refer to other sections, articles, or clauses.\n",
    "\n",
    "#### Step 2: Resolve References\n",
    "- Use the provided metadata to determine the specific Chapter, Article, or Clause that each reference points to.\n",
    "- Examples:\n",
    "  -第5項 means clause 5\n",
    "  - If the metadata indicates that the current passage is in Article 1:\n",
    "    - \"the above article\" refers to Article 0 (or the last article of the previous section, if applicable).\n",
    "    - \"clause 3 below\" refers to Clause 3 in the subsequent article or section.\n",
    "- If a reference cannot be precisely resolved, set its \"resolved\" value to -1.\n",
    "-\"has_reference\": \"yes\" if there is at least one reference, otherwise \"no\".\n",
    "-\"is_extractable\": \"yes\" if the reference can be extracted and resolved, otherwise \"no\".\n",
    "-\"references\": A list of extracted references, each containing:\n",
    "-\"text\": The extracted reference phrase.\n",
    "-\"resolved\": The resolved location, with -1 if it cannot be determined.\n",
    "\n",
    "The response should be in following JSON format. The answer contain json only:\n",
    "{{\n",
    "  \"has_reference\" : \"yes\" | \"no\",\n",
    "  \"is_extractable\" : \"yes\" | \"no\"\n",
    "  \"references\": [\n",
    "    {{\n",
    "      \"text\": str,\n",
    "      \"resolved\": {{\n",
    "        \"Chapter\": int| -1,\n",
    "        \"Article\": int| -1,\n",
    "        \"clause\": int| -1,\n",
    "        \"sub_clause\": int|-1,\n",
    "      }}\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "<|im_end|>\n",
    "\n",
    "<|im_start|>user\n",
    "contract:{document}\n",
    "metadata:{metadata}\n",
    "<|im_end|>\n",
    "\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "        extract_reference_prompt = PromptTemplate.from_template(extract_reference_template)\n",
    "        extract_reference_generator = extract_reference_prompt | self.vllm_model| self.runnable\n",
    "        return extract_reference_generator\n",
    "\n",
    "    def get_extract_reference_generator(self):\n",
    "        return self.extract_reference_generator\n",
    "    \n",
    "    def create_sematic_reference_generator(self):\n",
    "        sematic_reference_template = \"\"\"<|im_start|>system\n",
    "You are a language model specializing in analyzing relationships between texts. Your task is to determine whether a referenced text (Referenced Text) provides additional meaning that helps in understanding the main text (Main Text).\n",
    "\n",
    "Task:\n",
    "Check whether the referenced text provides necessary additional information to better understand the main text.\n",
    "\n",
    "If the referenced text contains important supplementary information that clarifies, explains, or provides essential context for understanding the main text, respond with \"yes\".\n",
    "If the referenced text does not add significant meaning or merely repeats existing information, respond with \"no\".\n",
    "\n",
    "The response must be returned in following JSON format, return json only:\n",
    "{{\n",
    "\"analysis\": \"A brief explanation of the relationship between the texts, must not include `\"`.\",\n",
    "\"provide_additional_meaning\": \"yes\"|\"no\",\n",
    "\n",
    "}}\n",
    "<|im_end|>\n",
    "\n",
    "<|im_start|>user\n",
    "Main Text: {main_text}\n",
    "Referenced Text: {referenced_text}\n",
    "<|im_end|>\n",
    "\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "        sematic_reference_prompt = PromptTemplate.from_template(sematic_reference_template)\n",
    "        sematic_reference_generator = sematic_reference_prompt | self.vllm_model| self.runnable\n",
    "        return sematic_reference_generator\n",
    "\n",
    "    def get_sematic_reference_generator(self):\n",
    "        return self.sematic_reference_generator\n",
    "\n",
    "    def create_rewrite_clause_generator(self):\n",
    "        rewrite_clause_template = \"\"\"<|im_start|>system\n",
    "You are a legal text processing expert. Your task is to merge two contract sections into a single coherent text while ensuring clarity, accuracy, and legal consistency.\n",
    "\n",
    "main_section: This is the primary contract section, which may reference specific clauses from another section.\n",
    "refer_section: This section contains detailed information referenced in main_section.\n",
    "Requirements:\n",
    "1.If the main_section references a specific clause in the refer_section, replace the reference with the actual content from the refer_section.\n",
    "2.Completely remove any reference phrases such as \"as defined in clause X\", \"according to article Y\", or any similar wording. The final text should only contain the actual content without referring back to any section or clause.\n",
    "3.Preserve the original meaning of the contract without altering its legal intent.\n",
    "4.Ensure the final merged text maintains a clear, structured, and legally sound format.\n",
    "5.The final \"generated_clause\" must be self-contained and must not contain any reference phrases even if reference phrases are present in the refer_sections.\n",
    "6.All newlines are represented using \"\\\\n\" instead of actual line breaks.\n",
    "The response must be in following JSON format.:\n",
    "{{\n",
    "    \"analysis\": \"Logical reasoning behind how the generated clause was created, including how references were replaced.\",\n",
    "    \"generated_section\": \"The fully rewritten section with all references replaced by full content, properly formatted and structured, without any reference phrases.Ensure that in the JSON response\"\n",
    "}}\n",
    "<|im_end|>\n",
    "\n",
    "<|im_start|>user\n",
    "main section:\n",
    "{main_section}\n",
    "refer sections:\n",
    "{refer_sections}\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "        rewrite_clause_prompt = PromptTemplate.from_template(rewrite_clause_template)\n",
    "        rewrite_clause_generator = rewrite_clause_prompt | self.vllm_model | self.runnable\n",
    "        return rewrite_clause_generator\n",
    "\n",
    "    def get_rewrite_clause_generator(self):\n",
    "        return self.rewrite_clause_generator\n",
    "\n",
    "    def create_proposition_generator(self): \n",
    "        proposition_template=\"\"\"<|im_start|>system\n",
    "The following input is a legal document or contract. Your task is to break down the document into simple, self-contained propositions that are easy to understand. Follow these steps:\n",
    "\n",
    "1. Identify key sentences: \n",
    "Read the entire document and extract factual information, rules, or conditions. Ignore introductory or transitional phrases that do not contain substantial content.\n",
    "\n",
    "2. Ensure completeness of definitions:  \n",
    "- If a proposition defines a term (for example: \"Xとは…\"), ensure it contains a complete definition.  \n",
    "- Do not generate incomplete propositions such as:  \n",
    "    \"A worker is defined as follows.\" → This does not provide a definition.  \n",
    "    Instead, include the full definition within the same proposition:  \n",
    "    \"A worker is a person who joins the company according to the procedure set forth in Chapter 2 and meets the following criteria: [list of criteria].\"  \n",
    "\n",
    "3. Make each proposition self-contained:  \n",
    "- Ensure the proposition is understandable without additional context.  \n",
    "- Replace pronouns or vague references with explicit terms.  \n",
    "\n",
    "4. Maintain accuracy and include all necessary details:  \n",
    "- Retain legal references, dates, and conditions to avoid ambiguity.  \n",
    "- If a rule contains multiple conditions, split them into separate propositions while preserving logical relationships.  \n",
    "\n",
    "5. Avoid overlapping or redundant propositions:  \n",
    "- Do not generate broad statements that are just incomplete versions of other propositions.  \n",
    "\n",
    "6. Use appropriate legal or technical terms: Write all propositions in janpanese and use proper legal or technical terminology.\n",
    "The response should be in Japanese and returned in following JSON format:\n",
    "{{\n",
    "\"propositions\": \n",
    "    [\n",
    "    {{\"proposition\":\"Generated proposition in japanese\"}}\n",
    "    ]\n",
    "}}\n",
    "<|im_end|>\n",
    "\n",
    "<|im_start|>user\n",
    "contract:{document}\n",
    "<|im_end|>\n",
    "\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "        proposition_prompt = PromptTemplate.from_template(proposition_template)\n",
    "        proposition_generator = proposition_prompt | self.vllm_model| self.runnable\n",
    "        return proposition_generator\n",
    "\n",
    "    def get_proposition_generator(self):\n",
    "        return self.proposition_generator\n",
    "\n",
    "    def create_retrieval_grader(self):\n",
    "        retrieval_grader_template=\"\"\"<|im_start|>system\n",
    "You are an evaluator assessing the relevance of a retrieved law to a given contract segment.\n",
    "Evaluation Criteria:\n",
    "Answer \"yes\" only if the retrieved law can be used as direct evidence to determine whether the contract segment complies with the law or violates the law.\n",
    "In all other cases, including where the law is not directly applicable or does not provide clear evidence of compliance or non-compliance, answer \"no\".\n",
    "The law must have a direct and actionable link to the contract segment's compliance or violation of legal standards.\n",
    "\n",
    "Your response should focus on whether the law can serve as direct evidence of the contract’s legal compliance or non-compliance.\n",
    "\n",
    "Provide a binary score to indicate relevance:\n",
    "\"yes\": The law provides direct evidence of the contract segment’s compliance or violation of the law.\n",
    "\"no\": The law does not provide direct evidence regarding compliance or violation.\n",
    "\n",
    "The response should be returned in following JSON format:\n",
    "{{\n",
    "\"binary_score\": \"yes\" |\"no\"\n",
    "}}\n",
    "<|im_end|>\n",
    "\n",
    "<|im_start|>user\n",
    "Law: [{context}].\n",
    "Contract Segment: [{contract}]\n",
    "\n",
    "<|im_end|>\n",
    "\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "        retrieval_grader_prompt = PromptTemplate.from_template(retrieval_grader_template)\n",
    "\n",
    "        # Chain\n",
    "        retrieval_grader = retrieval_grader_prompt | self.vllm_model | self.runnable\n",
    "        return retrieval_grader\n",
    "\n",
    "    def get_retrieval_grader(self):\n",
    "        return self.retrieval_grader\n",
    "\n",
    "    def create_rag_chain_both(self):\n",
    "        template_both = \"\"\"<|im_start|>system\n",
    "You are a legal AI specializing in contract evaluation. Your task is to analyze a given contract clause and determine whether it *complies with, violates, or lacks sufficient information* to conclude compliance based on the provided legal provisions.\n",
    "\n",
    "### *Inputs:*  \n",
    "- *Laws:* Relevant legal provisions related to the contract.  \n",
    "- *Contract Clause:* The specific contract segment to be evaluated.  \n",
    "- *Rewritten Contract Clause:* An alternative, clearer version of the contract clause for better understanding.\n",
    "\n",
    "### *Evaluation Criteria:*  \n",
    "1. *Compliance:* The contract fully adheres to the relevant legal provisions.  \n",
    "2. *Violation:* The contract contradicts or does not meet the legal requirements.  \n",
    "3. *Insufficient Information:* The provided contract clause lacks enough details to determine compliance or violation.  \n",
    "\n",
    "### *Response Format (in Japanese, JSON format):*  \n",
    "json\n",
    "{{\n",
    "\"evaluation\": \"compliance\" | \"violation\" | \"insufficient_information\",\n",
    "\"explanation\": \"A concise explanation of why the contract complies, violates, or lacks sufficient information.\"\n",
    "}}\n",
    "\n",
    "<|im_end|>\n",
    "\n",
    "<|im_start|>user\n",
    "Laws: [{context}].  \n",
    "Contract Clause: [{contract}].  \n",
    "Rewritten Contract Clause: [{new_query}].  \n",
    "<|im_end|>\n",
    "\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "        # Tạo PromptTemplate\n",
    "        prompt_both = PromptTemplate.from_template(template_both)\n",
    "\n",
    "        # Chuỗi chạy RAG\n",
    "        rag_chain_both = prompt_both | self.vllm_model | self.runnable\n",
    "        return rag_chain_both\n",
    "\n",
    "    def get_rag_chain_both(self):\n",
    "        return self.rag_chain_both\n",
    "\n",
    "    def create_rag_chain_doc(self):\n",
    "        template_doc = \"\"\"<|im_start|>system\n",
    "You are a legal AI specializing in contract evaluation. Your task is to analyze a given contract clause and determine whether it *complies with, violates, or lacks sufficient information* to conclude compliance based on the provided legal provisions.  \n",
    "\n",
    "### *Inputs:*  \n",
    "- *Laws:* Relevant legal provisions related to the contract.  \n",
    "- *Contract Clause:* The specific contract segment to be evaluated.  \n",
    "\n",
    "### *Evaluation Criteria:*  \n",
    "1. *Compliance:* The contract fully adheres to the relevant legal provisions.  \n",
    "2. *Violation:* The contract contradicts or does not meet the legal requirements.  \n",
    "3. *Insufficient Information:* The provided contract clause lacks enough details to determine compliance or violation.  \n",
    "\n",
    "### *Response Format (in Japanese, JSON format):*  \n",
    "```json\n",
    "{{\n",
    "\"evaluation\": \"compliance\" | \"violation\" | \"insufficient_information\",\n",
    "\"explanation\": \"A concise explanation of why the contract complies, violates, or lacks sufficient information.\"\n",
    "}}\n",
    "<|im_end|>\n",
    "\n",
    "<|im_start|>user\n",
    "Laws: [{context}].\n",
    "Contract Segment: [{contract}]\n",
    "\n",
    "<|im_end|>\n",
    "\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "        # Create prompt\n",
    "        prompt_doc = PromptTemplate.from_template(template_doc)\n",
    "\n",
    "        rag_chain_doc = prompt_doc | self.vllm_model | self.runnable\n",
    "        return rag_chain_doc\n",
    "\n",
    "    def get_rag_chain_doc(self):\n",
    "        return self.rag_chain_doc\n",
    "\n",
    "    def create_query_transformation_generator(self):\n",
    "        query_transformation_template=\"\"\"<|im_start|>system\n",
    "You are a natural language processing expert. Your task is to transform the input query into different variations based on the following criteria:  \n",
    "### Instructions:  \n",
    "1. **Paraphrasing (1 variations):** Rewrite the query in a different way while keeping the meaning unchanged.  \n",
    "2. **Granularity Adjustment - More Detailed (1 variations):** Rewrite the query with a higher level of detail by adding specific elements without changing its meaning.  \n",
    "3. **Granularity Adjustment - Less Detailed (1 variations):** Rewrite the query in a more generalized way by removing details while maintaining the overall meaning.  \n",
    "4. **Query Expansion (1 variations):** Expand the query by adding relevant information to make it more comprehensive.  \n",
    "\n",
    "The response must be returned in the following JSON format:\n",
    "{{\n",
    "\"paraphrased_queries\": [\n",
    "    \"Paraphrased query\"\n",
    "],\n",
    "\"detailed_queries\": [\n",
    "    \"More detailed query\"\n",
    "],\n",
    "\"generalized_queries\": [\n",
    "    \"More generalized query\"\n",
    "],\n",
    "\"expanded_queries\": [\n",
    "    \"Expanded query\"\n",
    "]\n",
    "}}\n",
    "<|im_end|>\n",
    "\n",
    "<|im_start|>user\n",
    "Input Query:{query}  \n",
    "<|im_end|> \n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "        query_transformation_prompt = PromptTemplate.from_template(query_transformation_template)\n",
    "\n",
    "        # Chain\n",
    "        query_transformation_generator = query_transformation_prompt | self.vllm_model| self.runnable\n",
    "        return query_transformation_generator\n",
    "\n",
    "    def get_query_transformation_generator(self):\n",
    "        return self.query_transformation_generator\n",
    "    \n",
    "    def create_hallucination_grader(self):\n",
    "        hallucination_grader_template=\"\"\"<|im_start|>system\n",
    "The input consists of laws, a contract, and a generated explanation.  \n",
    "Your task is to determine whether the generated explanation strictly relies on the provided laws and contract without introducing new context.  \n",
    "\n",
    "### Evaluation criteria:  \n",
    "- If the explanation only uses information directly from the laws and contract, return \"yes\".  \n",
    "- The explanation may include a conclusion (e.g., stating whether the contract is correct, incorrect, or lacks enough information), and this is **acceptable** as long as the reasoning **before** the conclusion strictly follows the laws and contract.  \n",
    "- Return \"no\" **only if** the explanation introduces new context, makes assumptions, or includes information not explicitly stated in the laws and contract.  \n",
    "\n",
    "### Examples:\n",
    "\n",
    "#### Example 1 \n",
    "**Laws:** \"Employers must give at least 30 days’ notice before termination.\"  \n",
    "**Contract:** \"Employees must be given at least 40 days notice before termination.\"  \n",
    "**Generated Explanation:**  \n",
    "\"The contract states that employees must be given at least 40 days' notice before termination, which is stricter than the law requiring 30 days. This means the contract is legally valid but more protective than the minimum legal requirement.\"  \n",
    "\n",
    "*Analysis: The explanation strictly follows the laws and contract without adding new information. The conclusion is based entirely on the provided text.*  \n",
    "*is_supported: yes\n",
    "Your response must be in the following JSON format:  \n",
    "{{\n",
    "  \"analysis\": \"A brief explanation of whether the generated explanation strictly follows the laws and contract.\",\n",
    "  \"is_supported\": \"yes\" or \"no\"\n",
    "}}\n",
    "<|im_end|>\n",
    "\n",
    "<|im_start|>user\n",
    "laws: [{laws}]  \n",
    "contract: [{contract}]  \n",
    "generated explanation: [{explanation}]  \n",
    "<|im_end|>  \n",
    "<|im_start|>assistant\n",
    "\n",
    "\"\"\"\n",
    "        hallucination_grader_prompt = PromptTemplate.from_template(hallucination_grader_template)\n",
    "        # Chain\n",
    "        hallucination_grader = hallucination_grader_prompt | self.vllm_model| self.runnable\n",
    "        return hallucination_grader\n",
    "\n",
    "    def get_hallucination_grader(self): \n",
    "        return self.hallucination_grader\n",
    "    \n",
    "    def create_answer_grader(self): \n",
    "        answer_grader_template=\"\"\"<|im_start|>system\n",
    "The input consists of a contract and a generated explanation.\n",
    "Your task is to determine whether the generated explanation truly explains the contract.\n",
    "\n",
    "### Evaluation criteria:\n",
    "- If the explanation correctly interprets or clarifies the contract, return \"yes\".\n",
    "- If the explanation does not accurately explain the contract, return \"no\".\n",
    "\n",
    "The response must be returned in the following JSON format:\n",
    "{{\n",
    "\"is_valid_explanation\": \"yes\" or \"no\",\n",
    "}}\n",
    "<|im_end|>\n",
    "\n",
    "<|im_start|>user\n",
    "contract: [{contract}]  \n",
    "generated explanation: [{explanation}]  \n",
    "<|im_end|> \n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "        answer_grader_prompt = PromptTemplate.from_template(answer_grader_template)\n",
    "        # Chain\n",
    "        answer_grader = answer_grader_prompt | self.vllm_model | self.runnable\n",
    "        return answer_grader\n",
    "\n",
    "    def get_answer_grander(self):\n",
    "        return self.answer_grader\n",
    "    \n",
    "    def create_web_search_tool(self):\n",
    "        web_search_tool = TavilySearchResults(max_results=1,include_images=False,)\n",
    "        return web_search_tool\n",
    "    \n",
    "    def get_web_search_tool(self): \n",
    "        return self.web_search_tool\n",
    "    \n",
    "    def create_grade_web_search_docs(self): \n",
    "        websearch_grader_template=\"\"\"<|im_start|>system\n",
    "You are an evaluator assessing whether a retrieved reference can serve as **direct legal evidence** to determine if a given contract segment complies with legal standards.\n",
    "\n",
    "### **Evaluation Criteria:**\n",
    "- Answer **\"yes\"** **only if** the retrieved reference is a law, regulation, or legally binding document that provides **direct evidence** for assessing the contract segment’s compliance or violation.\n",
    "- Answer **\"no\"** in all other cases, including:\n",
    "  - The reference is **not an actual law** (e.g., a general explanation, opinion, or guideline).\n",
    "  - The reference **does not directly address** the criteria mentioned in the contract segment.\n",
    "  - The reference is **too vague or general** to be used as clear legal evidence.\n",
    "\n",
    "Your response should be **strictly based on whether the reference can serve as legal proof**.\n",
    "\n",
    "### **Reasoning Steps:**\n",
    "Follow these steps to evaluate the relevance of the reference context:\n",
    "\n",
    "**Step 1: Understand the Contract Segment**\n",
    "- Read the contract segment carefully to identify its key terms, obligations, rights, or conditions.\n",
    "- Highlight the main requirements or claims in the segment. For example:\n",
    "  - What is being promised or agreed upon?\n",
    "  - Are there specific conditions, deadlines, or obligations?\n",
    "  - What legal standards or regulations might apply?\n",
    "\n",
    "**Step 2: Understand the Reference Context**\n",
    "- Read the reference context carefully to understand the information it provides.\n",
    "- Identify any legal provisions, rules, or facts that relate to the contract segment. For example:\n",
    "  - Does the reference context mention laws, regulations, or standards that apply to the contract segment?\n",
    "  - Does it provide evidence or context that supports or challenges the segment's compliance?\n",
    "\n",
    "**Step 3: Compare and Analyze**\n",
    "- Compare the reference context with the contract segment. Look for:\n",
    "  - **Alignment**: Does the reference context confirm or support the contract segment's compliance?\n",
    "  - **Contradiction**: Does the reference context contradict or disprove the contract segment's compliance?\n",
    "  - **Gaps**: Is the reference context insufficient or irrelevant to the contract segment?\n",
    "- Evaluate whether the reference context provides **direct and actionable evidence** to determine compliance or violation.\n",
    "\n",
    "### Expected Output:\n",
    "Provide your response in the following JSON format:\n",
    "{{\n",
    "  \"analysis\": \"your step-by-step reasoning and analysis\",\n",
    "  \"binary_score\": \"yes\" | \"no\"\n",
    "}}\n",
    "\n",
    "### Examples:\n",
    "#### Example 1:\n",
    "- Reference Context: \"According to Article 12 of the Labor Law, employees must be provided with a minimum of 12 days of paid annual leave.\"\n",
    "- Contract Segment: \"The company agrees to provide employees with 10 days of paid annual leave.\"\n",
    "- Analysis: \"The reference context states that the Labor Law requires a minimum of 12 days of paid annual leave, while the contract segment provides only 10 days. This directly contradicts the legal requirement, proving non-compliance.\"\n",
    "- Binary Score: \"yes\"\n",
    "\n",
    "#### Example 2:\n",
    "- Reference Context: \"According to Article 15 of the Tax Code, businesses must file their tax returns by March 31st each year.\"\n",
    "- Contract Segment: \"The company agrees to deliver the product by January 15, 2024.\"\n",
    "- Analysis: \"The reference context discusses tax filing deadlines, which is unrelated to the contract segment about product delivery. There is no direct link between the two.\"\n",
    "- Binary Score: \"no\"\n",
    "\n",
    "#### Example 3:\n",
    "- Reference Context: \"According to Article 20 of the Consumer Protection Law, sellers must provide a 30-day return policy for defective products.\"\n",
    "- Contract Segment: \"The seller agrees to provide a 14-day return policy for defective products.\"\n",
    "- Analysis: \"The reference context states that the Consumer Protection Law requires a 30-day return policy, while the contract segment provides only 14 days. This directly contradicts the legal requirement, proving non-compliance.\"\n",
    "- Binary Score: \"yes\"\n",
    "\n",
    "#### Example 4:\n",
    "- Reference Context: \"According to Article 5 of the Environmental Protection Act, companies must reduce carbon emissions by 20% by 2030.\"\n",
    "- Contract Segment: \"The company agrees to reduce carbon emissions by 15% by 2030.\"\n",
    "- Analysis: \"The reference context states that the Environmental Protection Act requires a 20% reduction in carbon emissions, while the contract segment commits to only 15%. This directly contradicts the legal requirement, proving non-compliance.\"\n",
    "- Binary Score: \"yes\"\n",
    "\n",
    "#### Example 5:\n",
    "- Reference Context: \"According to Article 8 of the Data Privacy Law, companies must obtain explicit consent before collecting personal data.\"\n",
    "- Contract Segment: \"The company agrees to deliver the product within 7 days of purchase.\"\n",
    "- Analysis: \"The reference context discusses data privacy requirements, which is unrelated to the contract segment about product delivery. There is no direct link between the two.\"\n",
    "- Binary Score: \"no\"\n",
    "\n",
    "<|im_end|>\n",
    "\n",
    "<|im_start|>user\n",
    "Reference Context: [{context}].\n",
    "Contract Segment: [{contract}].\n",
    "<|im_end|>\n",
    "\n",
    "<|im_start|>assistant\"\"\"\n",
    "\n",
    "        websearch_grader_prompt = PromptTemplate.from_template(websearch_grader_template)\n",
    "        # Chain\n",
    "        websearch_grader = websearch_grader_prompt | self.vllm_model | self.runnable\n",
    "        return websearch_grader\n",
    "\n",
    "    def get_grade_web_search_docs(self): \n",
    "        return self.grade_web_search_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ModuleRetrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import hashlib\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "class ModuleRetrieval:\n",
    "    def __init__(self, data_path, is_embedding=False, vector_db_path=\"vectorDB\"):\n",
    "        self.vector_db_path = vector_db_path\n",
    "\n",
    "        # Nếu không cần tạo embedding, thử tải vectorDB\n",
    "        if not is_embedding and os.path.exists(vector_db_path):\n",
    "            self.retriever = Chroma(persist_directory=vector_db_path).as_retriever(search_kwargs={\"k\": 10})\n",
    "\n",
    "            print('TRUNGGGG CHUNK 1', len(Chroma(persist_directory=vector_db_path)))\n",
    "            print(\"Loaded existing vectorDB.\")\n",
    "            return\n",
    "        \n",
    "        # Đọc dữ liệu từ file JSON\n",
    "        with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data_docs = json.load(f)\n",
    "\n",
    "        # Loại bỏ trùng lặp tài liệu dựa trên content & metadata\n",
    "        seen = set()\n",
    "        unique_documents = []\n",
    "        for doc in data_docs:\n",
    "            doc_str = json.dumps(doc, sort_keys=True)  # JSON hóa tài liệu để hash\n",
    "            doc_hash = hashlib.md5(doc_str.encode('utf-8')).hexdigest()\n",
    "            if doc_hash not in seen:\n",
    "                unique_documents.append(Document(page_content=doc[\"content\"], metadata=doc[\"metadata\"]))\n",
    "                seen.add(doc_hash)\n",
    "\n",
    "        print(f\"Unique documents: {len(unique_documents)}\")\n",
    "\n",
    "        # Chia nhỏ tài liệu\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=100)\n",
    "        chunks = text_splitter.split_documents(unique_documents)\n",
    "\n",
    "        print('TRUNGGGG CHUNK 2', len(chunks))\n",
    "        # Tạo embeddings\n",
    "        embedding_function = HuggingFaceEmbeddings(model_name=\"intfloat/multilingual-e5-large\")\n",
    "        vector_store = Chroma.from_documents(\n",
    "            documents=chunks,\n",
    "            embedding=embedding_function,\n",
    "            persist_directory=vector_db_path  # Lưu để sử dụng lại\n",
    "        )\n",
    "\n",
    "        print('TRUNGGGGG    CHUNK 3', len(vector_store))\n",
    "        print(\"Vector database created.\")\n",
    "        self.retriever = vector_store.as_retriever(search_kwargs={\"k\": 10})\n",
    "\n",
    "    def get_module(self):\n",
    "        return self.retriever\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from my_utils import retry_failed_batches\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.documents import Document\n",
    "from langgraph.graph import END, StateGraph\n",
    "from typing import List, Dict, Tuple\n",
    "from collections import defaultdict\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "class Proposition(TypedDict):\n",
    "    proposition_content: str # sửa khi trích metadata\n",
    "    documents: List[Document]  # Sử dụng Document thay vì List[str]\n",
    "    filtered_documents: List[Document]\n",
    "\n",
    "class reference(TypedDict):\n",
    "    text: str\n",
    "    index_list: List[int]\n",
    "    \n",
    "class Question(TypedDict):\n",
    "    question_content: Document\n",
    "    rewrite_question_content: str # rewrite with ref\n",
    "    propositions: List[Proposition]\n",
    "    ref_propositions: List[Proposition]\n",
    "    direct_reference:List[Document]\n",
    "    direct_reference_text:List[reference]\n",
    "    direct_reference_index: List[int]\n",
    "    merged_reference_text: str\n",
    "    generation: str\n",
    "    all_docs: List[Document] #$%^&*\n",
    "    web_search_docs: List[Document]\n",
    "    filtered_web_search_docs: List[Document]\n",
    "    # regen_count: int\n",
    "\n",
    "\n",
    "class propositions_to_rewrite(TypedDict):\n",
    "    question_index: int\n",
    "    proposition_index: int\n",
    "    ref_propositions_index: int\n",
    "    original_proposition: Proposition\n",
    "    \n",
    "class RootText(TypedDict):\n",
    "    Chapter: Dict[int, str]\n",
    "    Article: Dict[int, str]\n",
    "    clause: Dict[Tuple[int, int], str]\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    questions: List[Question]  # Lưu danh sách các câu hỏi thay vì chỉ một câu hỏi\n",
    "    processed_question_index: List[int]\n",
    "    current_question_index: List[int]\n",
    "    propositions_to_rewrite : List[propositions_to_rewrite]\n",
    "    rewrite_count :int # rewrite proposition\n",
    "    regen_count :int #\n",
    "    question_to_regen_index: List[int]#\n",
    "    root_text: RootText\n",
    "\n",
    "\n",
    "class GraphRAG:\n",
    "    def __init__(self, extract_reference_generator, sematic_reference_generator, rewrite_clause_generator, proposition_generator, retriever, retrieval_grader, rag_chain_both, rag_chain_doc, query_transformation_generator, hallucination_grader, answer_grader, web_search_tool, grade_web_search_docs):\n",
    "        self.extract_reference_generator = extract_reference_generator\n",
    "        self.sematic_reference_generator = sematic_reference_generator\n",
    "        self.rewrite_clause_generator = rewrite_clause_generator\n",
    "        self.proposition_generator = proposition_generator\n",
    "        self.retriever = retriever\n",
    "        self.retrieval_grader = retrieval_grader\n",
    "        self.rag_chain_both = rag_chain_both\n",
    "        self.rag_chain_doc = rag_chain_doc\n",
    "        self.query_transformation_generator = query_transformation_generator\n",
    "        self.hallucination_grader = hallucination_grader\n",
    "        self.answer_grader = answer_grader\n",
    "        self.web_search_tool = web_search_tool\n",
    "\n",
    "        ## create app\n",
    "        workflow = StateGraph(GraphState)\n",
    "\n",
    "        # # Define the nodes\n",
    "        workflow.add_node(\"direct_reference\", self.direct_reference)\n",
    "        workflow.add_node(\"grade_direct_reference\", self.grade_direct_reference)\n",
    "        workflow.add_node(\"rewrite_question_content\", self.rewrite_question_content)\n",
    "        workflow.add_node(\"loopback_node\",self.loopback_node) \n",
    "        # workflow.add_node(\"sematic_reference\", sematic_reference) \n",
    "        workflow.add_node(\"create_propositions\", self.create_propositions) \n",
    "        # workflow.add_node(\"conditional_propositions\", conditional_propositions) \n",
    "        workflow.add_node(\"retrieve\", self.retrieve) \n",
    "        workflow.add_node(\"grade_documents\", self.grade_documents)\n",
    "        workflow.add_node(\"generation\", self.generate)\n",
    "        # workflow.add_node(\"hallucination\", hallucination)\n",
    "        workflow.add_node(\"answer_grade_node\", self.answer_grade_node)\n",
    "\n",
    "        workflow.add_node(\"query_transformation\", self.query_transformation)\n",
    "        workflow.add_node(\"check_empty_propositions\", self.check_empty_propositions)\n",
    "        workflow.add_node(\"web_search\", self.web_search)\n",
    "        workflow.add_node(\"grade_web_search_docs\", self.grade_web_search_docs)\n",
    "\n",
    "\n",
    "        workflow.set_entry_point(\"direct_reference\")\n",
    "        workflow.add_edge(\"direct_reference\", \"grade_direct_reference\")\n",
    "        workflow.add_edge(\"grade_direct_reference\", \"rewrite_question_content\")\n",
    "        workflow.add_edge(\"rewrite_question_content\", \"loopback_node\")\n",
    "        # workflow.add_edge(\"direct_reference\", \"grade_direct_reference\")\n",
    "        # workflow.add_edge(\"sematic_reference\", \"grade_direct_reference\")\n",
    "        workflow.add_conditional_edges( \"loopback_node\",\n",
    "                                    self.should_create_proposition,\n",
    "                                    {\n",
    "                                        \"STILL HAVE INDEPENDENT CHUNK\":\"create_propositions\",\n",
    "                                        \"NO INDEPENDENT CHUNK\":END\n",
    "                                    })  \n",
    "        workflow.add_edge(\"create_propositions\", \"retrieve\")  \n",
    "        workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "        workflow.add_edge(\"grade_documents\",\"check_empty_propositions\")\n",
    "        def check_related_laws(state):\n",
    "            if len(state[\"propositions_to_rewrite\"])!=0: \n",
    "                if state[\"rewrite_count\"]<1:\n",
    "                    return \"CANNOT FIND RELATED LAWS\"\n",
    "                else: return \"web_search\"   ######################\n",
    "            else: return \"web_search\"\n",
    "        workflow.add_conditional_edges(\n",
    "            \"check_empty_propositions\",\n",
    "            check_related_laws,\n",
    "            {\n",
    "                \"CANNOT FIND RELATED LAWS\":\"query_transformation\",\n",
    "                \"web_search\":\"web_search\"\n",
    "            }\n",
    "        )\n",
    "        workflow.add_edge(\"web_search\",\"grade_web_search_docs\")\n",
    "        workflow.add_edge(\"grade_web_search_docs\",\"generation\")\n",
    "\n",
    "        # def check_hallucination(state):\n",
    "        #     for question in state[\"questions\"]:\n",
    "        #         if q_idx in new_graph_state[\"current_question_index\"]:\n",
    "        #             if question[\"needed_to_regen\"] and question[\"regen_count\"]<=2:\n",
    "        #                 return \"generation\"\n",
    "        #     return \"answer_grader\"\n",
    "\n",
    "        def check_answer(state):\n",
    "            if state[\"question_to_regen_index\"]!=[] and state[\"regen_count\"]<=1:\n",
    "                    return \"UNSATISFACTORY ANSWER\"\n",
    "            return \"loopback_node\"\n",
    "\n",
    "\n",
    "        workflow.add_conditional_edges(\n",
    "            \"answer_grade_node\",\n",
    "            check_answer,\n",
    "            {\n",
    "                \"UNSATISFACTORY ANSWER\":\"generation\",\n",
    "                \"loopback_node\":\"loopback_node\"\n",
    "            }\n",
    "        )\n",
    "        workflow.add_edge(\"query_transformation\",\"grade_documents\")\n",
    "        workflow.add_edge(\"generation\", \"answer_grade_node\")\n",
    "\n",
    "        # workflow.add_edge(\"hallucination\", \"loopback_node\")\n",
    "\n",
    "        self.app = workflow.compile()\n",
    "\n",
    "    def get_app(self):\n",
    "        return self.app\n",
    "\n",
    "    def direct_reference(self, state):\n",
    "        level_order = [\"Chapter\", \"Article\", \"clause\", \"sub_clause\"]\n",
    "        level_dict={\"Chapter\":\"chapter_number\",\"Article\":\"article_number\",\"clause\":\"clause_number\",\"sub_clause\":\"sub_clause_number\"}\n",
    "\n",
    "        def get_min_level(metadata):\n",
    "            \"\"\"Xác định mức chỉ số nhỏ nhất khác None trong metadata.\"\"\"\n",
    "            # Duyệt ngược từ mức cao nhất xuống mức thấp nhất\n",
    "            # print(\"get_min_level\")\n",
    "            for level in reversed(level_order):\n",
    "                if str(metadata[level]) !='-1':\n",
    "                    # print(f\"level: {level}\")\n",
    "                    return level\n",
    "            return \"Chapter\"\n",
    "        def find_sub_refer(refer):\n",
    "            # print(f\"refer {refer}\")\n",
    "            min_level = get_min_level(refer)\n",
    "            next_min_level=level_order[level_order.index(min_level)+1]\n",
    "            # print(f\"min_level {min_level}\")\n",
    "            # print(f\"next_min_level {next_min_level}\")\n",
    "            sub_refer = set()\n",
    "            for doc in state[\"questions\"]:\n",
    "                # kiểm tra lọc doc từ chapter đến min_level\n",
    "                check=True\n",
    "                if str(refer['Article'])!='-1':\n",
    "                    for level in level_order[1:level_order.index(min_level)+1]:\n",
    "                        # print(f\"level_dict[level] {level_dict[level]}\")\n",
    "                        # print(f\"\"\"doc[\"question_content\"].metadata {doc[\"question_content\"].metadata}\"\"\")\n",
    "                        # print(f\"\"\"doc[\"question_content\"].metadata[level_dict[level]] {doc[\"question_content\"].metadata[level_dict[level]]}\"\"\")\n",
    "                        # print(f\"\"\"refer[level] {refer[level]}\"\"\")\n",
    "                        # print(f\"\"\"level_dict[level] in doc[\"question_content\"].metadata {level_dict[level] in doc[\"question_content\"].metadata}\"\"\")\n",
    "                        # print(f\"\"\"doc[\"question_content\"].metadata[level_dict[level]] != refer[level] {doc[\"question_content\"].metadata[level_dict[level]] != str(refer[level])}\"\"\")\n",
    "                        if level_dict[level] in doc[\"question_content\"].metadata and doc[\"question_content\"].metadata[level_dict[level]] != str(refer[level]):\n",
    "                            check=False\n",
    "                            break\n",
    "                else:\n",
    "                    for level in level_order[:level_order.index(min_level)+1]:\n",
    "                        # print(f\"level_dict[level] {level_dict[level]}\")\n",
    "                        # print(f\"\"\"doc[\"question_content\"].metadata {doc[\"question_content\"].metadata}\"\"\")\n",
    "                        # print(f\"\"\"doc[\"question_content\"].metadata[level_dict[level]] {doc[\"question_content\"].metadata[level_dict[level]]}\"\"\")\n",
    "                        # print(f\"\"\"refer[level] {refer[level]}\"\"\")\n",
    "                        # print(f\"\"\"level_dict[level] in doc[\"question_content\"].metadata {level_dict[level] in doc[\"question_content\"].metadata}\"\"\")\n",
    "                        # print(f\"\"\"doc[\"question_content\"].metadata[level_dict[level]] != refer[level] {doc[\"question_content\"].metadata[level_dict[level]] != str(refer[level])}\"\"\")\n",
    "                        if level_dict[level] in doc[\"question_content\"].metadata and doc[\"question_content\"].metadata[level_dict[level]] != str(refer[level]):\n",
    "                            check=False\n",
    "                            break\n",
    "                if check:\n",
    "                    # append vào sub_refer từ chapter đến next_min_level\n",
    "                    dict_refer = {}\n",
    "                    # print(f\"next_min_level {next_min_level}\")\n",
    "                    for level in level_order[:level_order.index(next_min_level)+1]:\n",
    "                        # print(f\"level {level}\")\n",
    "                        dict_refer[level]=doc[\"question_content\"].metadata[level_dict[level]]\n",
    "                    # các phần tử sau gán = -1\n",
    "                    for level in level_order[level_order.index(next_min_level)+1:]:\n",
    "                        dict_refer[level]=-1\n",
    "                    sub_refer.add(tuple(dict_refer.items()))\n",
    "                else:\n",
    "                    pass\n",
    "                    # sub_refer.append({\"chapter\":doc.metadata[\"chapter_number\"],\"article\":doc.metadata[\"article_number\"],\"clause\":doc.metadata[\"clause_number\"],\"sub_clause\":doc.metadata[\"sub_clause_number\"]})\n",
    "            # chuyển sub_refer từ set sang list\n",
    "            sub_refer = list(sub_refer)\n",
    "            # chuyển từng phần tử trong list thành dict\n",
    "            for i in range(len(sub_refer)):\n",
    "                sub_refer[i]=dict(sub_refer[i])\n",
    "            # print(f\"sub_refer {sub_refer}\")\n",
    "            return sub_refer\n",
    "        def refer2refer_text(refer):\n",
    "            min_level=get_min_level(refer)\n",
    "            refer_text = \"\"\n",
    "            refer_obj={}\n",
    "            if min_level==\"Chapter\":\n",
    "                refer_obj=state[\"root_text\"][min_level][str(refer[min_level])]\n",
    "                refer_obj[\"full_text\"]=refer_obj[\"chapter\"]+refer_obj[\"text\"]\n",
    "            elif  min_level==\"Article\":\n",
    "                refer_obj=state[\"root_text\"][min_level][str(refer[min_level])]\n",
    "                refer_obj[\"full_text\"]=refer_obj[\"chapter\"]+refer_obj[\"article\"]+refer_obj[\"text\"]\n",
    "            elif min_level==\"clause\":\n",
    "                if str(refer[\"Article\"])=='-1':\n",
    "                    print(\"---CAN'T FIND CLAUSE---\")\n",
    "                else :\n",
    "                    refer_obj=state[\"root_text\"][min_level][(str(refer[\"Article\"]),str(refer[min_level]))]\n",
    "                    refer_obj[\"full_text\"]=refer_obj[\"chapter\"]+refer_obj[\"article\"]+refer_obj[\"clause\"]+refer_obj[\"text\"]\n",
    "            else:\n",
    "                for ref_q_index,ref_q in enumerate(state[\"questions\"]):\n",
    "                    if str(refer[\"Article\"])=='-1' or str(refer[\"clause\"])=='-1' :\n",
    "                        print(\"---CAN'T FIND SUB_CLAUSE---\")\n",
    "                        break\n",
    "                    else:    \n",
    "                        if (ref_q[\"question_content\"].metadata[\"article_number\"] == str(refer[\"Article\"])) and \\\n",
    "                        (ref_q[\"question_content\"].metadata[\"clause_number\"] == str(refer[\"clause\"])) and \\\n",
    "                        (ref_q[\"question_content\"].metadata[\"sub_clause_number\"] == str(refer[\"sub_clause\"])) :\n",
    "                            refer_obj={\n",
    "                                \"chapter\":f\"\"\"第{ref_q[\"question_content\"].metadata[\"chapter_number\"]}章　{ref_q[\"question_content\"].metadata[\"chapter_title\"]} \\n \"\"\",\n",
    "                                \"article\":f\"\"\"第{ref_q[\"question_content\"].metadata[\"article_number\"]}条（{ref_q[\"question_content\"].metadata[\"article_title\"]}）\\n \"\"\",\n",
    "                                \"clause\":f\"\"\"{ref_q[\"question_content\"].metadata[\"clause_number\"]}．\"\"\",\n",
    "                                \"sub_clause\":f\"\"\"{ref_q[\"question_content\"].page_content}\"\"\",\n",
    "                                \"text\":\"\"}\n",
    "                            refer_obj[\"full_text\"]=refer_obj[\"chapter\"]+refer_obj[\"article\"]+refer_obj[\"clause\"]+refer_obj[\"sub_clause\"]+refer_obj[\"text\"]\n",
    "            return refer_obj\n",
    "\n",
    "        def get_list_index_from_refer(refer):\n",
    "            # print(refer)\n",
    "            direct_reference_index=[]\n",
    "            for ref_q_index,ref_q in enumerate(state[\"questions\"]):\n",
    "                if str(refer[\"Article\"])!='-1':\n",
    "                    if (str(refer[\"Article\"])=='-1' or ref_q[\"question_content\"].metadata[\"article_number\"] == str(refer[\"Article\"])) and \\\n",
    "                    (str(refer[\"clause\"])=='-1' or ref_q[\"question_content\"].metadata[\"clause_number\"] == str(refer[\"clause\"])) and \\\n",
    "                    (str(refer[\"sub_clause\"])=='-1' or ref_q[\"question_content\"].metadata[\"sub_clause_number\"] == str(refer[\"sub_clause\"])) :\n",
    "                        direct_reference_index.append(ref_q_index)\n",
    "                else:\n",
    "                    if (str(refer[\"Chapter\"])=='-1' or ref_q[\"question_content\"].metadata[\"chapter_number\"] == str(refer[\"Chapter\"])) and \\\n",
    "                    (str(refer[\"Article\"])=='-1' or ref_q[\"question_content\"].metadata[\"article_number\"] == str(refer[\"Article\"])) and \\\n",
    "                    (str(refer[\"clause\"])=='-1' or ref_q[\"question_content\"].metadata[\"clause_number\"] == str(refer[\"clause\"])) and \\\n",
    "                    (str(refer[\"sub_clause\"])=='-1' or ref_q[\"question_content\"].metadata[\"sub_clause_number\"] == str(refer[\"sub_clause\"])) :\n",
    "                        direct_reference_index.append(ref_q_index)\n",
    "            return direct_reference_index\n",
    "        \n",
    "        def get_reftext(refer):\n",
    "            max_length = 512\n",
    "            refer_obj = refer2refer_text(refer)\n",
    "            index_list=get_list_index_from_refer(refer)\n",
    "            if len(refer_obj[\"full_text\"]) < max_length:\n",
    "                refer_obj[\"index_list\"]=index_list\n",
    "                return [refer_obj]  # Trả về dưới dạng danh sách\n",
    "            \n",
    "            sub_refers = find_sub_refer(refer)\n",
    "            # print(f\"sub_refers {sub_refers}\")\n",
    "            if not sub_refers:\n",
    "                refer_obj[\"index_list\"]=index_list\n",
    "                return [refer_obj]  # Trả về dưới dạng danh sách\n",
    "            ref_texts = []\n",
    "            for sub_refer in sub_refers:\n",
    "                indexes= get_list_index_from_refer(sub_refer)\n",
    "                ref_texts.extend(get_reftext(sub_refer))\n",
    "            return ref_texts\n",
    "\n",
    "        print(\"---DIRECT REFERENCE---\")\n",
    "        \"\"\"\n",
    "        Lặp qua các proposition để trích xuất phần reference, sau đó lọc các proposition \n",
    "        thật sự liên quan bằng LLM, rồi cập nhật lại state với thông tin tham chiếu.\n",
    "        đã bỏ qua sự cần thiết của chương bởi chương có thể bị sai, nên nếu có điều thì không cần xét chương\n",
    "        \"\"\"\n",
    "        #Kiểm tra độ dài tham chiếu nếu lớn hơn max_length thì kiểm tra cấp độ nhỏ hơn, nếu không có thì trả về.Ngược lại độ dài bé hơn max_length thì trả về \n",
    "        batch_inputs = []\n",
    "        # batch_map = []  # Mỗi phần tử là (q_idx, p_idx)\n",
    "        # new_graph_state = {\"questions\": []}\n",
    "        new_graph_state = deepcopy(state)\n",
    "        # Tạo batch_inputs cho LLM\n",
    "        for q_idx, question in enumerate(new_graph_state[\"questions\"]):\n",
    "            question_content = question[\"question_content\"]\n",
    "            # Lấy metadata từ đối tượng, nếu không có thì trả về None\n",
    "            question_metadata = getattr(question_content, \"metadata\", None)\n",
    "            # print(f\"question_metadata {question_metadata}\")\n",
    "            metadata_parts = []\n",
    "            # Sử dụng getattr để lấy thuộc tính nếu tồn tại\n",
    "            chapter = question_metadata[\"chapter_number\"] if question_metadata[\"chapter_number\"] else None\n",
    "            if chapter:\n",
    "                metadata_parts.append(f\"current chapter: {chapter}\")\n",
    "            else:\n",
    "                metadata_parts.append(f'current chapter: \"\"')\n",
    "\n",
    "            article = question_metadata[\"article_number\"] if question_metadata[\"article_number\"] else None\n",
    "            if article:\n",
    "                metadata_parts.append(f\"current article: {article}\")\n",
    "            clause = question_metadata[\"clause_number\"] if question_metadata[\"clause_number\"] else None\n",
    "            if clause:\n",
    "                metadata_parts.append(f\"current clause: {clause}\")\n",
    "            sub_clause = question_metadata[\"sub_clause_number\"] if question_metadata[\"sub_clause_number\"] else None\n",
    "            if sub_clause:\n",
    "                metadata_parts.append(f\"current sub-clause: {sub_clause}\")\n",
    "            entry = {\"document\": question_content.page_content}\n",
    "            entry[\"metadata\"] = \", \".join(metadata_parts)\n",
    "            print(\"entry+++++++\", entry)\n",
    "            batch_inputs.append(entry)\n",
    "        responses = self.extract_reference_generator.batch(batch_inputs) if batch_inputs else []\n",
    "        responses =retry_failed_batches(batch_inputs,responses,self.extract_reference_generator)\n",
    "        print(\"responses+++++++\", responses)\n",
    "        \n",
    "        # new_graph_state = {\"questions\": []}\n",
    "        count=0\n",
    "        questions=[]\n",
    "        # Tạo state mới\n",
    "        for q_idx, question in enumerate(new_graph_state[\"questions\"]):\n",
    "                new_question = question.copy()\n",
    "                new_question[\"direct_reference\"]=[]\n",
    "                new_question[\"direct_reference_index\"]=[]\n",
    "                new_question[\"direct_reference_text\"]=[]\n",
    "                response = responses[q_idx]\n",
    "                # print(f\"response {response}\")\n",
    "                if response['has_reference'] == \"yes\" and response[\"is_extractable\"] == \"yes\":\n",
    "                    # print(\"=========================\")\n",
    "                    # print(f\"response {response}\")\n",
    "                    # print(f\"\"\"question {question}\"\"\")\n",
    "                    for reference in response[\"references\"]:\n",
    "                        if reference[\"resolved\"][\"Chapter\"]==-1 and reference[\"resolved\"][\"Article\"]==-1 and reference[\"resolved\"][\"clause\"]==-1 and reference[\"resolved\"][\"sub_clause\"]==-1:\n",
    "                            continue\n",
    "                        count+=1\n",
    "                        # teee=\"No\"\n",
    "\n",
    "                        #####\n",
    "                \n",
    "                    \n",
    "                        resolved = reference[\"resolved\"]\n",
    "                        metadata = question[\"question_content\"].metadata\n",
    "\n",
    "                        if resolved[\"sub_clause\"] != -1:\n",
    "                            resolved[\"clause\"] = resolved[\"clause\"] if resolved[\"clause\"] != -1 else metadata[\"clause_number\"]\n",
    "                            resolved[\"Article\"] = resolved[\"Article\"] if resolved[\"Article\"] != -1 else metadata[\"article_number\"]\n",
    "\n",
    "                        if resolved[\"clause\"] != -1 and resolved[\"Article\"] == -1:\n",
    "                            resolved[\"Article\"] = metadata[\"article_number\"]\n",
    "                        # print(f\"resolved {resolved}\")\n",
    "                        for ref_q_index,ref_q in enumerate(state[\"questions\"]):\n",
    "                            if reference[\"resolved\"][\"Article\"]!=-1:\n",
    "                                if (reference[\"resolved\"][\"Article\"]==-1 or ref_q[\"question_content\"].metadata[\"article_number\"] == str(reference[\"resolved\"][\"Article\"])) and \\\n",
    "                                (reference[\"resolved\"][\"clause\"]==-1 or ref_q[\"question_content\"].metadata[\"clause_number\"] == str(reference[\"resolved\"][\"clause\"])) and \\\n",
    "                                (reference[\"resolved\"][\"sub_clause\"]==-1 or ref_q[\"question_content\"].metadata[\"sub_clause_number\"] == str(reference[\"resolved\"][\"sub_clause\"])) :\n",
    "                                    new_question[\"direct_reference\"].append(ref_q[\"question_content\"])\n",
    "                                    new_question[\"direct_reference_index\"].append(ref_q_index)\n",
    "                                    teee=\"yes\"\n",
    "                            else:\n",
    "                                if (reference[\"resolved\"][\"Chapter\"]==-1 or ref_q[\"question_content\"].metadata[\"chapter_number\"] == str(reference[\"resolved\"][\"Chapter\"])) and \\\n",
    "                                (reference[\"resolved\"][\"Article\"]==-1 or ref_q[\"question_content\"].metadata[\"article_number\"] == str(reference[\"resolved\"][\"Article\"])) and \\\n",
    "                                (reference[\"resolved\"][\"clause\"]==-1 or ref_q[\"question_content\"].metadata[\"clause_number\"] == str(reference[\"resolved\"][\"clause\"])) and \\\n",
    "                                (reference[\"resolved\"][\"sub_clause\"]==-1 or ref_q[\"question_content\"].metadata[\"sub_clause_number\"] == str(reference[\"resolved\"][\"sub_clause\"])) :\n",
    "                                    new_question[\"direct_reference\"].append(ref_q[\"question_content\"])\n",
    "                                    new_question[\"direct_reference_index\"].append(ref_q_index)\n",
    "                                    teee=\"yes\"\n",
    "                        if new_question[\"direct_reference\"]:\n",
    "                        # print(f\"\"\"batch_inputs: {batch_inputs[q_idx]}\"\"\")\n",
    "                        # print(f\"\"\"reference[\"resolved\"] {reference[\"resolved\"]}\"\"\")\n",
    "                            direct_reference_text=get_reftext(reference[\"resolved\"])\n",
    "                            new_question[\"direct_reference_text\"]=direct_reference_text\n",
    "\n",
    "                        # if teee==\"No\":\n",
    "                        #     print(\"|||||||||||||||||||||\")\n",
    "                        #     print(f\"response {response}\")\n",
    "                        #     print(f\"\"\"question {question}\"\"\")\n",
    "                questions.append(new_question)\n",
    "                new_graph_state[\"processed_question_index\"]=[]\n",
    "        new_graph_state[\"questions\"]=questions\n",
    "        return new_graph_state\n",
    "    \n",
    "    def grade_direct_reference(self, state):\n",
    "        \"\"\"\n",
    "        Kiểm tra direct_reference của mỗi câu hỏi:\n",
    "        - Chỉ thực hiện nếu ít nhất một câu hỏi có hơn 1 direct_reference.\n",
    "        - Dùng model để kiểm tra độ liên quan của direct_reference.\n",
    "        - Cập nhật lại graph state với direct_reference đã được lọc.\n",
    "        \"\"\"\n",
    "        print(\"---GRADE REFERENCE---\")\n",
    "\n",
    "        def nested_defaultdict():\n",
    "            return {\"values\": [], \"sub\": defaultdict(nested_defaultdict)}\n",
    "\n",
    "        def merge_filtered_list(filtered_list):\n",
    "            merged_dict = defaultdict(nested_defaultdict)\n",
    "\n",
    "            for item in filtered_list:\n",
    "                chapter = item[\"chapter\"]\n",
    "                article = item[\"article\"] if item[\"article\"] != \"-1\" else None\n",
    "                clause = item[\"clause\"] if item[\"clause\"] != \"-1\" else None\n",
    "                sub_clause = item[\"sub_clause\"] if item[\"sub_clause\"] != \"-1\" else None\n",
    "                value = item[\"text\"].strip()\n",
    "\n",
    "                if article is None:\n",
    "                    merged_dict[chapter][\"values\"].append(value)\n",
    "                elif clause is None:\n",
    "                    merged_dict[chapter][\"sub\"][article][\"values\"].append(value)\n",
    "                elif sub_clause is None:\n",
    "                    merged_dict[chapter][\"sub\"][article][\"sub\"][clause][\"values\"].append(value)\n",
    "                else:\n",
    "                    merged_dict[chapter][\"sub\"][article][\"sub\"][clause][\"sub\"][sub_clause][\"values\"].append(value)\n",
    "\n",
    "            # Chuyển về định dạng văn bản\n",
    "            def format_dict(d):\n",
    "                result = []\n",
    "                if \"values\" in d and d[\"values\"]:\n",
    "                    result.extend(v for v in d[\"values\"])\n",
    "                for key, sub_d in d[\"sub\"].items():\n",
    "                    result.append(key)\n",
    "                    result.extend(format_dict(sub_d))\n",
    "                return result\n",
    "\n",
    "            merged_result = []\n",
    "            for chapter, content in merged_dict.items():\n",
    "                merged_result.append(chapter)\n",
    "                merged_result.extend(format_dict(content))\n",
    "\n",
    "            return \"\\n\".join(merged_result)\n",
    "        print(\"---GRADE REFERENCE---\")\n",
    "        new_graph_state = deepcopy(state)  # Tạo bản sao để tránh sửa đổi trực tiếp\n",
    "    \n",
    "        batch_inputs = []\n",
    "        batch_indicates = []\n",
    "        has_multiple_references = False  # Cờ kiểm tra có câu hỏi nào có >1 reference không\n",
    "        question_have_ref_count=0\n",
    "        # Thu thập dữ liệu batch\n",
    "        temp =False\n",
    "        for q_idx, question in enumerate(state[\"questions\"]):\n",
    "            if len(question.get(\"direct_reference_text\", [])) > 1:\n",
    "                has_multiple_references = True\n",
    "                for r_idx, direct_reference_text in enumerate(question[\"direct_reference_text\"]):\n",
    "                    batch_inputs.append({\n",
    "                        \"main_text\": question[\"question_content\"].page_content,\n",
    "                        \"referenced_text\": direct_reference_text[\"full_text\"]\n",
    "                    })\n",
    "                    if not temp:\n",
    "                        print(f\"\"\"\n",
    "                            \"main_text\": {question[\"question_content\"].page_content},\n",
    "                            \"referenced_text\": {direct_reference_text[\"full_text\"]}\n",
    "                        \"\"\")\n",
    "                        temp=True\n",
    "                    batch_indicates.append((q_idx, r_idx))\n",
    "            elif len(question.get(\"direct_reference_text\", [])) == 1:\n",
    "                question[\"merged_reference_text\"]=question[\"direct_reference_text\"][0]\n",
    "            else:\n",
    "                question[\"merged_reference_text\"]=\"\"\n",
    "            if len(question.get(\"direct_reference_text\", [])) >= 1:\n",
    "                question_have_ref_count+=1\n",
    "        # Nếu không có câu hỏi nào có >1 direct_reference, thoát sớm\n",
    "        print(f\"question_have_ref_count {question_have_ref_count}\")\n",
    "        if not has_multiple_references:\n",
    "            print(\"NOTHING TO GRADE\")\n",
    "            return state\n",
    "\n",
    "        # Chạy mô hình kiểm tra tính liên quan\n",
    "        responses = self.sematic_reference_generator.batch(batch_inputs) if batch_inputs else []\n",
    "        responses =retry_failed_batches(batch_inputs,responses,self.sematic_reference_generator)\n",
    "        # Ánh xạ kết quả vào new_graph_state\n",
    "        result_map = {batch_indicates[i]: responses[i][\"provide_additional_meaning\"].strip().lower() == \"yes\"\n",
    "                    for i in range(len(responses))}\n",
    "        question_have_ref_count=0\n",
    "        for q_idx, question in enumerate(new_graph_state[\"questions\"]):\n",
    "            if len(question.get(\"direct_reference_text\", [])) > 1:\n",
    "                # Lọc lại danh sách direct_reference dựa trên kết quả kiểm tra\n",
    "                question[\"direct_reference_text\"] = [\n",
    "                    ref for r_idx, ref in enumerate(question[\"direct_reference_text\"])\n",
    "                    if result_map.get((q_idx, r_idx), False)  # Chỉ giữ lại ref nếu is_related == \"yes\"\n",
    "                ]\n",
    "            if len(question.get(\"direct_reference_text\", [])) != 0:\n",
    "                question[\"merged_reference_text\"]=merge_filtered_list(question[\"direct_reference_text\"])\n",
    "                question_have_ref_count+=1\n",
    "            else:\n",
    "                question[\"merged_reference_text\"]=\"\"\n",
    "                \n",
    "        print(f\"question_have_ref_count {question_have_ref_count}\")\n",
    "            \n",
    "        for q_idx, question in enumerate(new_graph_state[\"questions\"]):\n",
    "            new_index_list = []\n",
    "            for direct_reference_text in question[\"direct_reference_text\"]:\n",
    "                new_index_list.extend(direct_reference_text[\"index_list\"])\n",
    "            question[\"direct_reference_index\"]=new_index_list\n",
    "        \n",
    "        return new_graph_state\n",
    "    \n",
    "    def rewrite_question_content(self, state):\n",
    "        \"\"\"\n",
    "        Lặp qua từng câu hỏi và tạo batch_inputs để cập nhật lại nội dung câu hỏi:\n",
    "        - Với mỗi câu hỏi, tạo batch_inputs chứa {\"main_section\": question[\"question_content\"], \"refer_sections\": direct_reference_text}.\n",
    "        - Chạy batch qua mô hình để tạo câu hỏi rewrite.\n",
    "        - Lưu kết quả vào question[\"rewrite_question_content\"], là một list.\n",
    "        \"\"\"\n",
    "        print(\"---REWRITE QUESTION CONTENT---\")\n",
    "        new_graph_state = deepcopy(state)  # Sao chép tránh sửa đổi trực tiếp\n",
    "        \n",
    "        batch_inputs = []\n",
    "        batch_indicates = []\n",
    "        \n",
    "        # Thu thập dữ liệu batch\n",
    "        for q_idx, question in enumerate(state[\"questions\"]):\n",
    "            new_graph_state[\"questions\"][q_idx][\"rewrite_question_content\"] = \"\"\n",
    "            if question.get(\"merged_reference_text\",\"\") != \"\":\n",
    "                batch_inputs.append({\n",
    "                    \"main_section\": question[\"question_content\"].page_content,\n",
    "                    \"refer_sections\": question[\"merged_reference_text\"]\n",
    "                })\n",
    "                batch_indicates.append(q_idx)\n",
    "        # print(batch_inputs)\n",
    "        # Nếu không có dữ liệu nào để xử lý, thoát sớm\n",
    "        if not batch_inputs:\n",
    "            print(\"NO QUESTIONS TO REWRITE\")\n",
    "            return state\n",
    "        \n",
    "        # Chạy mô hình sinh câu hỏi rewrite\n",
    "        responses = self.rewrite_clause_generator.batch(batch_inputs) if batch_inputs else []\n",
    "        responses =retry_failed_batches(batch_inputs,responses,self.rewrite_clause_generator)\n",
    "        \n",
    "        # print(f\"responses {responses}\")\n",
    "        # Cập nhật kết quả vào new_graph_state\n",
    "        for i, q_idx in enumerate(batch_indicates):\n",
    "            new_graph_state[\"questions\"][q_idx][\"rewrite_question_content\"]=(responses[i][\"generated_section\"])\n",
    "        return new_graph_state\n",
    "    \n",
    "    def loopback_node(self, graph_state):\n",
    "        print(\"--- LOOPBACK NODE ---\")\n",
    "        \n",
    "        new_graph_state = deepcopy(graph_state)  # Tạo bản sao sâu để tránh sửa đổi trực tiếp vào state gốc\n",
    "        processed_question_index = graph_state[\"processed_question_index\"]\n",
    "        next_batch = []\n",
    "        current_batch =[]\n",
    "        direct_count=0\n",
    "        ref_count=0\n",
    "        for index, question in enumerate(graph_state[\"questions\"]):\n",
    "            if index not in processed_question_index:\n",
    "                if len(question[\"direct_reference_text\"])==0:\n",
    "                    current_batch.append(index)\n",
    "                    direct_count+=1\n",
    "                elif all(ref_q in processed_question_index for ref_q in question[\"direct_reference_index\"]):\n",
    "                    current_batch.append(index)\n",
    "                    ref_count+=1\n",
    "                else:\n",
    "                    next_batch.append(index)  # Chưa đủ điều kiện chạy\n",
    "                    \n",
    "                # if len(question[\"direct_reference_text\"])!=0 and (not all(ref_q in processed_question_index for ref_q in question[\"direct_reference_index\"])):\n",
    "                #     next_batch.append(index)  # Chưa đủ điều kiện chạy\n",
    "                # else:\n",
    "                #     current_batch.append(index)\n",
    "        print(f\"direct count {direct_count}\")\n",
    "        print(f\"ref count {ref_count}\")\n",
    "        new_graph_state[\"current_question_index\"]=current_batch\n",
    "        new_graph_state[\"question_to_regen_index\"]=[]\n",
    "        new_graph_state[\"regen_count\"]=0\n",
    "        \n",
    "        return new_graph_state\n",
    "    \n",
    "    def answer_grade_node(self, state):   #$%^&*\n",
    "        def docs2text(docs):\n",
    "            return '\\n'.join([doc.page_content for doc in docs])\n",
    "        def question2text(question):\n",
    "            return f\"\"\"第{question.metadata[\"chapter_number\"]}章　{question.metadata[\"chapter_title\"]} \\n 第{question.metadata[\"article_number\"]}条（{question.metadata[\"article_title\"]}）\\n{question.page_content}\"\"\"\n",
    "        \n",
    "        print(\"---HALLUCINATION GRADE---\")\n",
    "        new_graph_state = deepcopy(state)\n",
    "        batch_inputs = []\n",
    "        batch_indices = []\n",
    "        for q_idx, question in enumerate(new_graph_state[\"questions\"]):\n",
    "            if len(new_graph_state[\"question_to_regen_index\"])!=0:\n",
    "                if q_idx in new_graph_state[\"question_to_regen_index\"]:\n",
    "                    doc_list = set()  # Dùng set để loại bỏ tài liệu trùng lặp\n",
    "                    for p_idx, proposition in enumerate(question[\"propositions\"]):\n",
    "                        if proposition[\"filtered_documents\"]:\n",
    "                            doc_list.update(str(proposition[\"filtered_documents\"]))\n",
    "                    if len(question[\"direct_reference_text\"])!=0:\n",
    "                        # print(\"OH NOOOOOOOOOOOOOOO\")\n",
    "                        for ref_p_idx, proposition in enumerate(question[\"ref_propositions\"]):\n",
    "                            if proposition[\"filtered_documents\"]:\n",
    "                                doc_list.update(str(proposition[\"filtered_documents\"]))\n",
    "                    doc_list=[Document(doc) for doc in doc_list]\n",
    "                    batch_inputs.append({\"laws\":docs2text(doc_list),\"contract\":question2text(question[\"question_content\"]),\"explanation\":question[\"generation\"]})\n",
    "                    batch_indices.append(q_idx)\n",
    "            else:\n",
    "                if q_idx in new_graph_state[\"current_question_index\"] and question[\"generation\"]!=\"\"\"{\"evaluation\":\"insufficient_information\",\n",
    "                            \"explanation\":\"CANNOT FIND ANY RELEVANT DOCUMENT\"}\"\"\":\n",
    "                # print(f\"\"\"============== {question[\"generation\"]}\"\"\")\n",
    "                    doc_list = set()  # Dùng set để loại bỏ tài liệu trùng lặp\n",
    "                    for p_idx, proposition in enumerate(question[\"propositions\"]):\n",
    "                        if proposition[\"filtered_documents\"]:\n",
    "                            doc_list.update(str(proposition[\"filtered_documents\"]))\n",
    "                    if len(question[\"direct_reference_text\"])!=0:\n",
    "                        # print(\"OH NOOOOOOOOOOOOOOO\")\n",
    "                        for ref_p_idx, proposition in enumerate(question[\"ref_propositions\"]):\n",
    "                            if proposition[\"filtered_documents\"]:\n",
    "                                doc_list.update(str(proposition[\"filtered_documents\"]))\n",
    "                    doc_list=[Document(doc) for doc in doc_list]\n",
    "                    batch_inputs.append({\"laws\":docs2text(doc_list),\"contract\":question2text(question[\"question_content\"]),\"explanation\":question[\"generation\"]})\n",
    "                    batch_indices.append(q_idx)\n",
    "        responses = self.hallucination_grader.batch(batch_inputs)\n",
    "        responses = retry_failed_batches(batch_inputs, responses, self.hallucination_grader)\n",
    "        new_graph_state[\"question_to_regen_index\"]=[]\n",
    "        for idx, response in enumerate(responses):\n",
    "            q_idx = batch_indices[idx]\n",
    "            if response[\"is_supported\"].lower()==\"no\":\n",
    "                new_graph_state[\"question_to_regen_index\"].append(q_idx)\n",
    "        print(\"---GRADE ANSWER---\")\n",
    "        \n",
    "        grade_batch_inputs =[]\n",
    "        grade_batch_indicates =[]\n",
    "        for q_idx, question in enumerate(new_graph_state[\"questions\"]):\n",
    "            if q_idx in new_graph_state[\"current_question_index\"]:\n",
    "                if q_idx not in new_graph_state[\"question_to_regen_index\"] and question[\"generation\"]!=\"\"\"{\"evaluation\":\"insufficient_information\",\n",
    "                            \"explanation\":\"CANNOT FIND ANY RELEVANT DOCUMENT\"}\"\"\":\n",
    "                    grade_batch_inputs.append({\"contract\":question2text(question[\"question_content\"]),\"explanation\":question[\"generation\"]})\n",
    "                    grade_batch_indicates.append(q_idx)\n",
    "        grade_responses = self.answer_grader.batch(grade_batch_inputs)\n",
    "        grade_responses = retry_failed_batches(grade_batch_inputs, grade_responses, self.answer_grader)\n",
    "        for idx, response in enumerate(grade_responses):\n",
    "            q_idx = grade_batch_indicates[idx]\n",
    "            if response[\"is_valid_explanation\"].lower()==\"no\":\n",
    "                new_graph_state[\"question_to_regen_index\"].append(q_idx)\n",
    "        return new_graph_state\n",
    "    \n",
    "    def create_propositions(self, graph_state):   #$%^&*\n",
    "        def question2text(question):\n",
    "            return f\"\"\"第{question.metadata[\"chapter_number\"]}章　{question.metadata[\"chapter_title\"]} \\n 第{question.metadata[\"article_number\"]}条（{question.metadata[\"article_title\"]}）\\n{question.page_content}\"\"\"\n",
    "        \n",
    "        print(\"---CREATE PROPOSITIONS---\")\n",
    "        new_graph_state = deepcopy(graph_state)  # Tạo bản sao sâu để tránh sửa đổi trực tiếp vào state gốc\n",
    "        direct_batch_inputs = []\n",
    "        ref_batch_inputs = []\n",
    "        direct_batch_map = []\n",
    "        ref_batch_map = []\n",
    "        print(f\"\"\"len(graph_state[\"current_question_index\"]) {len(graph_state[\"current_question_index\"])}\"\"\")\n",
    "        count=0\n",
    "        for index, question in enumerate(graph_state[\"questions\"]):\n",
    "            if index in graph_state[\"current_question_index\"]:\n",
    "                if len(question[\"direct_reference_text\"])==0:\n",
    "                    # count+=1\n",
    "                    direct_batch_inputs.append({\"document\": question[\"question_content\"].page_content})  # Chạy hàm proposition_gen\n",
    "                    direct_batch_map.append(index)\n",
    "                elif  all(ref_q in graph_state[\"processed_question_index\"] for ref_q in question[\"direct_reference_index\"]):\n",
    "                    count+=1\n",
    "                    ref_batch_inputs.append({\"document\": question[\"rewrite_question_content\"]})\n",
    "                    ref_batch_map.append(index)\n",
    "                else:\n",
    "                    print(\"HAVE SOME TROUBLES\")\n",
    "        print(f\"len(direct ) {len(direct_batch_inputs)}\")\n",
    "        print(f\"len(ref ) {len(ref_batch_inputs)}\")\n",
    "        print(f\"count {count}\")\n",
    "        # Chạy batch proposition\n",
    "        direct_proposition_responses = self.proposition_generator.batch(direct_batch_inputs)\n",
    "        direct_proposition_responses =retry_failed_batches(direct_batch_inputs,direct_proposition_responses,self.proposition_generator)\n",
    "\n",
    "        ref_proposition_responses = self.proposition_generator.batch(ref_batch_inputs)\n",
    "        ref_proposition_responses =retry_failed_batches(ref_batch_inputs,ref_proposition_responses,self.proposition_generator)\n",
    "        # print(f\"direct_proposition_responses {direct_proposition_responses}\")\n",
    "        for i, proposition_response in enumerate(direct_proposition_responses):\n",
    "            propositions = [\n",
    "                {\n",
    "                    \"proposition_content\": prop[\"proposition\"],\n",
    "                    \"documents\": [],\n",
    "                    \"filtered_documents\": []\n",
    "                }\n",
    "                for prop in proposition_response['propositions']\n",
    "            ]\n",
    "            propositions.append({\n",
    "                \"proposition_content\":question2text(new_graph_state[\"questions\"][direct_batch_map[i]][\"question_content\"]),\n",
    "                \"documents\": [],\n",
    "                \"filtered_documents\": []\n",
    "            })\n",
    "            new_graph_state[\"questions\"][direct_batch_map[i]][\"propositions\"] = propositions\n",
    "            new_graph_state[\"processed_question_index\"].append(direct_batch_map[i])\n",
    "        for i, proposition_response in enumerate(ref_proposition_responses):\n",
    "            propositions = [\n",
    "                {\n",
    "                    \"proposition_content\": prop[\"proposition\"],\n",
    "                    \"documents\": [],\n",
    "                    \"filtered_documents\": []\n",
    "                }\n",
    "                for prop in proposition_response['propositions']\n",
    "            ]\n",
    "            new_graph_state[\"questions\"][ref_batch_map[i]][\"ref_propositions\"] = propositions\n",
    "            new_graph_state[\"processed_question_index\"].append(ref_batch_map[i])\n",
    "\n",
    "        return new_graph_state\n",
    "    \n",
    "    def retrieve(self, state):\n",
    "        print(\"---RETRIEVE---\")\n",
    "        new_graph_state = deepcopy(state)  # Tạo bản sao sâu để tránh sửa đổi trực tiếp vào state gốc\n",
    "        count=0\n",
    "        for index, question in enumerate(new_graph_state[\"questions\"]):\n",
    "        # for question in new_graph_state[\"questions\"]:\n",
    "            if index in new_graph_state[\"current_question_index\"]:\n",
    "                for p_idx, proposition in enumerate(question[\"propositions\"]):\n",
    "                    retrieved_docs = self.retriever.invoke(proposition[\"proposition_content\"])\n",
    "                    question[\"propositions\"][p_idx][\"documents\"] = retrieved_docs  # Cập nhật trực tiếp vào bản sao mới\n",
    "                    count+=1\n",
    "                for p_idx, proposition in enumerate(question[\"ref_propositions\"]):\n",
    "                    retrieved_docs = self.retriever.invoke(proposition[\"proposition_content\"])\n",
    "                    question[\"ref_propositions\"][p_idx][\"documents\"]= retrieved_docs\n",
    "                    count+=1\n",
    "        print(f\"---have retrieved for {count} propositions---\")\n",
    "        new_graph_state[\"rewrite_count\"]=0\n",
    "        return new_graph_state\n",
    "\n",
    "    def grade_documents(self, state):   #$%^&*\n",
    "        def question2text(question):\n",
    "            return f\"\"\"第{question.metadata[\"chapter_number\"]}章　{question.metadata[\"chapter_title\"]} \\n 第{question.metadata[\"article_number\"]}条（{question.metadata[\"article_title\"]}）\\n{question.page_content}\"\"\"\n",
    "    \n",
    "        \"\"\"\n",
    "        Đánh giá và lọc bỏ các tài liệu không liên quan cho những proposition có filtered_documents == [].\n",
    "        Những proposition đã có filtered_documents (không rỗng) sẽ được giữ nguyên.\n",
    "\n",
    "        Args:\n",
    "            state (Dict): Graph state chứa danh sách các câu hỏi và propositions.\n",
    "        \n",
    "        Returns:\n",
    "            Dict: Graph state mới với các proposition đã được cập nhật filtered_documents.\n",
    "        \"\"\"\n",
    "        print(\"---GRADE DOCUMENTS---\")\n",
    "        \n",
    "        new_graph_state = deepcopy(state)  # Tạo bản sao sâu để tránh sửa đổi trực tiếp vào state gốc\n",
    "        batch_inputs = []\n",
    "        batch_map = []  # Mỗi phần tử là (q_idx, p_idx, is_ref, doc_idx)\n",
    "        # if new_graph_state[\"rewrite_count\"]!=0:\n",
    "        #     for proposition in new_graph_state[\"propositions_to_rewrite\"]:\n",
    "        #         if proposition[\"proposition_index\"]!=-1:\n",
    "        #             for doc_idx,doc in new_graph_state[\"questions\"][proposition[\"question_index\"]][\"propositions\"][proposition[\"proposition_index\"]][\"documents\"]:\n",
    "        #                 batch_inputs.append({\n",
    "        #                     \"context\": doc.page_content,\n",
    "        #                     \"contract\": proposition[\"original_proposition\"][\"proposition_content\"]\n",
    "        #                 })\n",
    "        #                 batch_map.append((proposition[\"question_index\"],proposition[\"propositions_index\"],-1,doc_idx))\n",
    "        #         else:\n",
    "        #             for doc_idx,doc in new_graph_state[\"questions\"][proposition[\"question_index\"]][\"ref_propositions\"][proposition[\"ref_propositions_index\"]][\"documents\"]:\n",
    "        #                 batch_inputs.append({\n",
    "        #                     \"context\": doc.page_content,\n",
    "        #                     \"contract\": proposition[\"original_proposition\"][\"proposition_content\"]\n",
    "        #                 })\n",
    "        #                 batch_map.append((proposition[\"question_index\"],-1,proposition[\"ref_propositions_index\"],doc_idx))\n",
    "        \n",
    "        # Duyệt qua state để thu thập các input cần xử lý (chỉ với proposition chưa được đánh giá)\n",
    "        # else:\n",
    "        for q_idx, question in enumerate(new_graph_state[\"questions\"]):\n",
    "            # Xử lý propositions thông thường\n",
    "            if q_idx in new_graph_state[\"current_question_index\"]:\n",
    "                if question.get(\"rewrite_question_content\", \"\")!=\"\":\n",
    "                    contract= question[\"rewrite_question_content\"]\n",
    "                else:\n",
    "                    contract= question2text(question[\"question_content\"])\n",
    "                for p_idx, proposition in enumerate(question[\"propositions\"]):\n",
    "                    if proposition.get(\"filtered_documents\", []):\n",
    "                        continue\n",
    "                    for doc_idx, doc in enumerate(proposition[\"documents\"]):\n",
    "                        \n",
    "                        batch_inputs.append({\n",
    "                            \"context\": doc.page_content,\n",
    "                            # \"contract\": proposition[\"proposition_content\"]\n",
    "                            \"contract\": contract\n",
    "                        })\n",
    "                        batch_map.append((q_idx, p_idx, -1, doc_idx))  # is_ref = 0\n",
    "                \n",
    "                for ref_p_idx, proposition in enumerate(question.get(\"ref_propositions\", [])):\n",
    "                    if proposition.get(\"filtered_documents\", []):\n",
    "                        continue\n",
    "                    for doc_idx, doc in enumerate(proposition[\"documents\"]):\n",
    "                        batch_inputs.append({\n",
    "                            \"context\": doc.page_content,\n",
    "                            # \"contract\": proposition[\"proposition_content\"]\n",
    "                            \"contract\": contract\n",
    "                            \n",
    "                        })\n",
    "                        batch_map.append((q_idx, -1, ref_p_idx, doc_idx))  # is_ref = 1\n",
    "            \n",
    "        # Gọi batch processing nếu có input; nếu không có, responses là danh sách rỗng\n",
    "        responses = self.retrieval_grader.batch(batch_inputs) if batch_inputs else []\n",
    "        responses =retry_failed_batches(batch_inputs,responses,self.retrieval_grader)\n",
    "        count=0\n",
    "        # Cập nhật new_graph_state với kết quả đánh giá\n",
    "        for q_idx, question in enumerate(new_graph_state[\"questions\"]):\n",
    "            if q_idx in new_graph_state[\"current_question_index\"]:\n",
    "                for p_idx, proposition in enumerate(question[\"propositions\"]):\n",
    "                    if proposition.get(\"filtered_documents\", []):\n",
    "                        continue\n",
    "                    filtered_docs = []\n",
    "                    for map_idx, (mq, mp, m_ref, md) in enumerate(batch_map):\n",
    "                        if mq == q_idx and mp == p_idx and m_ref == -1:\n",
    "                            response = responses[map_idx][\"binary_score\"].strip() if responses else \"\"\n",
    "                            if response.lower() == \"yes\":\n",
    "                                filtered_docs.append(proposition[\"documents\"][md])\n",
    "                    proposition[\"filtered_documents\"] = filtered_docs\n",
    "                    count+= len(filtered_docs)\n",
    "                for ref_p_idx, proposition in enumerate(question.get(\"ref_propositions\", [])):\n",
    "                    if proposition.get(\"filtered_documents\", []):\n",
    "                        continue\n",
    "                    filtered_docs = []\n",
    "                    for map_idx, (mq, mp, m_ref, md) in enumerate(batch_map):\n",
    "                        if mq == q_idx and mp == -1 and ref_p_idx == mp:\n",
    "                            response = responses[map_idx][\"binary_score\"].strip() if responses else \"\"\n",
    "                            if response.lower() == \"yes\":\n",
    "                                filtered_docs.append(proposition[\"documents\"][md])\n",
    "                    proposition[\"filtered_documents\"] = filtered_docs\n",
    "                    count+= len(filtered_docs)\n",
    "        print(f\"---have filtered and have {count} docs left---\")\n",
    "        return new_graph_state\n",
    "    \n",
    "    def generate(self, state):    #$%^&*\n",
    "        def docs2text(docs):\n",
    "            return '\\n'.join([doc.page_content for doc in docs])\n",
    "            \n",
    "        def question2text(question):\n",
    "            return f\"\"\"第{question.metadata[\"chapter_number\"]}章　{question.metadata[\"chapter_title\"]} \\n 第{question.metadata[\"article_number\"]}条（{question.metadata[\"article_title\"]}）\\n{question.page_content}\"\"\"\n",
    "        \n",
    "        print(\"---GENERATE---\")\n",
    "        new_graph_state = deepcopy(state)\n",
    "        batch_doc_inputs = []\n",
    "        batch_both_inputs = []\n",
    "        batch_doc_indices = []\n",
    "        batch_both_indices = []\n",
    "        print(f\"\"\"current_question_index {new_graph_state[\"current_question_index\"]}\"\"\")\n",
    "\n",
    "        if len(new_graph_state[\"question_to_regen_index\"])!=0 and new_graph_state[\"regen_count\"]<=2:\n",
    "            for q_idx, question in enumerate(new_graph_state[\"questions\"]):\n",
    "                if q_idx in new_graph_state[\"question_to_regen_index\"]:\n",
    "                    if question[\"all_docs\"]!=[]:\n",
    "                        if len(question[\"direct_reference_text\"])!=0:\n",
    "                            batch_both_inputs.append({\n",
    "                                    \"context\": docs2text(question[\"all_docs\"]),\n",
    "                                    \"contract\": question2text(question[\"question_content\"]),\n",
    "                                    \"new_query\": question[\"rewrite_question_content\"]\n",
    "                                })\n",
    "                            batch_both_indices.append(q_idx)\n",
    "                        else:\n",
    "                            batch_doc_inputs.append({\n",
    "                                    \"context\": docs2text(question[\"all_docs\"]),\n",
    "                                    \"contract\": question2text(question[\"question_content\"])\n",
    "                                })\n",
    "                            batch_doc_indices.append(q_idx)\n",
    "                    elif question[\"filtered_web_search_docs\"]!= []:\n",
    "                        if len(question[\"direct_reference_text\"])!=0:\n",
    "                            batch_both_inputs.append({\n",
    "                                    \"context\": docs2text(question[\"filtered_web_search_docs\"]),\n",
    "                                    \"contract\": question2text(question[\"question_content\"]),\n",
    "                                    \"new_query\": question[\"rewrite_question_content\"]\n",
    "                                })\n",
    "                            batch_both_indices.append(q_idx)\n",
    "                        else:\n",
    "                            batch_doc_inputs.append({\n",
    "                                    \"context\": docs2text(question[\"filtered_web_search_docs\"]),\n",
    "                                    \"contract\": question2text(question[\"question_content\"])\n",
    "                                })\n",
    "                            batch_doc_indices.append(q_idx)\n",
    "                    else:\n",
    "                        question[\"generation\"] = \"\"\"{\"evaluation\":\"insufficient_information\",\n",
    "                                \"explanation\":\"CANNOT FIND ANY RELEVANT DOCUMENT\"}\"\"\"\n",
    "                        \n",
    "                # if q_idx in new_graph_state[\"question_to_regen_index\"]:\n",
    "                #     doc_list = set()  # Dùng set để loại bỏ tài liệu trùng lặp\n",
    "                #     for p_idx, proposition in enumerate(question[\"propositions\"]):\n",
    "                #         if proposition[\"filtered_documents\"]:\n",
    "                #             doc_list.update(str(proposition[\"filtered_documents\"]))\n",
    "                    \n",
    "                #     if len(question[\"direct_reference_text\"])!=0:\n",
    "                #         # print(\"OH NOOOOOOOOOOOOOOO\")\n",
    "                #         for ref_p_idx, proposition in enumerate(question[\"ref_propositions\"]):\n",
    "                #             if proposition[\"filtered_documents\"]:\n",
    "                #                 doc_list.update(str(proposition[\"filtered_documents\"]))\n",
    "                #         doc_list=[Document(doc) for doc in doc_list]\n",
    "                #         if doc_list:\n",
    "                #             batch_both_inputs.append({\n",
    "                #                 \"context\": docs2text(doc_list),\n",
    "                #                 \"contract\": question2text(question[\"question_content\"]),\n",
    "                #                 \"new_query\": question[\"rewrite_question_content\"]\n",
    "                #             })\n",
    "                #             batch_both_indices.append(q_idx)\n",
    "                #         else:\n",
    "                #             question[\"generation\"] = \"\"\"{\"evaluation\":\"insufficient_information\",\n",
    "                #             \"explanation\":\"CANNOT FIND ANY RELEVANT DOCUMENT\"}\"\"\"\n",
    "                #     else:\n",
    "                #         doc_list=[Document(doc) for doc in doc_list]\n",
    "                #         if doc_list:\n",
    "                #             batch_doc_inputs.append({\n",
    "                #                 \"context\": docs2text(doc_list),\n",
    "                #                 \"contract\": question2text(question[\"question_content\"])\n",
    "                #             })\n",
    "                #             batch_doc_indices.append(q_idx)\n",
    "                #         else:\n",
    "                #             question[\"generation\"] = \"\"\"{\"evaluation\":\"insufficient_information\",\n",
    "                #             \"explanation\":\"CANNOT FIND ANY RELEVANT DOCUMENT\"}\"\"\"\n",
    "            # new_graph_state[\"question_to_regen_index\"]=[]\n",
    "            new_graph_state[\"regen_count\"]+=1\n",
    "        else:\n",
    "            # for q_idx, question in enumerate(new_graph_state[\"questions\"]):\n",
    "            #     if q_idx in new_graph_state[\"current_question_index\"]:\n",
    "            #         doc_list = set()  # Dùng set để loại bỏ tài liệu trùng lặp\n",
    "            #         for p_idx, proposition in enumerate(question[\"propositions\"]):\n",
    "            #             if proposition[\"filtered_documents\"]:\n",
    "            #                 doc_list.update(str(proposition[\"filtered_documents\"]))\n",
    "                    \n",
    "            #         if len(question[\"direct_reference_text\"])!=0:\n",
    "            #             # print(\"OH NOOOOOOOOOOOOOOO\")\n",
    "            #             for ref_p_idx, proposition in enumerate(question[\"ref_propositions\"]):\n",
    "            #                 if proposition[\"filtered_documents\"]:\n",
    "            #                     doc_list.update(str(proposition[\"filtered_documents\"]))\n",
    "            #             doc_list=[Document(doc) for doc in doc_list]\n",
    "            #             if doc_list:\n",
    "            #                 batch_both_inputs.append({\n",
    "            #                     \"context\": docs2text(doc_list),\n",
    "            #                     \"contract\": question2text(question[\"question_content\"]),\n",
    "            #                     \"new_query\": question[\"rewrite_question_content\"]\n",
    "            #                 })\n",
    "            #                 batch_both_indices.append(q_idx)\n",
    "            #             else:\n",
    "            #                 question[\"generation\"] = \"\"\"{\"evaluation\":\"insufficient_information\",\n",
    "            #                 \"explanation\":\"CANNOT FIND ANY RELEVANT DOCUMENT\"}\"\"\"\n",
    "            #         else:\n",
    "            #             doc_list=[Document(doc) for doc in doc_list]\n",
    "            #             if doc_list:\n",
    "            #                 batch_doc_inputs.append({\n",
    "            #                     \"context\": docs2text(doc_list),\n",
    "            #                     \"contract\": question2text(question[\"question_content\"])\n",
    "            #                 })\n",
    "            #                 batch_doc_indices.append(q_idx)\n",
    "            #             else:\n",
    "            #                 question[\"generation\"] = \"\"\"{\"evaluation\":\"insufficient_information\",\n",
    "            #                 \"explanation\":\"CANNOT FIND ANY RELEVANT DOCUMENT\"}\"\"\"\n",
    "            for q_idx, question in enumerate(new_graph_state[\"questions\"]):\n",
    "                if q_idx in new_graph_state[\"current_question_index\"]:\n",
    "                    if question[\"all_docs\"]!=[]:\n",
    "                        if len(question[\"direct_reference_text\"])!=0:\n",
    "                            batch_both_inputs.append({\n",
    "                                    \"context\": docs2text(question[\"all_docs\"]),\n",
    "                                    \"contract\": question2text(question[\"question_content\"]),\n",
    "                                    \"new_query\": question[\"rewrite_question_content\"]\n",
    "                                })\n",
    "                            batch_both_indices.append(q_idx)\n",
    "                        else:\n",
    "                            batch_doc_inputs.append({\n",
    "                                    \"context\": docs2text(question[\"all_docs\"]),\n",
    "                                    \"contract\": question2text(question[\"question_content\"])\n",
    "                                })\n",
    "                            batch_doc_indices.append(q_idx)\n",
    "                    elif question[\"filtered_web_search_docs\"]!= []:\n",
    "                        if len(question[\"direct_reference_text\"])!=0:\n",
    "                            batch_both_inputs.append({\n",
    "                                    \"context\": docs2text(question[\"filtered_web_search_docs\"]),\n",
    "                                    \"contract\": question2text(question[\"question_content\"]),\n",
    "                                    \"new_query\": question[\"rewrite_question_content\"]\n",
    "                                })\n",
    "                            batch_both_indices.append(q_idx)\n",
    "                        else:\n",
    "                            batch_doc_inputs.append({\n",
    "                                    \"context\": docs2text(question[\"filtered_web_search_docs\"]),\n",
    "                                    \"contract\": question2text(question[\"question_content\"])\n",
    "                                })\n",
    "                            batch_doc_indices.append(q_idx)\n",
    "                    else:\n",
    "                        question[\"generation\"] = \"\"\"{\"evaluation\":\"insufficient_information\",\n",
    "                                \"explanation\":\"CANNOT FIND ANY RELEVANT DOCUMENT\"}\"\"\"\n",
    "                \n",
    "        print(\"start both\")\n",
    "        both_responses = self.rag_chain_both.batch(batch_both_inputs)\n",
    "        both_responses = retry_failed_batches(batch_both_inputs, both_responses, self.rag_chain_both)\n",
    "        print(\"end both\")\n",
    "        print(\"start doc\")\n",
    "        \n",
    "        doc_responses = self.rag_chain_doc.batch(batch_doc_inputs)\n",
    "        doc_responses = retry_failed_batches(batch_doc_inputs, doc_responses, self.rag_chain_doc)\n",
    "        print(\"end doc\")\n",
    "        \n",
    "        for idx, response in enumerate(both_responses):\n",
    "            q_idx = batch_both_indices[idx]\n",
    "            new_graph_state[\"questions\"][q_idx][\"generation\"] = response\n",
    "        \n",
    "        for idx, response in enumerate(doc_responses):\n",
    "            q_idx = batch_doc_indices[idx]\n",
    "            new_graph_state[\"questions\"][q_idx][\"generation\"] = response\n",
    "        return new_graph_state\n",
    "    \n",
    "    def query_transformation(self, state):\n",
    "        print(\"---QUERY TRANSFORMATION---\")\n",
    "        # print(f\"graph_state {state}\")\n",
    "        propositions_to_recheck = []\n",
    "        count = 0\n",
    "        # 1. Chuẩn bị input cho query transformation\n",
    "        query_transformation_input = [\n",
    "            {\"query\": prop_data[\"original_proposition\"][\"proposition_content\"]}\n",
    "            for prop_data in state[\"propositions_to_rewrite\"]\n",
    "        ]\n",
    "\n",
    "        # 2. Gọi query transformation\n",
    "        transformed_queries = self.query_transformation_generator.batch(query_transformation_input)\n",
    "        transformed_queries =retry_failed_batches(query_transformation_input,transformed_queries,self.query_transformation_generator)\n",
    "        print(\"---trieve---\")\n",
    "        # 3. Xử lý retrieval từng query một\n",
    "        prop_documents = {\n",
    "            prop_data[\"original_proposition\"][\"proposition_content\"]: {}\n",
    "            for prop_data in state[\"propositions_to_rewrite\"]\n",
    "        }\n",
    "        \n",
    "        for i, transformed_query in enumerate(transformed_queries):\n",
    "            original_prop = query_transformation_input[i][\"query\"]\n",
    "            queries = (\n",
    "                transformed_query[\"paraphrased_queries\"]\n",
    "                + transformed_query[\"detailed_queries\"]\n",
    "                + transformed_query[\"generalized_queries\"]\n",
    "                + transformed_query[\"expanded_queries\"]\n",
    "            )\n",
    "            for query in queries:\n",
    "                retrieved_docs = self.retriever.invoke(query)  # Chạy tuần tự từng query\n",
    "\n",
    "                for doc in retrieved_docs:\n",
    "                    # Dùng doc.page_content làm key, và lưu đối tượng document đầy đủ\n",
    "                    prop_documents[original_prop][doc.page_content] = doc\n",
    "\n",
    "        # 4. Cập nhật lại graph_state với các documents mới\n",
    "        updated_state = deepcopy(state)\n",
    "        old_docs_keys = set()\n",
    "        for prop_data in state[\"propositions_to_rewrite\"]:\n",
    "            for d in prop_data[\"original_proposition\"].get('documents', []):\n",
    "                old_docs_keys.add(d.page_content)\n",
    "\n",
    "        for prop_data in updated_state[\"propositions_to_rewrite\"]:\n",
    "            q_idx = prop_data[\"question_index\"]\n",
    "            p_idx = prop_data[\"proposition_index\"]\n",
    "            ref_p_idx = prop_data[\"ref_propositions_index\"]\n",
    "            proposition_text = prop_data[\"original_proposition\"][\"proposition_content\"]\n",
    "\n",
    "            new_docs_dict = {\n",
    "                key: doc for key, doc in prop_documents[proposition_text].items()\n",
    "                if key not in old_docs_keys\n",
    "            }\n",
    "            new_documents = list(new_docs_dict.values())\n",
    "            count+=len(new_documents)\n",
    "            # print(\"len new docs\",len(new_documents))\n",
    "            if new_documents:\n",
    "                if p_idx!=-1:\n",
    "                    updated_state[\"questions\"][q_idx][\"propositions\"][p_idx][\"documents\"] = new_documents\n",
    "                else:\n",
    "                    updated_state[\"questions\"][q_idx][\"ref_propositions\"][ref_p_idx][\"documents\"] = new_documents\n",
    "                # print(updated_state[\"questions\"][q_idx][\"propositions\"][p_idx])\n",
    "            else:\n",
    "                if p_idx!=-1:\n",
    "                    propositions_to_recheck.append(updated_state[\"questions\"][q_idx][\"propositions\"][p_idx])\n",
    "                else:\n",
    "                    propositions_to_recheck.append(updated_state[\"questions\"][q_idx][\"ref_propositions\"][ref_p_idx])\n",
    "        if not updated_state.get(\"rewrite_count\", 0):  # Mặc định là 0 nếu không có\n",
    "            updated_state[\"rewrite_count\"] = 1\n",
    "        else:\n",
    "            updated_state[\"rewrite_count\"] += 1\n",
    "        updated_state[\"propositions_to_rewrite\"]=[]\n",
    "        print(f\"---new docs num: {count}\")\n",
    "        return updated_state\n",
    "\n",
    "    def check_empty_propositions(self, state):\n",
    "        print(\"---CHECK EMPTY PROPOSITIONS---\")\n",
    "        new_graph_state = deepcopy(state)  # Tạo bản sao sâu để tránh sửa đổi trực tiếp vào state gốc\n",
    "\n",
    "        # new_graph_state = {\"questions\": [],\"propositions_to_rewrite\":[]}\n",
    "        propositions_to_rewrite = []  # Danh sách propositions cần rewrite\n",
    "        count = 0\n",
    "        \n",
    "        for q_idx, question in enumerate(state[\"questions\"]):\n",
    "            # new_propositions = []\n",
    "            if q_idx in new_graph_state[\"current_question_index\"]:\n",
    "                for p_idx, proposition in enumerate(question[\"propositions\"]):\n",
    "                    if not proposition[\"filtered_documents\"]:  # Nếu danh sách documents rỗng\n",
    "                        count+=1\n",
    "                        propositions_to_rewrite.append({\n",
    "                            \"question_index\": q_idx,  # Lưu vị trí question\n",
    "                            \"proposition_index\": p_idx,  # Lưu vị trí proposition\n",
    "                            \"ref_propositions_index\": -1,  # Lưu vị trí proposition\n",
    "                            \"original_proposition\": proposition,  # Lưu thông tin gốc\n",
    "                        })\n",
    "                for ref_p_idx, proposition in enumerate(question[\"ref_propositions\"]):\n",
    "                    if not proposition[\"filtered_documents\"]:  # Nếu danh sách documents rỗng\n",
    "                        count+=1\n",
    "                        propositions_to_rewrite.append({\n",
    "                            \"question_index\": q_idx,  # Lưu vị trí question\n",
    "                            \"proposition_index\": -1,  # Lưu vị trí proposition\n",
    "                            \"ref_propositions_index\": ref_p_idx,  # Lưu vị trí proposition\n",
    "                            \"original_proposition\": proposition,  # Lưu thông tin gốc\n",
    "                        })        \n",
    "                    # else:\n",
    "                    # new_propositions.append(proposition)  # Giữ lại propositions hợp lệ\n",
    "                \n",
    "                # new_graph_state[\"questions\"].append({\n",
    "                #     \"question_content\": question[\"question_content\"],\n",
    "                #     \"propositions\": new_propositions\n",
    "                # })\n",
    "                # new_graph_state[\"propositions_to_rewrite\"]=propositions_to_rewrite\n",
    "        print(f\"---{count}--- propositions that can not find documents\")\n",
    "        new_graph_state[\"propositions_to_rewrite\"]=propositions_to_rewrite\n",
    "        return new_graph_state\n",
    "    \n",
    "    def should_create_proposition(self, graph_state):\n",
    "        \"\"\"Kiểm tra xem có cần chạy create_proposition không.\"\"\"\n",
    "        if len(graph_state[\"current_question_index\"]) != 0:  # True nếu có câu hỏi có thể chạy\n",
    "            print(\"STILL HAVE INDEPENDENT CHUNK\")\n",
    "            return \"STILL HAVE INDEPENDENT CHUNK\"\n",
    "        else:\n",
    "            print(\"NO INDEPENDENT CHUNK\")\n",
    "            return \"NO INDEPENDENT CHUNK\"\n",
    "\n",
    "    def web_search(self, state):   ####\n",
    "        print(\"---WEB SEARCH---\")\n",
    "        new_graph_state = deepcopy(state)\n",
    "        input_batch = []\n",
    "        batch_indices=[]\n",
    "        # for q_idx,question in enumerate(new_graph_state[\"questions\"]):\n",
    "        #     if q_idx in new_graph_state[\"current_question_index\"]:\n",
    "        #         input_batch.append({\"query\": question[\"question_content\"].page_content})\n",
    "        #         batch_indices.append(q_idx)\n",
    "        # # Web search\n",
    "        # responses = web_search_tool.batch(input_batch)\n",
    "        # for idx, response in enumerate(responses):\n",
    "        #     print(f\"\"\"len(response) {len(response)}\"\"\")\n",
    "        #     print(f\"response {response}\")\n",
    "        #     q_idx = batch_indices[idx]\n",
    "        #     for d in response:\n",
    "        #         new_graph_state[\"questions\"][q_idx][\"web_search_docs\"].append(Document(page_content=d[\"content\"]))\n",
    "        #     text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=1024, chunk_overlap=30)\n",
    "        #     new_graph_state[\"questions\"][q_idx][\"web_search_docs\"] = text_splitter.split_documents(new_graph_state[\"questions\"][q_idx][\"web_search_docs\"])\n",
    "        \n",
    "        search_count=0\n",
    "        for q_idx, question in enumerate(new_graph_state[\"questions\"]):\n",
    "                if q_idx in new_graph_state[\"current_question_index\"]:\n",
    "                    # if question[\"all_docs\"]==[]:\n",
    "                    #     pass\n",
    "                    doc_list = set()  # Dùng set để loại bỏ tài liệu trùng lặp\n",
    "                    for p_idx, proposition in enumerate(question[\"propositions\"]):\n",
    "                        if proposition[\"filtered_documents\"]:\n",
    "                            doc_list.update(str(proposition[\"filtered_documents\"]))\n",
    "                    \n",
    "                    if len(question[\"direct_reference_text\"])!=0:\n",
    "                        # print(\"OH NOOOOOOOOOOOOOOO\")\n",
    "                        for ref_p_idx, proposition in enumerate(question[\"ref_propositions\"]):\n",
    "                            if proposition[\"filtered_documents\"]:\n",
    "                                doc_list.update(str(proposition[\"filtered_documents\"]))\n",
    "                    doc_list=[Document(doc) for doc in doc_list]\n",
    "                    if doc_list:\n",
    "                        question[\"all_docs\"]=doc_list\n",
    "                    else:\n",
    "                        search_count+=1\n",
    "                        if question.get(\"rewrite_question_content\", \"\")!=\"\":\n",
    "                            input_batch.append({\"query\": question[\"rewrite_question_content\"][:100]})\n",
    "                        else:\n",
    "                            input_batch.append({\"query\": question[\"question_content\"].page_content[:100]})\n",
    "                        batch_indices.append(q_idx)\n",
    "        responses = self.web_search_tool.batch(input_batch)\n",
    "        print(f\"websearch input {input_batch}\")\n",
    "        for idx, response in enumerate(responses):\n",
    "            # print(f\"\"\"len(response) {len(response)}\"\"\")\n",
    "            # print(f\"response {response}\")\n",
    "            q_idx = batch_indices[idx]\n",
    "            for d in response:\n",
    "                new_graph_state[\"questions\"][q_idx][\"web_search_docs\"].append(Document(page_content=d[\"content\"]))\n",
    "            text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=512, chunk_overlap=30)\n",
    "            new_graph_state[\"questions\"][q_idx][\"web_search_docs\"] = text_splitter.split_documents(new_graph_state[\"questions\"][q_idx][\"web_search_docs\"])\n",
    "        print(f\"HAVE SEARCH DOCS FOR: {search_count} CHUNKS\")\n",
    "        return new_graph_state\n",
    "\n",
    "    def grade_web_search_docs(self, state):\n",
    "        def question2text(question):\n",
    "            return f\"\"\"第{question.metadata[\"chapter_number\"]}章　{question.metadata[\"chapter_title\"]} \\n 第{question.metadata[\"article_number\"]}条（{question.metadata[\"article_title\"]}）\\n{question.page_content}\"\"\"\n",
    "    \n",
    "        print(\"---GRADE WEB DOCUMENTS---\")\n",
    "        \n",
    "        new_graph_state = deepcopy(state)  # Tạo bản sao để tránh sửa đổi trực tiếp vào state gốc\n",
    "        batch_inputs = []\n",
    "        batch_map = []  # Mỗi phần tử là (q_idx, doc_idx)\n",
    "\n",
    "        for q_idx, question in enumerate(new_graph_state[\"questions\"]):\n",
    "            if q_idx in new_graph_state[\"current_question_index\"]:\n",
    "                contract= \"\"\n",
    "                if question.get(\"rewrite_question_content\", \"\")!=\"\":\n",
    "                    contract=question[\"rewrite_question_content\"]\n",
    "                else: \n",
    "                    contract= question2text(question[\"question_content\"])\n",
    "                for doc_idx, doc in enumerate(question[\"web_search_docs\"]):  # Sửa lỗi unpacking\n",
    "                    batch_inputs.append({\n",
    "                        \"context\": doc.page_content,\n",
    "                        \"contract\": contract\n",
    "                    })\n",
    "                    batch_map.append((q_idx, doc_idx))\n",
    "        print(f\"len(batch_map) {len(batch_map)}\")\n",
    "        # Xử lý batch nếu có dữ liệu\n",
    "        responses = self.retrieval_grader.batch(batch_inputs) if batch_inputs else []\n",
    "        responses = retry_failed_batches(batch_inputs, responses, self.retrieval_grader)\n",
    "\n",
    "        count = 0  # Đếm số tài liệu hợp lệ còn lại\n",
    "\n",
    "        # Cập nhật new_graph_state với kết quả đánh giá\n",
    "        for map_idx, (q_idx, doc_idx) in enumerate(batch_map):\n",
    "            response = responses[map_idx][\"binary_score\"].strip().lower() if responses else \"\"\n",
    "            if response == \"yes\":\n",
    "                # Chỉ giữ lại tài liệu được chấp nhận\n",
    "                new_graph_state[\"questions\"][q_idx][\"filtered_web_search_docs\"].append(new_graph_state[\"questions\"][q_idx][\"web_search_docs\"][doc_idx])\n",
    "                count += 1  # Tăng biến đếm số tài liệu hợp lệ\n",
    "\n",
    "        print(f\"---Have filtered and have {count} docs left---\")\n",
    "        return new_graph_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAGPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trung/.conda/envs/rag_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-14 07:04:39,647\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-14 07:04:40 __init__.py:207] Automatically detected platform cuda.\n",
      "INFO 03-14 07:04:46 config.py:549] This model supports multiple tasks: {'embed', 'reward', 'generate', 'score', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 03-14 07:04:47 gptq_marlin.py:147] Detected that the model can run with gptq_marlin, however you specified quantization=gptq explicitly, so forcing gptq. Use quantization=gptq_marlin for faster inference\n",
      "WARNING 03-14 07:04:47 config.py:628] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 03-14 07:04:47 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/home/trung/RAG_ADVANCED/Qwen2.5-14B-Instruct-GPTQ-Int4', speculative_config=None, tokenizer='/home/trung/RAG_ADVANCED/Qwen2.5-14B-Instruct-GPTQ-Int4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/trung/RAG_ADVANCED/Qwen2.5-14B-Instruct-GPTQ-Int4, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 03-14 07:04:48 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 03-14 07:04:48 model_runner.py:1110] Starting to load model /home/trung/RAG_ADVANCED/Qwen2.5-14B-Instruct-GPTQ-Int4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:01<00:02,  1.19s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:02<00:01,  1.29s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:03<00:00,  1.02s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:03<00:00,  1.08s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-14 07:04:52 model_runner.py:1115] Loading model weights took 9.3794 GB\n",
      "INFO 03-14 07:04:54 worker.py:267] Memory profiling takes 1.37 seconds\n",
      "INFO 03-14 07:04:54 worker.py:267] the current vLLM instance can use total_gpu_memory (23.65GiB) x gpu_memory_utilization (0.50) = 11.82GiB\n",
      "INFO 03-14 07:04:54 worker.py:267] model weights take 9.38GiB; non_torch_memory takes 0.08GiB; PyTorch activation peak memory takes 1.43GiB; the rest of the memory reserved for KV Cache is 0.94GiB.\n",
      "INFO 03-14 07:04:54 executor_base.py:111] # cuda blocks: 320, # CPU blocks: 1365\n",
      "INFO 03-14 07:04:54 executor_base.py:116] Maximum concurrency for 4096 tokens per request: 1.25x\n",
      "INFO 03-14 07:04:58 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-14 07:05:20 model_runner.py:1562] Graph capturing finished in 23 secs, took 1.91 GiB\n",
      "INFO 03-14 07:05:20 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 27.99 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique documents: 4609\n",
      "TRUNGGGG CHUNK 2 4945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2478390/529331294.py:43: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_function = HuggingFaceEmbeddings(model_name=\"intfloat/multilingual-e5-large\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRUNGGGGG    CHUNK 3 4945\n",
      "Vector database created.\n",
      "type(data) <class 'str'>\n",
      "Docs have  1 chapters\n",
      "graph_state {'questions': [{'question_content': Document(metadata={'chapter_title': '', 'chapter_number': '', 'article_title': '目的', 'article_number': '1', 'clause_number': '1', 'clause_title': 'この規程は、株式会社●●●●の賃金規程第21条に基づき、会社の労働者の退職金に関する事項を定めたものである。', 'sub_clause_number': '', 'sub_clause_content': ''}, page_content='この規程は、株式会社●●●●の賃金規程第21条に基づき、会社の労働者の退職金に関する事項を定めたものである。'), 'propositions': [], 'ref_propositions': [], 'all_docs': [], 'web_search_docs': [], 'filtered_web_search_docs': []}], 'root_text': {'Chapter': {'': {'chapter': '\\n', 'article': '-1', 'clause': '-1', 'sub_clause': '-1', 'text': '1\\u3000この規程は、株式会社●●●●の賃金規程第21条に基づき、会社の労働者の退職金に関する事項を定めたものである。'}}, 'Article': {'1': {'chapter': '\\n', 'article': '第1条\\u3000目的\\n', 'clause': '-1', 'sub_clause': '-1', 'text': '1\\u3000この規程は、株式会社●●●●の賃金規程第21条に基づき、会社の労働者の退職金に関する事項を定めたものである。'}}, 'clause': {('1', '1'): {'chapter': '\\n', 'article': '第1条\\u3000目的\\n', 'clause': '1. ', 'sub_clause': '-1', 'text': 'この規程は、株式会社●●●●の賃金規程第2条に基づき、会社の労働者の退職金に関する事項を定めたものである。'}}}}\n",
      "---DIRECT REFERENCE---\n",
      "entry+++++++ {'document': 'この規程は、株式会社●●●●の賃金規程第21条に基づき、会社の労働者の退職金に関する事項を定めたものである。', 'metadata': 'current chapter: \"\", current article: 1, current clause: 1'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.22s/it, est. speed input: 403.72 toks/s, output: 75.64 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "responses+++++++ [{'has_reference': 'yes', 'is_extractable': 'yes', 'references': [{'text': '株式会社●●●●の賃金規程第21条', 'resolved': {'Chapter': -1, 'Article': -1, 'clause': 21, 'sub_clause': -1}}]}]\n",
      "---GRADE REFERENCE---\n",
      "---GRADE REFERENCE---\n",
      "question_have_ref_count 0\n",
      "NOTHING TO GRADE\n",
      "---REWRITE QUESTION CONTENT---\n",
      "NO QUESTIONS TO REWRITE\n",
      "--- LOOPBACK NODE ---\n",
      "direct count 1\n",
      "ref count 0\n",
      "STILL HAVE INDEPENDENT CHUNK\n",
      "---CREATE PROPOSITIONS---\n",
      "len(graph_state[\"current_question_index\"]) 1\n",
      "len(direct ) 1\n",
      "len(ref ) 0\n",
      "count 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.23it/s, est. speed input: 484.49 toks/s, output: 75.20 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n",
      "---have retrieved for 2 propositions---\n",
      "---GRADE DOCUMENTS---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 20/20 [00:01<00:00, 14.94it/s, est. speed input: 5565.32 toks/s, output: 149.42 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---have filtered and have 0 docs left---\n",
      "---CHECK EMPTY PROPOSITIONS---\n",
      "---2--- propositions that can not find documents\n",
      "---QUERY TRANSFORMATION---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:03<00:00,  1.78s/it, est. speed input: 155.50 toks/s, output: 143.95 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---trieve---\n",
      "---new docs num: 14\n",
      "---GRADE DOCUMENTS---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 14/14 [00:01<00:00, 12.73it/s, est. speed input: 5174.05 toks/s, output: 127.26 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---have filtered and have 0 docs left---\n",
      "---CHECK EMPTY PROPOSITIONS---\n",
      "---2--- propositions that can not find documents\n",
      "---WEB SEARCH---\n",
      "websearch input [{'query': 'この規程は、株式会社●●●●の賃金規程第21条に基づき、会社の労働者の退職金に関する事項を定めたものである。'}]\n",
      "HAVE SEARCH DOCS FOR: 1 CHUNKS\n",
      "---GRADE WEB DOCUMENTS---\n",
      "len(batch_map) 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  4.77it/s, est. speed input: 1541.32 toks/s, output: 48.01 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Have filtered and have 1 docs left---\n",
      "---GENERATE---\n",
      "current_question_index [0]\n",
      "start both\n",
      "end both\n",
      "start doc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.35s/it, est. speed input: 245.25 toks/s, output: 80.26 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end doc\n",
      "---HALLUCINATION GRADE---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.14s/it, est. speed input: 442.63 toks/s, output: 78.16 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---GRADE ANSWER---\n",
      "---GENERATE---\n",
      "current_question_index [0]\n",
      "start both\n",
      "end both\n",
      "start doc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.31s/it, est. speed input: 252.43 toks/s, output: 80.32 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end doc\n",
      "---HALLUCINATION GRADE---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.32s/it, est. speed input: 378.57 toks/s, output: 79.34 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---GRADE ANSWER---\n",
      "---GENERATE---\n",
      "current_question_index [0]\n",
      "start both\n",
      "end both\n",
      "start doc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.27s/it, est. speed input: 261.39 toks/s, output: 80.00 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end doc\n",
      "---HALLUCINATION GRADE---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.03it/s, est. speed input: 511.29 toks/s, output: 77.16 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---GRADE ANSWER---\n",
      "--- LOOPBACK NODE ---\n",
      "direct count 0\n",
      "ref count 0\n",
      "NO INDEPENDENT CHUNK\n",
      "final_state {'questions': [{'question_content': Document(metadata={'chapter_title': '', 'chapter_number': '', 'article_title': '目的', 'article_number': '1', 'clause_number': '1', 'clause_title': 'この規程は、株式会社●●●●の賃金規程第21条に基づき、会社の労働者の退職金に関する事項を定めたものである。', 'sub_clause_number': '', 'sub_clause_content': ''}, page_content='この規程は、株式会社●●●●の賃金規程第21条に基づき、会社の労働者の退職金に関する事項を定めたものである。'), 'propositions': [{'proposition_content': 'この規程は、株式会社●●●●の賃金規程第21条に基づき、会社の労働者の退職金に関する事項を定める。', 'documents': [Document(metadata={'article_name': '金品の返還', 'article_number': '第二十三条', 'chapter_name': '労働契約', 'chapter_number': '第二章', 'division_name': '', 'division_number': '', 'document_source': 'https://laws.e-gov.go.jp/law/322AC0000000049', 'enactment_year': '令和6年5月31日 施行', 'law_name': '労働基準法', 'law_number': '昭和二十二年法律第四十九号'}, page_content='昭和二十二年法律第四十九号\\u3000労働基準法/第二章\\u3000労働契約/第二十三条\\u3000金品の返還/使用者は、労働者の死亡又は退職の場合において、権利者の請求があつた場合においては、七日以内に賃金を支払い、積立金、保証金、貯蓄金その他名称の如何を問わず、労働者の権利に属する金品を返還しなければならない。\\n前項の賃金又は金品に関して争がある場合においては、使用者は、異議のない部分を、同項の期間中に支払い、又は返還しなければならない。'), Document(metadata={'article_name': '第二十二条の五', 'article_number': '第二十二条の五', 'chapter_name': '保険給付', 'chapter_number': '第三章', 'division_name': '通勤災害に関する保険給付', 'division_number': '第三節', 'document_source': 'https://laws.e-gov.go.jp/law/322AC0000000050', 'enactment_year': '令和4年6月17日 施行', 'law_name': '労働者災害補償保険法', 'law_number': '昭和二十二年法律第五十号'}, page_content='昭和二十二年法律第五十号\\u3000労働者災害補償保険法/第三章\\u3000保険給付/第三節\\u3000通勤災害に関する保険給付/第二十二条の五/葬祭給付は、労働者が通勤により死亡した場合に、葬祭を行なう者に対し、その請求に基づいて行なう。\\n第十七条の規定は、葬祭給付について準用する。'), Document(metadata={'article_name': '第十七条', 'article_number': '第十七条', 'chapter_name': '保険給付', 'chapter_number': '第三章', 'division_name': '業務災害に関する保険給付', 'division_number': '第二節', 'document_source': 'https://laws.e-gov.go.jp/law/322AC0000000050', 'enactment_year': '令和4年6月17日 施行', 'law_name': '労働者災害補償保険法', 'law_number': '昭和二十二年法律第五十号'}, page_content='昭和二十二年法律第五十号\\u3000労働者災害補償保険法/第三章\\u3000保険給付/第二節\\u3000業務災害に関する保険給付/第十七条/葬祭料は、通常葬祭に要する費用を考慮して厚生労働大臣が定める金額とする。'), Document(metadata={'article_name': '出来高払制の保障給', 'article_number': '第二十七条', 'chapter_name': '賃金', 'chapter_number': '第三章', 'division_name': '', 'division_number': '', 'document_source': 'https://laws.e-gov.go.jp/law/322AC0000000049', 'enactment_year': '令和6年5月31日 施行', 'law_name': '労働基準法', 'law_number': '昭和二十二年法律第四十九号'}, page_content='昭和二十二年法律第四十九号\\u3000労働基準法/第三章\\u3000賃金/第二十七条\\u3000出来高払制の保障給/出来高払制その他の請負制で使用する労働者については、使用者は、労働時間に応じ一定額の賃金の保障をしなければならない。'), Document(metadata={'article_name': '葬祭料', 'article_number': '第八十条', 'chapter_name': '災害補償', 'chapter_number': '第八章', 'division_name': '', 'division_number': '', 'document_source': 'https://laws.e-gov.go.jp/law/322AC0000000049', 'enactment_year': '令和6年5月31日 施行', 'law_name': '労働基準法', 'law_number': '昭和二十二年法律第四十九号'}, page_content='昭和二十二年法律第四十九号\\u3000労働基準法/第八章\\u3000災害補償/第八十条\\u3000葬祭料/労働者が業務上死亡した場合においては、使用者は、葬祭を行う者に対して、平均賃金の六十日分の葬祭料を支払わなければならない。'), Document(metadata={'article_name': '第二十条の七', 'article_number': '第二十条の七', 'chapter_name': '保険給付', 'chapter_number': '第三章', 'division_name': '複数業務要因災害に関する保険給付', 'division_number': '第二節の二', 'document_source': 'https://laws.e-gov.go.jp/law/322AC0000000050', 'enactment_year': '令和4年6月17日 施行', 'law_name': '労働者災害補償保険法', 'law_number': '昭和二十二年法律第五十号'}, page_content='昭和二十二年法律第五十号\\u3000労働者災害補償保険法/第三章\\u3000保険給付/第二節の二\\u3000複数業務要因災害に関する保険給付/第二十条の七/複数事業労働者葬祭給付は、複数事業労働者がその従事する二以上の事業の業務を要因として死亡した場合に、葬祭を行う者に対し、その請求に基づいて行う。\\n第十七条の規定は、複数事業労働者葬祭給付について準用する。'), Document(metadata={'article_name': '定義', 'article_number': '第二条', 'chapter_name': '総則', 'chapter_number': '第一章', 'division_name': '', 'division_number': '', 'document_source': 'https://laws.e-gov.go.jp/law/351AC0000000034', 'enactment_year': '令和4年6月17日 施行', 'law_name': '賃金の支払の確保等に関する法律', 'law_number': '昭和五十一年法律第三十四号'}, page_content='昭和五十一年法律第三十四号\\u3000賃金の支払の確保等に関する法律/第一章\\u3000総則/第二条\\u3000定義/この法律において「賃金」とは、労働基準法（昭和二十二年法律第四十九号）第十一条に規定する賃金をいう。\\nこの法律において「労働者」とは、労働基準法第九条に規定する労働者（同居の親族のみを使用する事業又は事務所に使用される者及び家事使用人を除く。）をいう。'), Document(metadata={'article_name': '特定業種退職金共済規程', 'article_number': '第七十一条', 'chapter_name': '独立行政法人勤労者退職金共済機構', 'chapter_number': '第六章', 'division_name': '業務等', 'division_number': '第五節', 'document_source': 'https://laws.e-gov.go.jp/law/334AC0000000160', 'enactment_year': '令和4年6月17日 施行', 'law_name': '中小企業退職金共済法', 'law_number': '昭和三十四年法律第百六十号'}, page_content='昭和三十四年法律第百六十号\\u3000中小企業退職金共済法/第六章\\u3000独立行政法人勤労者退職金共済機構/第五節\\u3000業務等/第七十一条\\u3000特定業種退職金共済規程/機構は、特定業種退職金共済規程をもつて次に掲げる事項を規定しなければならない。\\n一\\u3000運営委員会に関する事項\\n二\\u3000特定業種退職金共済契約に係る掛金に関する事項\\n特定業種退職金共済規程の変更は、厚生労働大臣の認可を受けなければ、その効力を生じない。'), Document(metadata={'article_name': '退職労働者の賃金に係る遅延利息', 'article_number': '第六条', 'chapter_name': '貯蓄金及び賃金に係る保全措置等', 'chapter_number': '第二章', 'division_name': '', 'division_number': '', 'document_source': 'https://laws.e-gov.go.jp/law/351AC0000000034', 'enactment_year': '令和4年6月17日 施行', 'law_name': '賃金の支払の確保等に関する法律', 'law_number': '昭和五十一年法律第三十四号'}, page_content='昭和五十一年法律第三十四号\\u3000賃金の支払の確保等に関する法律/第二章\\u3000貯蓄金及び賃金に係る保全措置等/第六条\\u3000退職労働者の賃金に係る遅延利息/事業主は、その事業を退職した労働者に係る賃金（退職手当を除く。以下この条において同じ。）の全部又は一部をその退職の日（退職の日後に支払期日が到来する賃金にあつては、当該支払期日。以下この条において同じ。）までに支払わなかつた場合には、当該労働者に対し、当該退職の日の翌日からその支払をする日までの期間について、その日数に応じ、当該退職の日の経過後まだ支払われていない賃金の額に年十四・六パーセントを超えない範囲内で政令で定める率を乗じて得た金額を遅延利息として支払わなければならない。\\n前項の規定は、賃金の支払の遅滞が天災地変その他のやむを得ない事由で厚生労働省令で定めるものによるものである場合には、その事由の存する期間について適用しない。'), Document(metadata={'article_name': '第二十条', 'article_number': '第二十条', 'chapter_name': '罰則', 'chapter_number': '第五章', 'division_name': '', 'division_number': '', 'document_source': 'https://laws.e-gov.go.jp/law/351AC0000000034', 'enactment_year': '令和4年6月17日 施行', 'law_name': '賃金の支払の確保等に関する法律', 'law_number': '昭和五十一年法律第三十四号'}, page_content='昭和五十一年法律第三十四号\\u3000賃金の支払の確保等に関する法律/第五章\\u3000罰則/第二十条/法人の代表者又は法人若しくは人の代理人、使用人その他の従業者が、その法人又は人の業務に関して、第十七条から前条までの違反行為をしたときは、行為者を罰するほか、その法人又は人に対しても、各本条の罰金刑を科する。'), Document(metadata={'article_name': '一般の退職手当', 'article_number': '第二条の四', 'chapter_name': '一般の退職手当', 'chapter_number': '第二章', 'division_name': '', 'division_number': '', 'document_source': 'https://laws.e-gov.go.jp/law/328AC1000000182', 'enactment_year': '令和6年5月17日 施行', 'law_name': '国家公務員退職手当法', 'law_number': '昭和二十八年法律第百八十二号'}, page_content='昭和二十八年法律第百八十二号\\u3000国家公務員退職手当法/第二章\\u3000一般の退職手当/第二条の四\\u3000一般の退職手当/退職した者に対する退職手当の額は、次条から第六条の三までの規定により計算した退職手当の基本額に、第六条の四の規定により計算した退職手当の調整額を加えて得た額とする。')], 'filtered_documents': []}, {'proposition_content': '第章\\u3000 \\n 第1条（目的）\\nこの規程は、株式会社●●●●の賃金規程第21条に基づき、会社の労働者の退職金に関する事項を定めたものである。', 'documents': [Document(metadata={'article_name': 'この法律の目的', 'article_number': '第一条', 'chapter_name': '総則', 'chapter_number': '第一章', 'division_name': '', 'division_number': '', 'document_source': 'https://laws.e-gov.go.jp/law/336AC0000000155', 'enactment_year': '令和4年12月16日 施行', 'law_name': '社会福祉施設職員等退職手当共済法', 'law_number': '昭和三十六年法律第百五十五号'}, page_content='昭和三十六年法律第百五十五号\\u3000社会福祉施設職員等退職手当共済法/第一章\\u3000総則/第一条\\u3000この法律の目的/この法律は、社会福祉施設、特定社会福祉事業及び特定介護保険施設等を経営する社会福祉法人の相互扶助の精神に基づき、社会福祉施設の職員、特定社会福祉事業に従事する職員及び特定介護保険施設等の職員について退職手当共済制度を確立し、もつて社会福祉事業の振興に寄与することを目的とする。'), Document(metadata={'article_name': '目的', 'article_number': '第一条', 'chapter_name': '総則', 'chapter_number': '第一章', 'division_name': '', 'division_number': '', 'document_source': 'https://laws.e-gov.go.jp/law/349AC0000000116', 'enactment_year': '令和6年10月1日 施行', 'law_name': '雇用保険法', 'law_number': '昭和四十九年法律第百十六号'}, page_content='昭和四十九年法律第百十六号\\u3000雇用保険法/第一章\\u3000総則/第一条\\u3000目的/雇用保険は、労働者が失業した場合及び労働者について雇用の継続が困難となる事由が生じた場合に必要な給付を行うほか、労働者が自ら職業に関する教育訓練を受けた場合及び労働者が子を養育するための休業をした場合に必要な給付を行うことにより、労働者の生活及び雇用の安定を図るとともに、求職活動を容易にする等その就職を促進し、あわせて、労働者の職業の安定に資するため、失業の予防、雇用状態の是正及び雇用機会の増大、労働者の能力の開発及び向上その他労働者の福祉の増進を図ることを目的とする。'), Document(metadata={'article_name': '目的', 'article_number': '第一条', 'chapter_name': '総則', 'chapter_number': '第一章', 'division_name': '', 'division_number': '', 'document_source': 'https://laws.e-gov.go.jp/law/346AC0000000068', 'enactment_year': '令和4年10月1日 施行', 'law_name': '高年齢者等の雇用の安定等に関する法律', 'law_number': '昭和四十六年法律第六十八号'}, page_content='昭和四十六年法律第六十八号\\u3000高年齢者等の雇用の安定等に関する法律/第一章\\u3000総則/第一条\\u3000目的/この法律は、定年の引上げ、継続雇用制度の導入等による高年齢者の安定した雇用の確保の促進、高年齢者等の再就職の促進、定年退職者その他の高年齢退職者に対する就業の機会の確保等の措置を総合的に講じ、もつて高年齢者等の職業の安定その他福祉の増進を図るとともに、経済及び社会の発展に寄与することを目的とする。')], 'filtered_documents': []}], 'ref_propositions': [], 'all_docs': [], 'web_search_docs': [Document(metadata={}, page_content='そして、賃金や退職金の決定・計算及び支払方法等につきましては、労働基準法におきまして就業規則上の必要記載事項と定められています。 従いまして、')], 'filtered_web_search_docs': [Document(metadata={}, page_content='そして、賃金や退職金の決定・計算及び支払方法等につきましては、労働基準法におきまして就業規則上の必要記載事項と定められています。 従いまして、')], 'direct_reference': [], 'direct_reference_index': [], 'direct_reference_text': [], 'merged_reference_text': '', 'generation': {'evaluation': 'insufficient_information', 'explanation': 'この契約条項は、退職金に関する事項を定めたものであると述べていますが、具体的な退職金の決定、計算、支払方法等の詳細が記載されていません。そのため、労働基準法における就業規則上の必要記載事項が満たされているかどうかを判断するのに十分な情報がありません。'}}], 'processed_question_index': [0], 'current_question_index': [], 'propositions_to_rewrite': [{'question_index': 0, 'proposition_index': 0, 'ref_propositions_index': -1, 'original_proposition': {'proposition_content': 'この規程は、株式会社●●●●の賃金規程第21条に基づき、会社の労働者の退職金に関する事項を定める。', 'documents': [Document(metadata={'article_name': '金品の返還', 'article_number': '第二十三条', 'chapter_name': '労働契約', 'chapter_number': '第二章', 'division_name': '', 'division_number': '', 'document_source': 'https://laws.e-gov.go.jp/law/322AC0000000049', 'enactment_year': '令和6年5月31日 施行', 'law_name': '労働基準法', 'law_number': '昭和二十二年法律第四十九号'}, page_content='昭和二十二年法律第四十九号\\u3000労働基準法/第二章\\u3000労働契約/第二十三条\\u3000金品の返還/使用者は、労働者の死亡又は退職の場合において、権利者の請求があつた場合においては、七日以内に賃金を支払い、積立金、保証金、貯蓄金その他名称の如何を問わず、労働者の権利に属する金品を返還しなければならない。\\n前項の賃金又は金品に関して争がある場合においては、使用者は、異議のない部分を、同項の期間中に支払い、又は返還しなければならない。'), Document(metadata={'article_name': '第二十二条の五', 'article_number': '第二十二条の五', 'chapter_name': '保険給付', 'chapter_number': '第三章', 'division_name': '通勤災害に関する保険給付', 'division_number': '第三節', 'document_source': 'https://laws.e-gov.go.jp/law/322AC0000000050', 'enactment_year': '令和4年6月17日 施行', 'law_name': '労働者災害補償保険法', 'law_number': '昭和二十二年法律第五十号'}, page_content='昭和二十二年法律第五十号\\u3000労働者災害補償保険法/第三章\\u3000保険給付/第三節\\u3000通勤災害に関する保険給付/第二十二条の五/葬祭給付は、労働者が通勤により死亡した場合に、葬祭を行なう者に対し、その請求に基づいて行なう。\\n第十七条の規定は、葬祭給付について準用する。'), Document(metadata={'article_name': '第十七条', 'article_number': '第十七条', 'chapter_name': '保険給付', 'chapter_number': '第三章', 'division_name': '業務災害に関する保険給付', 'division_number': '第二節', 'document_source': 'https://laws.e-gov.go.jp/law/322AC0000000050', 'enactment_year': '令和4年6月17日 施行', 'law_name': '労働者災害補償保険法', 'law_number': '昭和二十二年法律第五十号'}, page_content='昭和二十二年法律第五十号\\u3000労働者災害補償保険法/第三章\\u3000保険給付/第二節\\u3000業務災害に関する保険給付/第十七条/葬祭料は、通常葬祭に要する費用を考慮して厚生労働大臣が定める金額とする。'), Document(metadata={'article_name': '出来高払制の保障給', 'article_number': '第二十七条', 'chapter_name': '賃金', 'chapter_number': '第三章', 'division_name': '', 'division_number': '', 'document_source': 'https://laws.e-gov.go.jp/law/322AC0000000049', 'enactment_year': '令和6年5月31日 施行', 'law_name': '労働基準法', 'law_number': '昭和二十二年法律第四十九号'}, page_content='昭和二十二年法律第四十九号\\u3000労働基準法/第三章\\u3000賃金/第二十七条\\u3000出来高払制の保障給/出来高払制その他の請負制で使用する労働者については、使用者は、労働時間に応じ一定額の賃金の保障をしなければならない。'), Document(metadata={'article_name': '葬祭料', 'article_number': '第八十条', 'chapter_name': '災害補償', 'chapter_number': '第八章', 'division_name': '', 'division_number': '', 'document_source': 'https://laws.e-gov.go.jp/law/322AC0000000049', 'enactment_year': '令和6年5月31日 施行', 'law_name': '労働基準法', 'law_number': '昭和二十二年法律第四十九号'}, page_content='昭和二十二年法律第四十九号\\u3000労働基準法/第八章\\u3000災害補償/第八十条\\u3000葬祭料/労働者が業務上死亡した場合においては、使用者は、葬祭を行う者に対して、平均賃金の六十日分の葬祭料を支払わなければならない。'), Document(metadata={'article_name': '第二十条の七', 'article_number': '第二十条の七', 'chapter_name': '保険給付', 'chapter_number': '第三章', 'division_name': '複数業務要因災害に関する保険給付', 'division_number': '第二節の二', 'document_source': 'https://laws.e-gov.go.jp/law/322AC0000000050', 'enactment_year': '令和4年6月17日 施行', 'law_name': '労働者災害補償保険法', 'law_number': '昭和二十二年法律第五十号'}, page_content='昭和二十二年法律第五十号\\u3000労働者災害補償保険法/第三章\\u3000保険給付/第二節の二\\u3000複数業務要因災害に関する保険給付/第二十条の七/複数事業労働者葬祭給付は、複数事業労働者がその従事する二以上の事業の業務を要因として死亡した場合に、葬祭を行う者に対し、その請求に基づいて行う。\\n第十七条の規定は、複数事業労働者葬祭給付について準用する。'), Document(metadata={'article_name': '定義', 'article_number': '第二条', 'chapter_name': '総則', 'chapter_number': '第一章', 'division_name': '', 'division_number': '', 'document_source': 'https://laws.e-gov.go.jp/law/351AC0000000034', 'enactment_year': '令和4年6月17日 施行', 'law_name': '賃金の支払の確保等に関する法律', 'law_number': '昭和五十一年法律第三十四号'}, page_content='昭和五十一年法律第三十四号\\u3000賃金の支払の確保等に関する法律/第一章\\u3000総則/第二条\\u3000定義/この法律において「賃金」とは、労働基準法（昭和二十二年法律第四十九号）第十一条に規定する賃金をいう。\\nこの法律において「労働者」とは、労働基準法第九条に規定する労働者（同居の親族のみを使用する事業又は事務所に使用される者及び家事使用人を除く。）をいう。'), Document(metadata={'article_name': '特定業種退職金共済規程', 'article_number': '第七十一条', 'chapter_name': '独立行政法人勤労者退職金共済機構', 'chapter_number': '第六章', 'division_name': '業務等', 'division_number': '第五節', 'document_source': 'https://laws.e-gov.go.jp/law/334AC0000000160', 'enactment_year': '令和4年6月17日 施行', 'law_name': '中小企業退職金共済法', 'law_number': '昭和三十四年法律第百六十号'}, page_content='昭和三十四年法律第百六十号\\u3000中小企業退職金共済法/第六章\\u3000独立行政法人勤労者退職金共済機構/第五節\\u3000業務等/第七十一条\\u3000特定業種退職金共済規程/機構は、特定業種退職金共済規程をもつて次に掲げる事項を規定しなければならない。\\n一\\u3000運営委員会に関する事項\\n二\\u3000特定業種退職金共済契約に係る掛金に関する事項\\n特定業種退職金共済規程の変更は、厚生労働大臣の認可を受けなければ、その効力を生じない。'), Document(metadata={'article_name': '退職労働者の賃金に係る遅延利息', 'article_number': '第六条', 'chapter_name': '貯蓄金及び賃金に係る保全措置等', 'chapter_number': '第二章', 'division_name': '', 'division_number': '', 'document_source': 'https://laws.e-gov.go.jp/law/351AC0000000034', 'enactment_year': '令和4年6月17日 施行', 'law_name': '賃金の支払の確保等に関する法律', 'law_number': '昭和五十一年法律第三十四号'}, page_content='昭和五十一年法律第三十四号\\u3000賃金の支払の確保等に関する法律/第二章\\u3000貯蓄金及び賃金に係る保全措置等/第六条\\u3000退職労働者の賃金に係る遅延利息/事業主は、その事業を退職した労働者に係る賃金（退職手当を除く。以下この条において同じ。）の全部又は一部をその退職の日（退職の日後に支払期日が到来する賃金にあつては、当該支払期日。以下この条において同じ。）までに支払わなかつた場合には、当該労働者に対し、当該退職の日の翌日からその支払をする日までの期間について、その日数に応じ、当該退職の日の経過後まだ支払われていない賃金の額に年十四・六パーセントを超えない範囲内で政令で定める率を乗じて得た金額を遅延利息として支払わなければならない。\\n前項の規定は、賃金の支払の遅滞が天災地変その他のやむを得ない事由で厚生労働省令で定めるものによるものである場合には、その事由の存する期間について適用しない。'), Document(metadata={'article_name': '第二十条', 'article_number': '第二十条', 'chapter_name': '罰則', 'chapter_number': '第五章', 'division_name': '', 'division_number': '', 'document_source': 'https://laws.e-gov.go.jp/law/351AC0000000034', 'enactment_year': '令和4年6月17日 施行', 'law_name': '賃金の支払の確保等に関する法律', 'law_number': '昭和五十一年法律第三十四号'}, page_content='昭和五十一年法律第三十四号\\u3000賃金の支払の確保等に関する法律/第五章\\u3000罰則/第二十条/法人の代表者又は法人若しくは人の代理人、使用人その他の従業者が、その法人又は人の業務に関して、第十七条から前条までの違反行為をしたときは、行為者を罰するほか、その法人又は人に対しても、各本条の罰金刑を科する。'), Document(metadata={'article_name': '一般の退職手当', 'article_number': '第二条の四', 'chapter_name': '一般の退職手当', 'chapter_number': '第二章', 'division_name': '', 'division_number': '', 'document_source': 'https://laws.e-gov.go.jp/law/328AC1000000182', 'enactment_year': '令和6年5月17日 施行', 'law_name': '国家公務員退職手当法', 'law_number': '昭和二十八年法律第百八十二号'}, page_content='昭和二十八年法律第百八十二号\\u3000国家公務員退職手当法/第二章\\u3000一般の退職手当/第二条の四\\u3000一般の退職手当/退職した者に対する退職手当の額は、次条から第六条の三までの規定により計算した退職手当の基本額に、第六条の四の規定により計算した退職手当の調整額を加えて得た額とする。')], 'filtered_documents': []}}, {'question_index': 0, 'proposition_index': 1, 'ref_propositions_index': -1, 'original_proposition': {'proposition_content': '第章\\u3000 \\n 第1条（目的）\\nこの規程は、株式会社●●●●の賃金規程第21条に基づき、会社の労働者の退職金に関する事項を定めたものである。', 'documents': [Document(metadata={'article_name': 'この法律の目的', 'article_number': '第一条', 'chapter_name': '総則', 'chapter_number': '第一章', 'division_name': '', 'division_number': '', 'document_source': 'https://laws.e-gov.go.jp/law/336AC0000000155', 'enactment_year': '令和4年12月16日 施行', 'law_name': '社会福祉施設職員等退職手当共済法', 'law_number': '昭和三十六年法律第百五十五号'}, page_content='昭和三十六年法律第百五十五号\\u3000社会福祉施設職員等退職手当共済法/第一章\\u3000総則/第一条\\u3000この法律の目的/この法律は、社会福祉施設、特定社会福祉事業及び特定介護保険施設等を経営する社会福祉法人の相互扶助の精神に基づき、社会福祉施設の職員、特定社会福祉事業に従事する職員及び特定介護保険施設等の職員について退職手当共済制度を確立し、もつて社会福祉事業の振興に寄与することを目的とする。'), Document(metadata={'article_name': '目的', 'article_number': '第一条', 'chapter_name': '総則', 'chapter_number': '第一章', 'division_name': '', 'division_number': '', 'document_source': 'https://laws.e-gov.go.jp/law/349AC0000000116', 'enactment_year': '令和6年10月1日 施行', 'law_name': '雇用保険法', 'law_number': '昭和四十九年法律第百十六号'}, page_content='昭和四十九年法律第百十六号\\u3000雇用保険法/第一章\\u3000総則/第一条\\u3000目的/雇用保険は、労働者が失業した場合及び労働者について雇用の継続が困難となる事由が生じた場合に必要な給付を行うほか、労働者が自ら職業に関する教育訓練を受けた場合及び労働者が子を養育するための休業をした場合に必要な給付を行うことにより、労働者の生活及び雇用の安定を図るとともに、求職活動を容易にする等その就職を促進し、あわせて、労働者の職業の安定に資するため、失業の予防、雇用状態の是正及び雇用機会の増大、労働者の能力の開発及び向上その他労働者の福祉の増進を図ることを目的とする。'), Document(metadata={'article_name': '目的', 'article_number': '第一条', 'chapter_name': '総則', 'chapter_number': '第一章', 'division_name': '', 'division_number': '', 'document_source': 'https://laws.e-gov.go.jp/law/346AC0000000068', 'enactment_year': '令和4年10月1日 施行', 'law_name': '高年齢者等の雇用の安定等に関する法律', 'law_number': '昭和四十六年法律第六十八号'}, page_content='昭和四十六年法律第六十八号\\u3000高年齢者等の雇用の安定等に関する法律/第一章\\u3000総則/第一条\\u3000目的/この法律は、定年の引上げ、継続雇用制度の導入等による高年齢者の安定した雇用の確保の促進、高年齢者等の再就職の促進、定年退職者その他の高年齢退職者に対する就業の機会の確保等の措置を総合的に講じ、もつて高年齢者等の職業の安定その他福祉の増進を図るとともに、経済及び社会の発展に寄与することを目的とする。')], 'filtered_documents': []}}], 'rewrite_count': 1, 'regen_count': 0, 'question_to_regen_index': [], 'root_text': {'Chapter': {'': {'chapter': '\\n', 'article': '-1', 'clause': '-1', 'sub_clause': '-1', 'text': '1\\u3000この規程は、株式会社●●●●の賃金規程第21条に基づき、会社の労働者の退職金に関する事項を定めたものである。'}}, 'Article': {'1': {'chapter': '\\n', 'article': '第1条\\u3000目的\\n', 'clause': '-1', 'sub_clause': '-1', 'text': '1\\u3000この規程は、株式会社●●●●の賃金規程第21条に基づき、会社の労働者の退職金に関する事項を定めたものである。'}}, 'clause': {('1', '1'): {'chapter': '\\n', 'article': '第1条\\u3000目的\\n', 'clause': '1. ', 'sub_clause': '-1', 'text': 'この規程は、株式会社●●●●の賃金規程第2条に基づき、会社の労働者の退職金に関する事項を定めたものである。'}}}}\n",
      "response_json [{'question': {'metadata': {'chapter_title': '', 'chapter_number': '', 'article_title': '目的', 'article_number': '1', 'clause_number': '1', 'clause_title': 'この規程は、株式会社●●●●の賃金規程第21条に基づき、会社の労働者の退職金に関する事項を定めたものである。', 'sub_clause_number': '', 'sub_clause_content': ''}, 'page_content': 'この規程は、株式会社●●●●の賃金規程第21条に基づき、会社の労働者の退職金に関する事項を定めたものである。'}, 'direct_documents': [], 'question_reference': '', 'reference_documents': [], 'message': 'Using websearch', 'web_search_docs': [{'metadata': {}, 'page_content': 'そして、賃金や退職金の決定・計算及び支払方法等につきましては、労働基準法におきまして就業規則上の必要記載事項と定められています。 従いまして、'}], 'documents': [], 'response': {'evaluation': 'insufficient_information', 'explanation': 'この契約条項は、退職金に関する事項を定めたものであると述べていますが、具体的な退職金の決定、計算、支払方法等の詳細が記載されていません。そのため、労働基準法における就業規則上の必要記載事項が満たされているかどうかを判断するのに十分な情報がありません。'}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from my_utils import normalize_dict\n",
    "from processor import label_and_parse_text_from_content\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.llms.vllm import VLLM\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "# from .GraphRAG import GraphRAG\n",
    "# from .ModuleRAG import ModuleRAG\n",
    "# from .ModuleRetrieval import ModuleRetrieval\n",
    "\n",
    "class RAGPipeline:\n",
    "    def __init__(self, vllm_model_path, data_path_retrieval):\n",
    "        self.config = RunnableConfig(recursion_limit=50)\n",
    "        # Tạo VLLM model\n",
    "        vllm_model = VLLM(\n",
    "            model=vllm_model_path,\n",
    "            tensor_parallel_size=1,\n",
    "            n=1,\n",
    "            presence_penalty=0.0,\n",
    "            frequency_penalty=0.0,\n",
    "            temperature=0.7,\n",
    "            top_p=0.6,\n",
    "            top_k=20,\n",
    "            stop=None,\n",
    "            ignore_eos=False,\n",
    "            max_new_tokens=2048,\n",
    "            logprobs=None,\n",
    "            download_dir=None,\n",
    "            vllm_kwargs={\n",
    "                \"quantization\":\"gptq\",\n",
    "                \"max_model_len\": 4096,\n",
    "                \"gpu_memory_utilization\":0.5\n",
    "            }\n",
    "        )\n",
    "        # Tạo moduleRAG\n",
    "        moduleRAG = ModuleRAG(vllm_model)\n",
    "        moduleRetrieval = ModuleRetrieval(data_path_retrieval, is_embedding=True)\n",
    "        \n",
    "        # Khởi tạo app\n",
    "        graphRAG = GraphRAG(\n",
    "            extract_reference_generator = moduleRAG.get_extract_reference_generator(),\n",
    "            sematic_reference_generator = moduleRAG.get_sematic_reference_generator(),\n",
    "            rewrite_clause_generator = moduleRAG.get_rewrite_clause_generator(),\n",
    "            proposition_generator = moduleRAG.get_proposition_generator(),\n",
    "            retriever = moduleRetrieval.get_module(),\n",
    "            retrieval_grader = moduleRAG.get_retrieval_grader(),\n",
    "            rag_chain_both = moduleRAG.get_rag_chain_both(),\n",
    "            rag_chain_doc = moduleRAG.get_rag_chain_doc(),\n",
    "            query_transformation_generator = moduleRAG.get_query_transformation_generator(),\n",
    "            hallucination_grader = moduleRAG.get_hallucination_grader(),\n",
    "            answer_grader=moduleRAG.get_answer_grander(),\n",
    "            web_search_tool = moduleRAG.get_web_search_tool(),\n",
    "            grade_web_search_docs= moduleRAG.get_grade_web_search_docs()\n",
    "        )\n",
    "        self.app = graphRAG.get_app()\n",
    "\n",
    "    def processing(self, file_content):\n",
    "        json_output, root_text = label_and_parse_text_from_content(file_content)\n",
    "        return json_output, root_text\n",
    "    \n",
    "    def create_graph_state(self, json_output, root_text):\n",
    "        # Chuẩn hóa dữ liệu\n",
    "        normalized_data= normalize_dict(json_output)\n",
    "        # Chuyển thành json\n",
    "        normalized_data_json = json.loads(normalized_data)\n",
    "        # Lấy số chương cần xử lý\n",
    "        print(\"Docs have \",len(normalized_data_json['chapters']),\"chapters\")\n",
    "        useful_data = {'chapters':normalized_data_json['chapters']}\n",
    "        # useful_data = {'chapters':normalized_data_json['chapters'][:3]}\n",
    "        # print(\"We using 3 first chapters\")\n",
    "        docs_list = [\n",
    "            Document(\n",
    "                page_content=sub_clause.get(\"sub_clause_content\") if sub_clause else clause[\"clause_title\"],\n",
    "                metadata={\n",
    "                    \"chapter_title\": chapter['chapter_title'],\n",
    "                    \"chapter_number\": chapter['chapter_number'],\n",
    "                    # \"chapter_text\": chapter['chapter_text'],\n",
    "                    \"article_title\": article['article_title'], \n",
    "                    \"article_number\": article['article_number'],\n",
    "                    # \"article_text\": article['article_text'],\n",
    "                    \"clause_number\": clause['clause_number'],\n",
    "                    \"clause_title\": clause[\"clause_title\"],\n",
    "                    # \"clause_text\": clause[\"clause_content\"],\n",
    "                    \"sub_clause_number\": sub_clause.get(\"sub_clause_number\") if sub_clause else \"\",\n",
    "                    \"sub_clause_content\": sub_clause.get(\"sub_clause_content\") if sub_clause else \"\",\n",
    "                }\n",
    "            )\n",
    "            for chapter in useful_data['chapters'] \n",
    "            for article in chapter['articles']\n",
    "            for clause in article['clauses']\n",
    "            for sub_clause in (clause[\"sub_clauses\"] or [None])  # Nếu rỗng thì tạo danh sách chứa None\n",
    "        ]\n",
    "        # Split\n",
    "        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "            chunk_size=500, chunk_overlap=30\n",
    "        )\n",
    "\n",
    "        doc_splits = text_splitter.split_documents(docs_list)\n",
    "        for i, doc in enumerate(doc_splits):\n",
    "            doc.metadata['chunk_id'] = i+1 ### adding chunk id\n",
    "\n",
    "        graph_state = {\"questions\":\n",
    "                    [\n",
    "                        {\"question_content\":Document(\n",
    "                            page_content=doc_split.metadata['clause_title'] +\"\\n\"+ doc_split.page_content if doc_split.metadata['sub_clause_content'] else doc_split.page_content,\n",
    "                            metadata={\n",
    "                                \"chapter_title\": doc_split.metadata['chapter_title'],\n",
    "                                \"chapter_number\": doc_split.metadata['chapter_number'],\n",
    "                                # \"chapter_text\": doc_split.metadata['chapter_text'],\n",
    "                                \"article_title\": doc_split.metadata['article_title'], \n",
    "                                \"article_number\": doc_split.metadata['article_number'],\n",
    "                                # \"article_text\": doc_split.metadata['article_text'],\n",
    "                                \"clause_number\": doc_split.metadata['clause_number'],\n",
    "                                \"clause_title\": doc_split.metadata['clause_title'],\n",
    "                                # \"clause_text\": doc_split.metadata['clause_text'],\n",
    "                                \"sub_clause_number\": doc_split.metadata['sub_clause_number'],\n",
    "                                \"sub_clause_content\": doc_split.metadata['sub_clause_content'],\n",
    "                            }),\n",
    "                        \"propositions\":[],\n",
    "                        \"ref_propositions\":[],\n",
    "                        \"all_docs\":[],\n",
    "                        \"web_search_docs\":[],\n",
    "                        \"filtered_web_search_docs\": []\n",
    "                        }\n",
    "        for doc_split in doc_splits],\n",
    "                        \"root_text\":root_text}\n",
    "        return graph_state\n",
    "    \n",
    "    def processsing_final_state(self, final_state):\n",
    "        response_json = []\n",
    "        # chạy tất cả các question trong final_state\n",
    "        for question in final_state[\"questions\"]:\n",
    "            docs = []\n",
    "            \n",
    "            for direct_proposition in question[\"propositions\"]:\n",
    "                for doc in direct_proposition[\"filtered_documents\"]:\n",
    "                    doc_dict = {\n",
    "                        \"metadata\": doc.metadata,\n",
    "                        \"page_content\": doc.page_content\n",
    "                    }\n",
    "                    if doc_dict not in docs:  # Tránh trùng lặp\n",
    "                        docs.append(doc_dict)\n",
    "\n",
    "            docs_ref = []\n",
    "\n",
    "            for ref_proposition in question[\"ref_propositions\"]:\n",
    "                for doc in ref_proposition[\"filtered_documents\"]:\n",
    "                    doc_dict = {\n",
    "                        \"metadata\": doc.metadata,\n",
    "                        \"page_content\": doc.page_content\n",
    "                    }\n",
    "                    if doc_dict not in docs:  # Tránh trùng lặp\n",
    "                        docs_ref.append(doc_dict)\n",
    "            \n",
    "            doc_web = []\n",
    "            for doc in question.get(\"filtered_web_search_docs\", []): \n",
    "                doc_web.append({\n",
    "                    \"metadata\": doc.metadata,\n",
    "                    \"page_content\": doc.page_content\n",
    "                })\n",
    "\n",
    "            full_docs = docs + docs_ref\n",
    "\n",
    "            response_json.append({\n",
    "                # question gốc sau khi xử lý\n",
    "                \"question\": {\n",
    "                        \"metadata\": question[\"question_content\"].metadata,\n",
    "                        \"page_content\": question[\"question_content\"].page_content\n",
    "                    },\n",
    "                # luật trực tiếp từ question gốc\n",
    "                \"direct_documents\": docs,\n",
    "                # câu question mà question gốc trỏ đến\n",
    "                \"question_reference\": question[\"merged_reference_text\"],\n",
    "                # những documents kiếm được từ câu viết lại của question và question_reference\n",
    "                \"reference_documents\": docs_ref,\n",
    "                # có sử dụng websearch hay không?\n",
    "                \"message\": \"Using websearch\" if question[\"web_search_docs\"]!=[] else \"Using vectorDB\",\n",
    "                \"web_search_docs\": doc_web,\n",
    "                \"documents\": full_docs,\n",
    "                \"response\": json.loads(question[\"generation\"]) if type(question[\"generation\"])==str else question[\"generation\"] \n",
    "            })\n",
    "\n",
    "        return response_json\n",
    "        \n",
    "    def run(self, file_content):\n",
    "        # Xử lý dữ liệu thành cấu trúc JSON và root_text\n",
    "        json_output, root_text = self.processing(file_content)\n",
    "        \n",
    "        # Tạo graph state\n",
    "        graph_state = self.create_graph_state(json_output, root_text)\n",
    "        print(\"graph_state\", graph_state)\n",
    "        final_state = self.app.invoke(graph_state,config=self.config)\n",
    "        print(\"final_state\",final_state)\n",
    "        response_json = self.processsing_final_state(final_state)\n",
    "        print(\"response_json\",response_json)\n",
    "        return response_json\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    print(\"Hello World\")\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "    os.environ['TAVILY_API_KEY'] = \"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\"\n",
    "    # Khởi tạo các biến cần thiết\n",
    "    vllm_model_path = \"/home/trung/RAG_ADVANCED/Qwen2.5-14B-Instruct-GPTQ-Int4\"\n",
    "    data_path_retrieval = \"/home/trung/RAG_ADVANCED/full_corpus_110225_with_metadata.json\"\n",
    "    # Khởi tạo RAG pipeline\n",
    "    rag_pipeline = RAGPipeline(vllm_model_path=vllm_model_path, data_path_retrieval=data_path_retrieval)\n",
    "    # Đọc dữ liệu file txt cần xử lý\n",
    "    input_file = r\"/home/trung/Paper2/gpt_api_update/Full_Module/text_file1_3chapters.txt\"\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile:\n",
    "        file_content = infile.read()\n",
    "\n",
    "    # Truyền dữ liệu vào pipeline\n",
    "    output_data = rag_pipeline.run(file_content)\n",
    "    # # Xuất kết quả\n",
    "    # final_state=\n",
    "    # RAGPipeline.processsing_final_state(final_state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
